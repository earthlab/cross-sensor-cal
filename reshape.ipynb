{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c501955a-4e56-40df-93e4-346c6e5ad935",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spectral_unmixing_tools as el_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5c38436-e4bf-4a83-82ed-fdb1ec79276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/earthlab/cross-sensor-cal.git\n",
      "  Cloning https://github.com/earthlab/cross-sensor-cal.git to /tmp/pip-req-build-3mcb9iw1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/earthlab/cross-sensor-cal.git /tmp/pip-req-build-3mcb9iw1\n",
      "  Resolved https://github.com/earthlab/cross-sensor-cal.git to commit 8717d6b99deb35c43dc57fc2bec4882c2f254814\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.26.3)\n",
      "Requirement already satisfied: spectral>=0.22 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.23.1)\n",
      "Requirement already satisfied: geopandas>=0.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.14.2)\n",
      "Requirement already satisfied: rasterio>=1.1.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.3.8)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.2.2)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.9.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.31.0)\n",
      "Requirement already satisfied: ray>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.8.0)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (1.9.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (23.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (3.6.0)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: affine in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (8.1.3)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (0.7.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.4.7)\n",
      "Requirement already satisfied: click-plugins in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (69.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.0.5)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.23.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (6.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (2.0.7)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.11.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas>=0.8.0->EarthLabSpectral==0.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.17.1)\n",
      "Building wheels for collected packages: EarthLabSpectral\n",
      "  Building wheel for EarthLabSpectral (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EarthLabSpectral: filename=EarthLabSpectral-0.1-py3-none-any.whl size=1417 sha256=bebfc882dd13f60e077c2b288b2ae7dd343eae9db8bd57580f13cd335263fe64\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e_8ou5jp/wheels/d4/97/e8/c93ae5a8364c661ba5327b524ebe4ee01e41cfe582ac66f3d0\n",
      "Successfully built EarthLabSpectral\n",
      "Installing collected packages: EarthLabSpectral\n",
      "Successfully installed EarthLabSpectral-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/earthlab/cross-sensor-cal.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c5d65-796e-43a7-b464-6d3007d7210c",
   "metadata": {},
   "source": [
    "## Extract spectra as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d381f8-9d78-4d4a-be6c-0fc5ee0262e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 10782, 1071)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None  # This will hold the raster data array\n",
    "        self.file_type = \"envi\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the raster data from the file_path into self.data\"\"\"\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            self.data = src.read()  # Read all bands\n",
    "\n",
    "    def get_chunk_from_extent(self, corrections=[], resample=False):\n",
    "        self.load_data()  # Ensure data is loaded\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            bounds = src.bounds\n",
    "            width, height = src.width, src.height\n",
    "            col_start, line_start = 0, 0\n",
    "            col_end, line_end = width, height\n",
    "\n",
    "            # Assuming self.data is a 3D numpy array with dimensions [bands, rows, cols]\n",
    "            chunk = self.data[:, line_start:line_end, col_start:col_end]\n",
    "\n",
    "            # Apply any processing to chunk here...\n",
    "            # For example, to demonstrate, flip chunk vertically\n",
    "            chunk = np.flip(chunk, axis=1)\n",
    "\n",
    "            return chunk\n",
    "\n",
    "def load_and_combine_rasters(raster_paths):\n",
    "    \"\"\"\n",
    "    Loads and combines raster data from a list of file paths.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for path in raster_paths:\n",
    "        processor = ENVIProcessor(path)\n",
    "        chunk = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    combined_array = np.concatenate(chunks, axis=0)  # Combine along the first axis (bands)\n",
    "    return combined_array\n",
    "\n",
    "# Provided raster paths\n",
    "raster_paths = [\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_7_ETMplus.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_8_OLI.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_9_OLI-2.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectance\"\n",
    "]\n",
    "\n",
    "# Ensure paths are updated correctly, especially for Landsat 7, 8, and 9 as provided paths are duplicates of Landsat 5\n",
    "combined_array = load_and_combine_rasters(raster_paths)\n",
    "combined_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e909a413-bff9-4a41-b7f0-5b08b0fc64b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_id</th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>Original_band_1</th>\n",
       "      <th>Original_band_2</th>\n",
       "      <th>Original_band_3</th>\n",
       "      <th>Original_band_4</th>\n",
       "      <th>Original_band_5</th>\n",
       "      <th>Original_band_6</th>\n",
       "      <th>Original_band_7</th>\n",
       "      <th>...</th>\n",
       "      <th>Landsat_8_band_3</th>\n",
       "      <th>Landsat_8_band_4</th>\n",
       "      <th>Landsat_8_band_5</th>\n",
       "      <th>Landsat_8_band_6</th>\n",
       "      <th>Landsat_9_band_1</th>\n",
       "      <th>Landsat_9_band_2</th>\n",
       "      <th>Landsat_9_band_3</th>\n",
       "      <th>Landsat_9_band_4</th>\n",
       "      <th>Landsat_9_band_5</th>\n",
       "      <th>Landsat_9_band_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547517</th>\n",
       "      <td>11547517</td>\n",
       "      <td>10781</td>\n",
       "      <td>1066</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547518</th>\n",
       "      <td>11547518</td>\n",
       "      <td>10781</td>\n",
       "      <td>1067</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547519</th>\n",
       "      <td>11547519</td>\n",
       "      <td>10781</td>\n",
       "      <td>1068</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547520</th>\n",
       "      <td>11547520</td>\n",
       "      <td>10781</td>\n",
       "      <td>1069</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11547521</th>\n",
       "      <td>11547521</td>\n",
       "      <td>10781</td>\n",
       "      <td>1070</td>\n",
       "      <td>-9998.999998</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9998.857661</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11547522 rows Ã— 879 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pixel_id  Pixel_Row  Pixel_Col  Original_band_1  Original_band_2  \\\n",
       "0                0          0          0     -9998.999998          -9999.0   \n",
       "1                1          0          1     -9998.999998          -9999.0   \n",
       "2                2          0          2     -9998.999998          -9999.0   \n",
       "3                3          0          3     -9998.999998          -9999.0   \n",
       "4                4          0          4     -9998.999998          -9999.0   \n",
       "...            ...        ...        ...              ...              ...   \n",
       "11547517  11547517      10781       1066     -9998.999998          -9999.0   \n",
       "11547518  11547518      10781       1067     -9998.999998          -9999.0   \n",
       "11547519  11547519      10781       1068     -9998.999998          -9999.0   \n",
       "11547520  11547520      10781       1069     -9998.999998          -9999.0   \n",
       "11547521  11547521      10781       1070     -9998.999998          -9999.0   \n",
       "\n",
       "          Original_band_3  Original_band_4  Original_band_5  Original_band_6  \\\n",
       "0                 -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "1                 -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "2                 -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "3                 -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "4                 -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "...                   ...              ...              ...              ...   \n",
       "11547517          -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "11547518          -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "11547519          -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "11547520          -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "11547521          -9999.0          -9999.0          -9999.0     -9998.857661   \n",
       "\n",
       "          Original_band_7  ...  Landsat_8_band_3  Landsat_8_band_4  \\\n",
       "0                 -9999.0  ...           -9999.0           -9999.0   \n",
       "1                 -9999.0  ...           -9999.0           -9999.0   \n",
       "2                 -9999.0  ...           -9999.0           -9999.0   \n",
       "3                 -9999.0  ...           -9999.0           -9999.0   \n",
       "4                 -9999.0  ...           -9999.0           -9999.0   \n",
       "...                   ...  ...               ...               ...   \n",
       "11547517          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547518          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547519          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547520          -9999.0  ...           -9999.0           -9999.0   \n",
       "11547521          -9999.0  ...           -9999.0           -9999.0   \n",
       "\n",
       "          Landsat_8_band_5  Landsat_8_band_6  Landsat_9_band_1  \\\n",
       "0                  -9999.0           -9999.0           -9999.0   \n",
       "1                  -9999.0           -9999.0           -9999.0   \n",
       "2                  -9999.0           -9999.0           -9999.0   \n",
       "3                  -9999.0           -9999.0           -9999.0   \n",
       "4                  -9999.0           -9999.0           -9999.0   \n",
       "...                    ...               ...               ...   \n",
       "11547517           -9999.0           -9999.0           -9999.0   \n",
       "11547518           -9999.0           -9999.0           -9999.0   \n",
       "11547519           -9999.0           -9999.0           -9999.0   \n",
       "11547520           -9999.0           -9999.0           -9999.0   \n",
       "11547521           -9999.0           -9999.0           -9999.0   \n",
       "\n",
       "          Landsat_9_band_2  Landsat_9_band_3  Landsat_9_band_4  \\\n",
       "0                  -9999.0           -9999.0           -9999.0   \n",
       "1                  -9999.0           -9999.0           -9999.0   \n",
       "2                  -9999.0           -9999.0           -9999.0   \n",
       "3                  -9999.0           -9999.0           -9999.0   \n",
       "4                  -9999.0           -9999.0           -9999.0   \n",
       "...                    ...               ...               ...   \n",
       "11547517           -9999.0           -9999.0           -9999.0   \n",
       "11547518           -9999.0           -9999.0           -9999.0   \n",
       "11547519           -9999.0           -9999.0           -9999.0   \n",
       "11547520           -9999.0           -9999.0           -9999.0   \n",
       "11547521           -9999.0           -9999.0           -9999.0   \n",
       "\n",
       "          Landsat_9_band_5  Landsat_9_band_6  \n",
       "0                  -9999.0           -9999.0  \n",
       "1                  -9999.0           -9999.0  \n",
       "2                  -9999.0           -9999.0  \n",
       "3                  -9999.0           -9999.0  \n",
       "4                  -9999.0           -9999.0  \n",
       "...                    ...               ...  \n",
       "11547517           -9999.0           -9999.0  \n",
       "11547518           -9999.0           -9999.0  \n",
       "11547519           -9999.0           -9999.0  \n",
       "11547520           -9999.0           -9999.0  \n",
       "11547521           -9999.0           -9999.0  \n",
       "\n",
       "[11547522 rows x 879 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_and_flatten_array(array, landsat_versions=[5, 7, 8, 9], bands_per_landsat=6):\n",
    "    \"\"\"\n",
    "    Processes a 3D numpy array to a DataFrame, renames columns, and adds Pixel_id.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    - landsat_versions: A list of Landsat versions to use for naming.\n",
    "    - bands_per_landsat: Number of bands per Landsat version.\n",
    "    \n",
    "    Returns:\n",
    "    - A pandas DataFrame with processed and renamed columns and added Pixel_id.\n",
    "    \"\"\"\n",
    "    if len(array.shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3-dimensional.\")\n",
    "    \n",
    "    # Flatten the array\n",
    "    bands, rows, cols = array.shape\n",
    "    reshaped_array = array.reshape(bands, -1).T  # Transpose to make bands as columns\n",
    "    pixel_indices = np.indices((rows, cols)).reshape(2, -1).T  # Row and col indices\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(reshaped_array, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    df.insert(0, 'Pixel_Col', pixel_indices[:, 1])\n",
    "    df.insert(0, 'Pixel_Row', pixel_indices[:, 0])\n",
    "    df.insert(0, 'Pixel_id', np.arange(len(df)))\n",
    "\n",
    "    # Renaming columns\n",
    "    total_bands = bands\n",
    "    original_and_corrected_bands = total_bands - bands_per_landsat * len(landsat_versions)\n",
    "    band_per_version = original_and_corrected_bands // 2  # Assuming equal original and corrected bands\n",
    "    \n",
    "    new_names = ([f\"Original_band_{i}\" for i in range(1, band_per_version + 1)] +\n",
    "                 [f\"Corrected_band_{i}\" for i in range(1, band_per_version + 1)])\n",
    "    \n",
    "    for version in landsat_versions:\n",
    "        new_names.extend([f\"Landsat_{version}_band_{i}\" for i in range(1, bands_per_landsat + 1)])\n",
    "    \n",
    "    # Apply new column names for band columns\n",
    "    df.columns = ['Pixel_id', 'Pixel_Row', 'Pixel_Col'] + new_names\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage with a hypothetical array shape\n",
    "# Assume 'combined_array' is your loaded and combined 3D numpy array\n",
    "# combined_array = np.random.rand(426*2 + 4*6, 100, 100)  # Example array\n",
    "\n",
    "# Process and flatten the array\n",
    "df_processed = process_and_flatten_array(combined_array)\n",
    "\n",
    "# Now 'df_processed' is the DataFrame with renamed columns and added Pixel_id\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58be238-60a4-43d1-ad30-a893a3c09211",
   "metadata": {},
   "source": [
    "## flatten array into a 2D df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56916c3a-60fe-4309-bffc-f8712256d5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning process, total rows: 11547522, chunk size: 100000, total chunks: 116\n",
      "Processed and wrote chunk 1/116 to CSV.\n",
      "Processed and wrote chunk 2/116 to CSV.\n",
      "Processed and wrote chunk 3/116 to CSV.\n",
      "Processed and wrote chunk 4/116 to CSV.\n",
      "Processed and wrote chunk 5/116 to CSV.\n",
      "Processed and wrote chunk 6/116 to CSV.\n",
      "Processed and wrote chunk 7/116 to CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_data_and_write_to_csv(df, output_csv_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame in chunks to minimize memory usage and writes the cleaned\n",
    "    chunks directly to a CSV file to avoid memory overload. It replaces values approximately\n",
    "    equal to -9999 (within a tolerance of 1) with NaN in columns not starting with 'Pixel', and\n",
    "    then drops rows where all such columns are NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to clean.\n",
    "    - output_csv_path: Path to the output CSV file.\n",
    "    - chunk_size: Number of rows in each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - None. The cleaned data is written directly to the specified CSV file.\n",
    "    \"\"\"\n",
    "    total_rows = df.shape[0]\n",
    "    num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size else 0)\n",
    "\n",
    "    print(f\"Starting cleaning process, total rows: {total_rows}, chunk size: {chunk_size}, total chunks: {num_chunks}\")\n",
    "\n",
    "    # Initialize CSV file writing\n",
    "    first_chunk = True\n",
    "\n",
    "    for i, start_row in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk = df.iloc[start_row:start_row + chunk_size].copy()\n",
    "\n",
    "        # Replace values close to -9999 with NaN\n",
    "        for col in chunk.columns:\n",
    "            if not col.startswith('Pixel'):\n",
    "                chunk[col] = np.where(np.isclose(chunk[col], -9999, atol=1), np.nan, chunk[col])\n",
    "\n",
    "        # Drop rows where all non-'Pixel' columns are NaN\n",
    "        non_pixel_columns = [col for col in chunk.columns if not col.startswith('Pixel')]\n",
    "        chunk.dropna(subset=non_pixel_columns, how='all', inplace=True)\n",
    "        \n",
    "        # Write processed chunk to CSV\n",
    "        if first_chunk:\n",
    "            chunk.to_csv(output_csv_path, mode='w', header=True, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_csv(output_csv_path, mode='a', header=False, index=False)\n",
    "        \n",
    "        print(f\"Processed and wrote chunk {i+1}/{num_chunks} to CSV.\")\n",
    "\n",
    "    print(\"Cleaning process completed and data written to CSV.\")\n",
    "\n",
    "# Specify the output CSV path\n",
    "output_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance_active_pixels.csv'\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "clean_data_and_write_to_csv(df_processed, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037be62c-97e0-48c2-aa40-0791e0704b48",
   "metadata": {},
   "source": [
    "## Reshape data frame and change labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6ac2ce0-247d-4576-8521-cb53d0504289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 'Band_1', -9998.999997820287)\n",
      "(0, 1, 'Band_1', -9998.999997820287)\n",
      "(0, 2, 'Band_1', -9998.999997820287)\n",
      "(0, 3, 'Band_1', -9998.999997820287)\n",
      "(0, 4, 'Band_1', -9998.999997820287)\n",
      "(0, 5, 'Band_1', -9998.999997820287)\n",
      "(0, 6, 'Band_1', -9998.999997820287)\n",
      "(0, 7, 'Band_1', -9998.999997820287)\n",
      "(0, 8, 'Band_1', -9998.999997820287)\n",
      "(0, 9, 'Band_1', -9998.999997820287)\n",
      "(0, 10, 'Band_1', -9998.999997820287)\n",
      "(0, 11, 'Band_1', -9998.999997820287)\n",
      "(0, 12, 'Band_1', -9998.999997820287)\n",
      "(0, 13, 'Band_1', -9998.999997820287)\n",
      "(0, 14, 'Band_1', -9998.999997820287)\n",
      "(0, 15, 'Band_1', -9998.999997820287)\n",
      "(0, 16, 'Band_1', -9998.999997820287)\n",
      "(0, 17, 'Band_1', -9998.999997820287)\n",
      "(0, 18, 'Band_1', -9998.999997820287)\n",
      "(0, 19, 'Band_1', -9998.999997820287)\n",
      "(0, 20, 'Band_1', -9998.999997820287)\n",
      "(0, 21, 'Band_1', -9998.999997820287)\n",
      "(0, 22, 'Band_1', -9998.999997820287)\n",
      "(0, 23, 'Band_1', -9998.999997820287)\n",
      "(0, 24, 'Band_1', -9998.999997820287)\n",
      "(0, 25, 'Band_1', -9998.999997820287)\n",
      "(0, 26, 'Band_1', -9998.999997820287)\n",
      "(0, 27, 'Band_1', -9998.999997820287)\n",
      "(0, 28, 'Band_1', -9998.999997820287)\n",
      "(0, 29, 'Band_1', -9998.999997820287)\n",
      "(0, 30, 'Band_1', -9998.999997820287)\n",
      "(0, 31, 'Band_1', -9998.999997820287)\n",
      "(0, 32, 'Band_1', -9998.999997820287)\n",
      "(0, 33, 'Band_1', -9998.999997820287)\n",
      "(0, 34, 'Band_1', -9998.999997820287)\n",
      "(0, 35, 'Band_1', -9998.999997820287)\n",
      "(0, 36, 'Band_1', -9998.999997820287)\n",
      "(0, 37, 'Band_1', -9998.999997820287)\n",
      "(0, 38, 'Band_1', -9998.999997820287)\n",
      "(0, 39, 'Band_1', -9998.999997820287)\n",
      "(0, 40, 'Band_1', -9998.999997820287)\n",
      "(0, 41, 'Band_1', -9998.999997820287)\n",
      "(0, 42, 'Band_1', -9998.999997820287)\n",
      "(0, 43, 'Band_1', -9998.999997820287)\n",
      "(0, 44, 'Band_1', -9998.999997820287)\n",
      "(0, 45, 'Band_1', -9998.999997820287)\n",
      "(0, 46, 'Band_1', -9998.999997820287)\n",
      "(0, 47, 'Band_1', -9998.999997820287)\n",
      "(0, 48, 'Band_1', -9998.999997820287)\n",
      "(0, 49, 'Band_1', -9998.999997820287)\n",
      "(0, 50, 'Band_1', -9998.999997820287)\n",
      "(0, 51, 'Band_1', -9998.999997820287)\n",
      "(0, 52, 'Band_1', -9998.999997820287)\n",
      "(0, 53, 'Band_1', -9998.999997820287)\n",
      "(0, 54, 'Band_1', -9998.999997820287)\n",
      "(0, 55, 'Band_1', -9998.999997820287)\n",
      "(0, 56, 'Band_1', -9998.999997820287)\n",
      "(0, 57, 'Band_1', -9998.999997820287)\n",
      "(0, 58, 'Band_1', -9998.999997820287)\n",
      "(0, 59, 'Band_1', -9998.999997820287)\n",
      "(0, 60, 'Band_1', -9998.999997820287)\n",
      "(0, 61, 'Band_1', -9998.999997820287)\n",
      "(0, 62, 'Band_1', -9998.999997820287)\n",
      "(0, 63, 'Band_1', -9998.999997820287)\n",
      "(0, 64, 'Band_1', -9998.999997820287)\n",
      "(0, 65, 'Band_1', -9998.999997820287)\n",
      "(0, 66, 'Band_1', -9998.999997820287)\n",
      "(0, 67, 'Band_1', -9998.999997820287)\n",
      "(0, 68, 'Band_1', -9998.999997820287)\n",
      "(0, 69, 'Band_1', -9998.999997820287)\n",
      "(0, 70, 'Band_1', -9998.999997820287)\n",
      "(0, 71, 'Band_1', -9998.999997820287)\n",
      "(0, 72, 'Band_1', -9998.999997820287)\n",
      "(0, 73, 'Band_1', -9998.999997820287)\n",
      "(0, 74, 'Band_1', -9998.999997820287)\n",
      "(0, 75, 'Band_1', -9998.999997820287)\n",
      "(0, 76, 'Band_1', -9998.999997820287)\n",
      "(0, 77, 'Band_1', -9998.999997820287)\n",
      "(0, 78, 'Band_1', -9998.999997820287)\n",
      "(0, 79, 'Band_1', -9998.999997820287)\n",
      "(0, 80, 'Band_1', -9998.999997820287)\n",
      "(0, 81, 'Band_1', -9998.999997820287)\n",
      "(0, 82, 'Band_1', -9998.999997820287)\n",
      "(0, 83, 'Band_1', -9998.999997820287)\n",
      "(0, 84, 'Band_1', -9998.999997820287)\n",
      "(0, 85, 'Band_1', -9998.999997820287)\n",
      "(0, 86, 'Band_1', -9998.999997820287)\n",
      "(0, 87, 'Band_1', -9998.999997820287)\n",
      "(0, 88, 'Band_1', -9998.999997820287)\n",
      "(0, 89, 'Band_1', -9998.999997820287)\n",
      "(0, 90, 'Band_1', -9998.999997820287)\n",
      "(0, 91, 'Band_1', -9998.999997820287)\n",
      "(0, 92, 'Band_1', -9998.999997820287)\n",
      "(0, 93, 'Band_1', -9998.999997820287)\n",
      "(0, 94, 'Band_1', -9998.999997820287)\n",
      "(0, 95, 'Band_1', -9998.999997820287)\n",
      "(0, 96, 'Band_1', -9998.999997820287)\n",
      "(0, 97, 'Band_1', -9998.999997820287)\n",
      "(0, 98, 'Band_1', -9998.999997820287)\n",
      "(0, 99, 'Band_1', -9998.999997820287)\n",
      "(0, 100, 'Band_1', -9998.999997820287)\n",
      "(0, 101, 'Band_1', -9998.999997820287)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iterate_flatten_melt_array(array):\n",
    "    \"\"\"\n",
    "    Generator to iterate over a 3D numpy array and yield \"melted\" data.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    \n",
    "    Yields:\n",
    "    - Tuple of (Pixel_Row, Pixel_Col, Band_ID, Wavelength) for each pixel-band combination.\n",
    "    \"\"\"\n",
    "    bands, rows, cols = array.shape\n",
    "    \n",
    "    for band in range(bands):\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                yield (row, col, f'Band_{band+1}', array[band, row, col])\n",
    "\n",
    "# Example usage\n",
    "#chunk = np.random.rand(426, 11138, 1031)  # Replace with your actual data\n",
    "\n",
    "# To demonstrate or test the generator, you can iterate through a small portion of it\n",
    "for i, data_point in enumerate(iterate_flatten_melt_array(chunk)):\n",
    "    print(data_point)\n",
    "    if i > 100:  # Adjust this condition to control how many items you want to print\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3768125-a13d-4c7e-8dfd-6cad4d1f0629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('melted_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Pixel_Row', 'Pixel_Col', 'Band_ID', 'Wavelength'])  # Write header\n",
    "\n",
    "    # Write each data point\n",
    "    for data_point in iterate_flatten_melt_array(chunk):\n",
    "        writer.writerow(data_point)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def batch_flatten_melt_array(array, batch_size=1000000):\n",
    "    \"\"\"\n",
    "    Generator to iterate over a 3D numpy array and yield batches of \"melted\" data.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    - batch_size: The number of rows in each batch.\n",
    "    \n",
    "    Yields:\n",
    "    - A DataFrame containing a batch of melted data.\n",
    "    \"\"\"\n",
    "    bands, rows, cols = array.shape\n",
    "    total_pixels = rows * cols\n",
    "    num_batches = (total_pixels + batch_size - 1) // batch_size  # Ceiling division to get the number of batches\n",
    "    \n",
    "    for batch in range(num_batches):\n",
    "        batch_data = []\n",
    "        start_index = batch * batch_size\n",
    "        end_index = min(start_index + batch_size, total_pixels)\n",
    "        \n",
    "        for index in range(start_index, end_index):\n",
    "            row = index // cols\n",
    "            col = index % cols\n",
    "            for band in range(bands):\n",
    "                batch_data.append((row, col, f'Band_{band+1}', array[band, row, col]))\n",
    "                \n",
    "        batch_df = pd.DataFrame(batch_data, columns=['Pixel_Row', 'Pixel_Col', 'Band_ID', 'Wavelength'])\n",
    "        yield batch_df\n",
    "\n",
    "# Example usage\n",
    "#chunk = np.random.rand(426, 11138, 1031)  # Replace with your actual data\n",
    "\n",
    "# Iterate through each batch and process\n",
    "for i, batch_df in enumerate(batch_flatten_melt_array(chunk)):\n",
    "    print(f\"Processing batch {i+1}\")\n",
    "    # Process the batch_df here\n",
    "    # For example, you could save each batch to a separate CSV file\n",
    "    batch_df.to_csv(f'melted_data_batch_{i+1}.csv', index=False)\n",
    "   # if i == 0:  # For demonstration, break after processing the first batch\n",
    "    #    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6d650-7c31-4100-bd3e-4a06a0af777d",
   "metadata": {},
   "source": [
    "## Exract by polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551e6f3c-68ea-446a-806d-d789ab16d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>EditDate</th>\n",
       "      <th>Editor</th>\n",
       "      <th>description_notes</th>\n",
       "      <th>dbh</th>\n",
       "      <th>tree_height</th>\n",
       "      <th>...</th>\n",
       "      <th>og_flight_date</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>collector_name</th>\n",
       "      <th>plot</th>\n",
       "      <th>location</th>\n",
       "      <th>woody_shrub_height</th>\n",
       "      <th>imagery</th>\n",
       "      <th>combined_all_category_species</th>\n",
       "      <th>area_m</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1223</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1224</td>\n",
       "      <td>558</td>\n",
       "      <td>{54D1A59F-2B5C-4D76-913A-690314E314A7}</td>\n",
       "      <td>2023-06-20 19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Non-vegetated &amp; dead_Rock__</td>\n",
       "      <td>6.064980</td>\n",
       "      <td>POLYGON ((452524.7605621438 4435014.4842204545...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>808</td>\n",
       "      <td>548</td>\n",
       "      <td>{C3209290-3F79-4268-AE0F-4BE7F43E1C71}</td>\n",
       "      <td>2023-06-20 21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:03:43+00:00</td>\n",
       "      <td>2023-06-20 21:03:34+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>23.032479</td>\n",
       "      <td>POLYGON ((452510.4188020002 4435431.162891659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>792</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>792</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>793</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>793</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pixel_Row  Pixel_Col                                GlobalID  \\\n",
       "0         1222        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "1         1222        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "2         1223        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "3         1223        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "4         1224        558  {54D1A59F-2B5C-4D76-913A-690314E314A7}   \n",
       "..         ...        ...                                     ...   \n",
       "492        808        548  {C3209290-3F79-4268-AE0F-4BE7F43E1C71}   \n",
       "493        792        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "494        792        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "495        793        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "496        793        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "\n",
       "                 CreationDate                     Creator  \\\n",
       "0   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "1   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "2   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "4   2023-06-20 19:28:37+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "..                        ...                         ...   \n",
       "492 2023-06-20 21:04:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "493 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "494 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "495 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "496 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "\n",
       "                      EditDate                      Editor description_notes  \\\n",
       "0    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "1    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "2    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "4    2023-06-20T19:28:37+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "..                         ...                         ...               ...   \n",
       "492  2023-06-20T21:04:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "493  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "494  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "495  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "496  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "\n",
       "      dbh tree_height  ...            og_flight_date  \\\n",
       "0    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "1    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "2    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "3    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "4    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "..    ...         ...  ...                       ...   \n",
       "492  None        None  ... 2023-06-20 21:03:43+00:00   \n",
       "493  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "494  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "495  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "496  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "\n",
       "              collection_date collector_name plot  location  \\\n",
       "0   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "1   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "2   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "3   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "4   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "..                        ...            ...  ...       ...   \n",
       "492 2023-06-20 21:03:34+00:00          Katie    0  Brainard   \n",
       "493 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "494 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "495 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "496 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "\n",
       "    woody_shrub_height imagery  \\\n",
       "0                 None     AOP   \n",
       "1                 None     AOP   \n",
       "2                 None     AOP   \n",
       "3                 None     AOP   \n",
       "4                 None     AOP   \n",
       "..                 ...     ...   \n",
       "492         10cm to 1m     AOP   \n",
       "493         10cm to 1m     AOP   \n",
       "494         10cm to 1m     AOP   \n",
       "495         10cm to 1m     AOP   \n",
       "496         10cm to 1m     AOP   \n",
       "\n",
       "                         combined_all_category_species     area_m  \\\n",
       "0                        Evergreen___Picea engelmannii   7.004149   \n",
       "1                        Evergreen___Picea engelmannii   7.004149   \n",
       "2                        Evergreen___Picea engelmannii   7.004149   \n",
       "3                        Evergreen___Picea engelmannii   7.004149   \n",
       "4                          Non-vegetated & dead_Rock__   6.064980   \n",
       "..                                                 ...        ...   \n",
       "492  Woody shrub_Woody shrub - Broadleaf__Unidentified  23.032479   \n",
       "493  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "494  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "495  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "496  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "1    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "2    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "3    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "4    POLYGON ((452524.7605621438 4435014.4842204545...  \n",
       "..                                                 ...  \n",
       "492  POLYGON ((452510.4188020002 4435431.162891659,...  \n",
       "493  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "494  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "495  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "496  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "\n",
       "[497 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, raster_path, polygons_path):\n",
    "        self.raster_path = raster_path\n",
    "        self.polygons_path = polygons_path\n",
    "        self.polygons = None\n",
    "        self.raster_meta = None\n",
    "        \n",
    "    def load_polygons(self):\n",
    "        \"\"\"Loads the polygons and ensures they are in the same CRS as the raster.\"\"\"\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            self.raster_meta = src.meta\n",
    "            self.polygons = gpd.read_file(self.polygons_path)\n",
    "            self.polygons = self.polygons.to_crs(src.crs)\n",
    "    \n",
    "    def extract_data_by_polygons(self):\n",
    "        \"\"\"Extracts the row and col indices from the raster for each polygon and appends all attributes from the polygons.\"\"\"\n",
    "        self.load_polygons()  # Load polygons and ensure CRS match\n",
    "        \n",
    "        all_data = []\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            raster_bounds = src.bounds\n",
    "            raster_box = box(*raster_bounds)\n",
    "            \n",
    "            for _, poly in self.polygons.iterrows():\n",
    "                geom = poly.geometry\n",
    "                # Skip invalid or empty geometries or those that do not intersect with the raster\n",
    "                if geom is None or geom.is_empty or not geom.intersects(raster_box):\n",
    "                    continue\n",
    "                \n",
    "                window = from_bounds(*geom.bounds, transform=src.transform)\n",
    "                # Skip windows that are completely outside the raster bounds\n",
    "                if window.width <= 0 or window.height <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert window offsets to integers\n",
    "                row_off = int(window.row_off)\n",
    "                col_off = int(window.col_off)\n",
    "                # Extract the rows and cols from the window\n",
    "                rows = range(row_off, row_off + int(window.height))\n",
    "                cols = range(col_off, col_off + int(window.width))\n",
    "                \n",
    "                # Collect all attributes from the polygon\n",
    "                attributes = poly.to_dict()\n",
    "                \n",
    "                # Append the rows and cols along with the polygon attributes to the data list\n",
    "                for row in rows:\n",
    "                    for col in cols:\n",
    "                        pixel_data = {\n",
    "                            'Pixel_Row': row,\n",
    "                            'Pixel_Col': col,\n",
    "                            **attributes  # This adds all polygon attributes\n",
    "                        }\n",
    "                        all_data.append(pixel_data)\n",
    "        \n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "\n",
    "gpkg_path = 'Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg'\n",
    "existing_raster_path = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\"\n",
    "processor = ENVIProcessor(existing_raster_path, gpkg_path)\n",
    "df_polygons = processor.extract_data_by_polygons()\n",
    "df_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb73915d-a626-4b9b-8578-b00551760870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>Band_1</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>EditDate</th>\n",
       "      <th>Editor</th>\n",
       "      <th>description_notes</th>\n",
       "      <th>dbh</th>\n",
       "      <th>...</th>\n",
       "      <th>Band_417</th>\n",
       "      <th>Band_418</th>\n",
       "      <th>Band_419</th>\n",
       "      <th>Band_420</th>\n",
       "      <th>Band_421</th>\n",
       "      <th>Band_422</th>\n",
       "      <th>Band_423</th>\n",
       "      <th>Band_424</th>\n",
       "      <th>Band_425</th>\n",
       "      <th>Band_426</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pixel_Row, Pixel_Col, Band_1, GlobalID, CreationDate, Creator, EditDate, Editor, description_notes, dbh, tree_height, other_species, species, other_subcategory, dead_subcategory, cover_subcategory, cover_category, og_flight_date, collection_date, collector_name, plot, location, woody_shrub_height, imagery, combined_all_category_species, area_m, geometry, Band_2, Band_3, Band_4, Band_5, Band_6, Band_7, Band_8, Band_9, Band_10, Band_11, Band_12, Band_13, Band_14, Band_15, Band_16, Band_17, Band_18, Band_19, Band_20, Band_21, Band_22, Band_23, Band_24, Band_25, Band_26, Band_27, Band_28, Band_29, Band_30, Band_31, Band_32, Band_33, Band_34, Band_35, Band_36, Band_37, Band_38, Band_39, Band_40, Band_41, Band_42, Band_43, Band_44, Band_45, Band_46, Band_47, Band_48, Band_49, Band_50, Band_51, Band_52, Band_53, Band_54, Band_55, Band_56, Band_57, Band_58, Band_59, Band_60, Band_61, Band_62, Band_63, Band_64, Band_65, Band_66, Band_67, Band_68, Band_69, Band_70, Band_71, Band_72, Band_73, Band_74, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 452 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_columns = [f'Band_{i}' for i in range(1, 426)]\n",
    "df_polygons = df_polygons.replace(-9999.0, np.nan)\n",
    "# Drop rows where all values in the specified columns are NaN\n",
    "df_cleaned_polygons = df_polygons.dropna(subset=layer_columns, how='any')\n",
    "df_cleaned_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85a55317-a896-4b16-86cf-5b48a22340c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4c3ee9a42742a4ac54d90430003bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV:   0%|          | 0/66 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path):\n",
    "    \"\"\"\n",
    "    Filters rows in a large CSV file, keeping only those that match the subset_columns\n",
    "    values found in the right_df. Writes the filtered rows to a new CSV file.\n",
    "    Includes a progress bar to track processing progress.\n",
    "\n",
    "    Parameters:\n",
    "    - large_csv_path (str): Path to the large CSV file.\n",
    "    - right_df (pd.DataFrame): Dataframe containing the filter criteria.\n",
    "    - subset_columns (list of str): Columns used for filtering.\n",
    "    - output_filtered_csv_path (str): Path to write the filtered CSV file.\n",
    "    \"\"\"\n",
    "    # Get the unique combinations of subset_columns in right_df\n",
    "    unique_combinations = right_df[subset_columns].drop_duplicates()\n",
    "\n",
    "    # Convert the unique combinations to a set of tuples for faster searching\n",
    "    unique_tuples = set([tuple(x) for x in unique_combinations.to_numpy()])\n",
    "\n",
    "    # Determine total number of rows for progress bar\n",
    "    total_rows = sum(1 for _ in open(large_csv_path, 'r', encoding='utf-8'))\n",
    "    chunksize = 100000  # Adjust based on your memory constraints\n",
    "    total_chunks = (total_rows // chunksize) + (1 if total_rows % chunksize else 0)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=total_chunks, desc='Processing CSV', unit='chunk')\n",
    "\n",
    "    # Initialize a DataFrame to hold chunks that pass the filter\n",
    "    filtered_chunks = []\n",
    "\n",
    "    # Read the large CSV in chunks\n",
    "    for chunk in pd.read_csv(large_csv_path, chunksize=chunksize):\n",
    "        # Filter the chunk\n",
    "        filtered_chunk = chunk[chunk.apply(lambda x: (x[subset_columns[0]], x[subset_columns[1]]) in unique_tuples, axis=1)]\n",
    "        \n",
    "        # If the filtered chunk is not empty, add it to the list\n",
    "        if not filtered_chunk.empty:\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Concatenate all filtered chunks and write to the output CSV\n",
    "    if filtered_chunks:\n",
    "        filtered_df = pd.concat(filtered_chunks)\n",
    "        filtered_df.to_csv(output_filtered_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"No matching rows found in the large CSV.\")\n",
    "\n",
    "# Example usage\n",
    "\n",
    "output_filtered_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectanc_polygons.csv'\n",
    "\n",
    "filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "568ea7cb-f5e1-4cb6-ae07-f74d78f5c455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pixel_Row  Pixel_Col                                GlobalID  \\\n",
      "0         1222        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "1         1222        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "2         1223        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "3         1223        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "4         1224        558  {54D1A59F-2B5C-4D76-913A-690314E314A7}   \n",
      "..         ...        ...                                     ...   \n",
      "492        808        548  {C3209290-3F79-4268-AE0F-4BE7F43E1C71}   \n",
      "493        792        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "494        792        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "495        793        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "496        793        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "\n",
      "                 CreationDate                     Creator  \\\n",
      "0   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "1   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "2   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "3   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "4   2023-06-20 19:28:37+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "..                        ...                         ...   \n",
      "492 2023-06-20 21:04:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "493 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "494 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "495 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "496 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "\n",
      "                      EditDate                      Editor description_notes  \\\n",
      "0    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "1    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "2    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "3    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "4    2023-06-20T19:28:37+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "..                         ...                         ...               ...   \n",
      "492  2023-06-20T21:04:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "493  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "494  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "495  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "496  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "\n",
      "      dbh tree_height  ... Landsat_8_band_3 Landsat_8_band_4 Landsat_8_band_5  \\\n",
      "0    None        None  ...       199.417260      2332.958904       678.536658   \n",
      "1    None        None  ...       162.287684      2109.051956       631.193919   \n",
      "2    None        None  ...       265.678544      2992.149994       850.298615   \n",
      "3    None        None  ...       237.316505      2784.182382       858.551085   \n",
      "4    None        None  ...       226.060785      2049.580959       800.528072   \n",
      "..    ...         ...  ...              ...              ...              ...   \n",
      "492  None        None  ...       341.122953      2771.077355       732.773500   \n",
      "493  None        None  ...       191.722239       540.843427       627.201560   \n",
      "494  None        None  ...       179.901588       622.884959       518.918862   \n",
      "495  None        None  ...       191.639779       759.106765       422.197877   \n",
      "496  None        None  ...       191.639786       759.107073       422.603923   \n",
      "\n",
      "    Landsat_8_band_6 Landsat_9_band_1 Landsat_9_band_2 Landsat_9_band_3  \\\n",
      "0         215.799060       154.560246       206.425940       199.417260   \n",
      "1         194.405484       131.443159       169.236334       162.287684   \n",
      "2         271.548456       189.652386       267.620950       265.678544   \n",
      "3         278.141484       171.409388       236.583686       237.316505   \n",
      "4         321.026416       168.612947       223.050103       226.060785   \n",
      "..               ...              ...              ...              ...   \n",
      "492       271.937625       252.914939       388.071835       341.122953   \n",
      "493       415.391701       164.353398       180.349969       191.722239   \n",
      "494       357.653013       186.867019       193.788724       179.901588   \n",
      "495       219.440107       163.033980       200.264439       191.639779   \n",
      "496       219.721380       163.033984       200.264447       191.639786   \n",
      "\n",
      "    Landsat_9_band_4 Landsat_9_band_5 Landsat_9_band_6  \n",
      "0        2332.958904       678.536658       215.799060  \n",
      "1        2109.051956       631.193919       194.405484  \n",
      "2        2992.149994       850.298615       271.548456  \n",
      "3        2784.182382       858.551085       278.141484  \n",
      "4        2049.580959       800.528072       321.026416  \n",
      "..               ...              ...              ...  \n",
      "492      2771.077355       732.773500       271.937625  \n",
      "493       540.843427       627.201560       415.391701  \n",
      "494       622.884959       518.918862       357.653013  \n",
      "495       759.106765       422.197877       219.440107  \n",
      "496       759.107073       422.603923       219.721380  \n",
      "\n",
      "[497 rows x 903 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_merge_csv_swapped(small_csv_path, other_df, merge_columns=['Pixel_Row', 'Pixel_Col'], how='inner'):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a DataFrame and merges it with another DataFrame based on specified columns.\n",
    "    In this version, the other DataFrame is treated as the left side of the merge.\n",
    "\n",
    "    Parameters:\n",
    "    - small_csv_path (str): Path to the CSV file to load.\n",
    "    - other_df (pd.DataFrame): The other DataFrame to merge with the loaded DataFrame, treated as the left DataFrame.\n",
    "    - merge_columns (list of str): Columns to merge on. Default is ['Pixel_Row', 'Pixel_Col'].\n",
    "    - how (str): Type of merge to be performed. Default is 'inner'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The resulting merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the small CSV file into a DataFrame\n",
    "    small_df = pd.read_csv(small_csv_path)\n",
    "\n",
    "    # Merge the other DataFrame with the loaded DataFrame, treating other_df as the left DataFrame\n",
    "    merged_df = pd.merge(other_df, small_df, on=merge_columns, how=how)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# Perform the merge\n",
    "merged_df = load_and_merge_csv_swapped('NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectanc_polygons.csv', df_polygons)\n",
    "\n",
    "# Optionally, you might want to view or save the resulting DataFrame\n",
    "print(merged_df)\n",
    "# merged_df.to_csv('path/to/your/merged_result.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macrosystems",
   "language": "python",
   "name": "macrosystems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
