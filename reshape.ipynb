{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c501955a-4e56-40df-93e4-346c6e5ad935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import spectral_unmixing_tools as el_spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f68d3-0ce8-47d8-9b10-1b2738833b02",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning process, total rows: 11547522, chunk size: 100000, total chunks: 116\n",
      "Processed and wrote chunk 1/116 to CSV.\n",
      "Processed and wrote chunk 2/116 to CSV.\n",
      "Processed and wrote chunk 3/116 to CSV.\n",
      "Processed and wrote chunk 4/116 to CSV.\n",
      "Processed and wrote chunk 5/116 to CSV.\n",
      "Processed and wrote chunk 6/116 to CSV.\n",
      "Processed and wrote chunk 7/116 to CSV.\n",
      "Processed and wrote chunk 8/116 to CSV.\n",
      "Processed and wrote chunk 9/116 to CSV.\n",
      "Processed and wrote chunk 10/116 to CSV.\n",
      "Processed and wrote chunk 11/116 to CSV.\n",
      "Processed and wrote chunk 12/116 to CSV.\n",
      "Processed and wrote chunk 13/116 to CSV.\n",
      "Processed and wrote chunk 14/116 to CSV.\n",
      "Processed and wrote chunk 15/116 to CSV.\n",
      "Processed and wrote chunk 16/116 to CSV.\n",
      "Processed and wrote chunk 17/116 to CSV.\n",
      "Processed and wrote chunk 18/116 to CSV.\n",
      "Processed and wrote chunk 19/116 to CSV.\n",
      "Processed and wrote chunk 20/116 to CSV.\n",
      "Processed and wrote chunk 21/116 to CSV.\n",
      "Processed and wrote chunk 22/116 to CSV.\n",
      "Processed and wrote chunk 23/116 to CSV.\n",
      "Processed and wrote chunk 24/116 to CSV.\n",
      "Processed and wrote chunk 25/116 to CSV.\n",
      "Processed and wrote chunk 26/116 to CSV.\n",
      "Processed and wrote chunk 27/116 to CSV.\n",
      "Processed and wrote chunk 28/116 to CSV.\n",
      "Processed and wrote chunk 29/116 to CSV.\n",
      "Processed and wrote chunk 30/116 to CSV.\n",
      "Processed and wrote chunk 31/116 to CSV.\n",
      "Processed and wrote chunk 32/116 to CSV.\n",
      "Processed and wrote chunk 33/116 to CSV.\n",
      "Processed and wrote chunk 34/116 to CSV.\n",
      "Processed and wrote chunk 35/116 to CSV.\n",
      "Processed and wrote chunk 36/116 to CSV.\n",
      "Processed and wrote chunk 37/116 to CSV.\n",
      "Processed and wrote chunk 38/116 to CSV.\n",
      "Processed and wrote chunk 39/116 to CSV.\n",
      "Processed and wrote chunk 40/116 to CSV.\n",
      "Processed and wrote chunk 41/116 to CSV.\n",
      "Processed and wrote chunk 42/116 to CSV.\n",
      "Processed and wrote chunk 43/116 to CSV.\n",
      "Processed and wrote chunk 44/116 to CSV.\n",
      "Processed and wrote chunk 45/116 to CSV.\n",
      "Processed and wrote chunk 46/116 to CSV.\n",
      "Processed and wrote chunk 47/116 to CSV.\n",
      "Processed and wrote chunk 48/116 to CSV.\n",
      "Processed and wrote chunk 49/116 to CSV.\n",
      "Processed and wrote chunk 50/116 to CSV.\n",
      "Processed and wrote chunk 51/116 to CSV.\n",
      "Processed and wrote chunk 52/116 to CSV.\n",
      "Processed and wrote chunk 53/116 to CSV.\n",
      "Processed and wrote chunk 54/116 to CSV.\n",
      "Processed and wrote chunk 55/116 to CSV.\n",
      "Processed and wrote chunk 56/116 to CSV.\n",
      "Processed and wrote chunk 57/116 to CSV.\n",
      "Processed and wrote chunk 58/116 to CSV.\n",
      "Processed and wrote chunk 59/116 to CSV.\n",
      "Processed and wrote chunk 60/116 to CSV.\n",
      "Processed and wrote chunk 61/116 to CSV.\n",
      "Processed and wrote chunk 62/116 to CSV.\n",
      "Processed and wrote chunk 63/116 to CSV.\n",
      "Processed and wrote chunk 64/116 to CSV.\n",
      "Processed and wrote chunk 65/116 to CSV.\n",
      "Processed and wrote chunk 66/116 to CSV.\n",
      "Processed and wrote chunk 67/116 to CSV.\n",
      "Processed and wrote chunk 68/116 to CSV.\n",
      "Processed and wrote chunk 69/116 to CSV.\n",
      "Processed and wrote chunk 70/116 to CSV.\n",
      "Processed and wrote chunk 71/116 to CSV.\n",
      "Processed and wrote chunk 72/116 to CSV.\n",
      "Processed and wrote chunk 73/116 to CSV.\n",
      "Processed and wrote chunk 74/116 to CSV.\n",
      "Processed and wrote chunk 75/116 to CSV.\n",
      "Processed and wrote chunk 76/116 to CSV.\n",
      "Processed and wrote chunk 77/116 to CSV.\n",
      "Processed and wrote chunk 78/116 to CSV.\n",
      "Processed and wrote chunk 79/116 to CSV.\n",
      "Processed and wrote chunk 80/116 to CSV.\n",
      "Processed and wrote chunk 81/116 to CSV.\n",
      "Processed and wrote chunk 82/116 to CSV.\n",
      "Processed and wrote chunk 83/116 to CSV.\n",
      "Processed and wrote chunk 84/116 to CSV.\n",
      "Processed and wrote chunk 85/116 to CSV.\n",
      "Processed and wrote chunk 86/116 to CSV.\n",
      "Processed and wrote chunk 87/116 to CSV.\n",
      "Processed and wrote chunk 88/116 to CSV.\n",
      "Processed and wrote chunk 89/116 to CSV.\n",
      "Processed and wrote chunk 90/116 to CSV.\n",
      "Processed and wrote chunk 91/116 to CSV.\n",
      "Processed and wrote chunk 92/116 to CSV.\n",
      "Processed and wrote chunk 93/116 to CSV.\n",
      "Processed and wrote chunk 94/116 to CSV.\n",
      "Processed and wrote chunk 95/116 to CSV.\n",
      "Processed and wrote chunk 96/116 to CSV.\n",
      "Processed and wrote chunk 97/116 to CSV.\n",
      "Processed and wrote chunk 98/116 to CSV.\n",
      "Processed and wrote chunk 99/116 to CSV.\n",
      "Processed and wrote chunk 100/116 to CSV.\n",
      "Processed and wrote chunk 101/116 to CSV.\n",
      "Processed and wrote chunk 102/116 to CSV.\n",
      "Processed and wrote chunk 103/116 to CSV.\n",
      "Processed and wrote chunk 104/116 to CSV.\n",
      "Processed and wrote chunk 105/116 to CSV.\n",
      "Processed and wrote chunk 106/116 to CSV.\n",
      "Processed and wrote chunk 107/116 to CSV.\n",
      "Processed and wrote chunk 108/116 to CSV.\n",
      "Processed and wrote chunk 109/116 to CSV.\n",
      "Processed and wrote chunk 110/116 to CSV.\n",
      "Processed and wrote chunk 111/116 to CSV.\n",
      "Processed and wrote chunk 112/116 to CSV.\n",
      "Processed and wrote chunk 113/116 to CSV.\n",
      "Processed and wrote chunk 114/116 to CSV.\n",
      "Processed and wrote chunk 115/116 to CSV.\n",
      "Processed and wrote chunk 116/116 to CSV.\n",
      "Cleaning process completed and data written to CSV.\n",
      "Processed and cleaned data saved to NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectance_spectral_data_all_sensors.csv\n",
      "Finished processing for directory: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance\n",
      "Starting cleaning process, total rows: 11483278, chunk size: 100000, total chunks: 115\n",
      "Processed and wrote chunk 1/115 to CSV.\n",
      "Processed and wrote chunk 2/115 to CSV.\n",
      "Processed and wrote chunk 3/115 to CSV.\n",
      "Processed and wrote chunk 4/115 to CSV.\n",
      "Processed and wrote chunk 5/115 to CSV.\n",
      "Processed and wrote chunk 6/115 to CSV.\n",
      "Processed and wrote chunk 7/115 to CSV.\n",
      "Processed and wrote chunk 8/115 to CSV.\n",
      "Processed and wrote chunk 9/115 to CSV.\n",
      "Processed and wrote chunk 10/115 to CSV.\n",
      "Processed and wrote chunk 11/115 to CSV.\n",
      "Processed and wrote chunk 12/115 to CSV.\n",
      "Processed and wrote chunk 13/115 to CSV.\n",
      "Processed and wrote chunk 14/115 to CSV.\n",
      "Processed and wrote chunk 15/115 to CSV.\n",
      "Processed and wrote chunk 16/115 to CSV.\n",
      "Processed and wrote chunk 17/115 to CSV.\n",
      "Processed and wrote chunk 18/115 to CSV.\n",
      "Processed and wrote chunk 19/115 to CSV.\n",
      "Processed and wrote chunk 20/115 to CSV.\n",
      "Processed and wrote chunk 21/115 to CSV.\n",
      "Processed and wrote chunk 22/115 to CSV.\n",
      "Processed and wrote chunk 23/115 to CSV.\n",
      "Processed and wrote chunk 24/115 to CSV.\n",
      "Processed and wrote chunk 25/115 to CSV.\n",
      "Processed and wrote chunk 26/115 to CSV.\n",
      "Processed and wrote chunk 27/115 to CSV.\n",
      "Processed and wrote chunk 28/115 to CSV.\n",
      "Processed and wrote chunk 29/115 to CSV.\n",
      "Processed and wrote chunk 30/115 to CSV.\n",
      "Processed and wrote chunk 31/115 to CSV.\n",
      "Processed and wrote chunk 32/115 to CSV.\n",
      "Processed and wrote chunk 33/115 to CSV.\n",
      "Processed and wrote chunk 34/115 to CSV.\n",
      "Processed and wrote chunk 35/115 to CSV.\n",
      "Processed and wrote chunk 36/115 to CSV.\n",
      "Processed and wrote chunk 37/115 to CSV.\n",
      "Processed and wrote chunk 38/115 to CSV.\n",
      "Processed and wrote chunk 39/115 to CSV.\n",
      "Processed and wrote chunk 40/115 to CSV.\n",
      "Processed and wrote chunk 41/115 to CSV.\n",
      "Processed and wrote chunk 42/115 to CSV.\n",
      "Processed and wrote chunk 43/115 to CSV.\n",
      "Processed and wrote chunk 44/115 to CSV.\n",
      "Processed and wrote chunk 45/115 to CSV.\n",
      "Processed and wrote chunk 46/115 to CSV.\n",
      "Processed and wrote chunk 47/115 to CSV.\n",
      "Processed and wrote chunk 48/115 to CSV.\n",
      "Processed and wrote chunk 49/115 to CSV.\n",
      "Processed and wrote chunk 50/115 to CSV.\n",
      "Processed and wrote chunk 51/115 to CSV.\n",
      "Processed and wrote chunk 52/115 to CSV.\n",
      "Processed and wrote chunk 53/115 to CSV.\n",
      "Processed and wrote chunk 54/115 to CSV.\n",
      "Processed and wrote chunk 55/115 to CSV.\n",
      "Processed and wrote chunk 56/115 to CSV.\n",
      "Processed and wrote chunk 57/115 to CSV.\n",
      "Processed and wrote chunk 58/115 to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "parent_directory = \"NIWOT_calibration_flight_08_2020\"  \n",
    "el_spectral.process_all_subdirectories(parent_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5c38436-e4bf-4a83-82ed-fdb1ec79276b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/earthlab/cross-sensor-cal.git\n",
      "  Cloning https://github.com/earthlab/cross-sensor-cal.git to /tmp/pip-req-build-3mcb9iw1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/earthlab/cross-sensor-cal.git /tmp/pip-req-build-3mcb9iw1\n",
      "  Resolved https://github.com/earthlab/cross-sensor-cal.git to commit 8717d6b99deb35c43dc57fc2bec4882c2f254814\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.26.3)\n",
      "Requirement already satisfied: spectral>=0.22 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.23.1)\n",
      "Requirement already satisfied: geopandas>=0.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (0.14.2)\n",
      "Requirement already satisfied: rasterio>=1.1.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.3.8)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.23.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (1.2.2)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (3.9.0)\n",
      "Requirement already satisfied: requests>=2.24.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.31.0)\n",
      "Requirement already satisfied: ray>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from EarthLabSpectral==0.1) (2.8.0)\n",
      "Requirement already satisfied: fiona>=1.8.21 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (1.9.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (23.2)\n",
      "Requirement already satisfied: pyproj>=3.3.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (3.6.0)\n",
      "Requirement already satisfied: shapely>=1.8.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from geopandas>=0.8.0->EarthLabSpectral==0.1) (2.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from matplotlib>=3.2.2->EarthLabSpectral==0.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from pandas>=1.0.5->EarthLabSpectral==0.1) (2023.3)\n",
      "Requirement already satisfied: affine in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2.4.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (2023.7.22)\n",
      "Requirement already satisfied: click>=4.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (8.1.3)\n",
      "Requirement already satisfied: cligj>=0.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (0.7.2)\n",
      "Requirement already satisfied: snuggs>=1.4.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.4.7)\n",
      "Requirement already satisfied: click-plugins in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from rasterio>=1.1.5->EarthLabSpectral==0.1) (69.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (3.12.2)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.0.5)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (4.23.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (6.0)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from ray>=1.0.0->EarthLabSpectral==0.1) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from requests>=2.24.0->EarthLabSpectral==0.1) (2.0.7)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.11.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from scikit-learn>=0.23.1->EarthLabSpectral==0.1) (3.1.0)\n",
      "Requirement already satisfied: six in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from fiona>=1.8.21->geopandas>=0.8.0->EarthLabSpectral==0.1) (1.16.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.32.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from jsonschema->ray>=1.0.0->EarthLabSpectral==0.1) (0.17.1)\n",
      "Building wheels for collected packages: EarthLabSpectral\n",
      "  Building wheel for EarthLabSpectral (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for EarthLabSpectral: filename=EarthLabSpectral-0.1-py3-none-any.whl size=1417 sha256=bebfc882dd13f60e077c2b288b2ae7dd343eae9db8bd57580f13cd335263fe64\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e_8ou5jp/wheels/d4/97/e8/c93ae5a8364c661ba5327b524ebe4ee01e41cfe582ac66f3d0\n",
      "Successfully built EarthLabSpectral\n",
      "Installing collected packages: EarthLabSpectral\n",
      "Successfully installed EarthLabSpectral-0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install git+https://github.com/earthlab/cross-sensor-cal.git\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c5d65-796e-43a7-b464-6d3007d7210c",
   "metadata": {},
   "source": [
    "## Extract spectra as array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d381f8-9d78-4d4a-be6c-0fc5ee0262e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None  # This will hold the raster data array\n",
    "        self.file_type = \"envi\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the raster data from the file_path into self.data\"\"\"\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            self.data = src.read()  # Read all bands\n",
    "\n",
    "    def get_chunk_from_extent(self, corrections=[], resample=False):\n",
    "        self.load_data()  # Ensure data is loaded\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            bounds = src.bounds\n",
    "            width, height = src.width, src.height\n",
    "            col_start, line_start = 0, 0\n",
    "            col_end, line_end = width, height\n",
    "\n",
    "            # Assuming self.data is a 3D numpy array with dimensions [bands, rows, cols]\n",
    "            chunk = self.data[:, line_start:line_end, col_start:col_end]\n",
    "\n",
    "            # Apply any processing to chunk here...\n",
    "            # For example, to demonstrate, flip chunk vertically\n",
    "            chunk = np.flip(chunk, axis=1)\n",
    "\n",
    "            return chunk\n",
    "\n",
    "def load_and_combine_rasters(raster_paths):\n",
    "    \"\"\"\n",
    "    Loads and combines raster data from a list of file paths.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for path in raster_paths:\n",
    "        processor = ENVIProcessor(path)\n",
    "        chunk = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    combined_array = np.concatenate(chunks, axis=0)  # Combine along the first axis (bands)\n",
    "    return combined_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b6b81-1c4d-48d5-bfd0-b509b025867d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided raster paths\n",
    "raster_paths = [\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_5_TM.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_7_ETMplus.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_8_OLI.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi_resample_Landsat_9_OLI-2.img\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\",\n",
    "    \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectance\"\n",
    "]\n",
    "\n",
    "# Ensure paths are updated correctly, especially for Landsat 7, 8, and 9 as provided paths are duplicates of Landsat 5\n",
    "combined_array = load_and_combine_rasters(raster_paths)\n",
    "combined_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58be238-60a4-43d1-ad30-a893a3c09211",
   "metadata": {},
   "source": [
    "## flatten array into a 2D df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e909a413-bff9-4a41-b7f0-5b08b0fc64b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def process_and_flatten_array(array, landsat_versions=[5, 7, 8, 9], bands_per_landsat=6):\n",
    "    \"\"\"\n",
    "    Processes a 3D numpy array to a DataFrame, renames columns, and adds Pixel_id.\n",
    "    \n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    - landsat_versions: A list of Landsat versions to use for naming.\n",
    "    - bands_per_landsat: Number of bands per Landsat version.\n",
    "    \n",
    "    Returns:\n",
    "    - A pandas DataFrame with processed and renamed columns and added Pixel_id.\n",
    "    \"\"\"\n",
    "    if len(array.shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3-dimensional.\")\n",
    "    \n",
    "    # Flatten the array\n",
    "    bands, rows, cols = array.shape\n",
    "    reshaped_array = array.reshape(bands, -1).T  # Transpose to make bands as columns\n",
    "    pixel_indices = np.indices((rows, cols)).reshape(2, -1).T  # Row and col indices\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(reshaped_array, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "    df.insert(0, 'Pixel_Col', pixel_indices[:, 1])\n",
    "    df.insert(0, 'Pixel_Row', pixel_indices[:, 0])\n",
    "    df.insert(0, 'Pixel_id', np.arange(len(df)))\n",
    "\n",
    "    # Renaming columns\n",
    "    total_bands = bands\n",
    "    original_and_corrected_bands = total_bands - bands_per_landsat * len(landsat_versions)\n",
    "    band_per_version = original_and_corrected_bands // 2  # Assuming equal original and corrected bands\n",
    "    \n",
    "    new_names = ([f\"Original_band_{i}\" for i in range(1, band_per_version + 1)] +\n",
    "                 [f\"Corrected_band_{i}\" for i in range(1, band_per_version + 1)])\n",
    "    \n",
    "    for version in landsat_versions:\n",
    "        new_names.extend([f\"Landsat_{version}_band_{i}\" for i in range(1, bands_per_landsat + 1)])\n",
    "    \n",
    "    # Apply new column names for band columns\n",
    "    df.columns = ['Pixel_id', 'Pixel_Row', 'Pixel_Col'] + new_names\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d7b76a-4965-48f5-881e-e2cc2f1e1296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with a hypothetical array shape\n",
    "# Assume 'combined_array' is your loaded and combined 3D numpy array\n",
    "# combined_array = np.random.rand(426*2 + 4*6, 100, 100)  # Example array\n",
    "\n",
    "# Process and flatten the array\n",
    "df_processed = process_and_flatten_array(combined_array)\n",
    "\n",
    "# Now 'df_processed' is the DataFrame with renamed columns and added Pixel_id\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e78bb-a2b1-4a93-9247-5132efa301b2",
   "metadata": {},
   "source": [
    "## Delete rows with NA as it writes to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56916c3a-60fe-4309-bffc-f8712256d5de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_data_and_write_to_csv(df, output_csv_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Cleans the DataFrame in chunks to minimize memory usage and writes the cleaned\n",
    "    chunks directly to a CSV file to avoid memory overload. It replaces values approximately\n",
    "    equal to -9999 (within a tolerance of 1) with NaN in columns not starting with 'Pixel', and\n",
    "    then drops rows where all such columns are NaN.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: pandas DataFrame to clean.\n",
    "    - output_csv_path: Path to the output CSV file.\n",
    "    - chunk_size: Number of rows in each chunk.\n",
    "    \n",
    "    Returns:\n",
    "    - None. The cleaned data is written directly to the specified CSV file.\n",
    "    \"\"\"\n",
    "    total_rows = df.shape[0]\n",
    "    num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size else 0)\n",
    "\n",
    "    print(f\"Starting cleaning process, total rows: {total_rows}, chunk size: {chunk_size}, total chunks: {num_chunks}\")\n",
    "\n",
    "    # Initialize CSV file writing\n",
    "    first_chunk = True\n",
    "\n",
    "    for i, start_row in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk = df.iloc[start_row:start_row + chunk_size].copy()\n",
    "\n",
    "        # Replace values close to -9999 with NaN\n",
    "        for col in chunk.columns:\n",
    "            if not col.startswith('Pixel'):\n",
    "                chunk[col] = np.where(np.isclose(chunk[col], -9999, atol=1), np.nan, chunk[col])\n",
    "\n",
    "        # Drop rows where all non-'Pixel' columns are NaN\n",
    "        non_pixel_columns = [col for col in chunk.columns if not col.startswith('Pixel')]\n",
    "        chunk.dropna(subset=non_pixel_columns, how='all', inplace=True)\n",
    "        \n",
    "        # Write processed chunk to CSV\n",
    "        if first_chunk:\n",
    "            chunk.to_csv(output_csv_path, mode='w', header=True, index=False)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            chunk.to_csv(output_csv_path, mode='a', header=False, index=False)\n",
    "        \n",
    "        print(f\"Processed and wrote chunk {i+1}/{num_chunks} to CSV.\")\n",
    "\n",
    "    print(\"Cleaning process completed and data written to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57ef4c-936c-4eb4-a9ab-4c0957ce3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the output CSV path\n",
    "output_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance_active_pixels.csv'\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "clean_data_and_write_to_csv(df_processed, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf6d650-7c31-4100-bd3e-4a06a0af777d",
   "metadata": {},
   "source": [
    "## Exract by polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1241d-aaa5-4f96-a8a6-6863e75a6693",
   "metadata": {},
   "source": [
    "### Get col and row index and polygon features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551e6f3c-68ea-446a-806d-d789ab16d2fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>EditDate</th>\n",
       "      <th>Editor</th>\n",
       "      <th>description_notes</th>\n",
       "      <th>dbh</th>\n",
       "      <th>tree_height</th>\n",
       "      <th>...</th>\n",
       "      <th>og_flight_date</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>collector_name</th>\n",
       "      <th>plot</th>\n",
       "      <th>location</th>\n",
       "      <th>woody_shrub_height</th>\n",
       "      <th>imagery</th>\n",
       "      <th>combined_all_category_species</th>\n",
       "      <th>area_m</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1222</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1222</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1223</td>\n",
       "      <td>554</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1223</td>\n",
       "      <td>555</td>\n",
       "      <td>{E9346797-777A-4D43-BD01-02A511C57DAA}</td>\n",
       "      <td>2023-06-20 19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:27:29+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>7.004149</td>\n",
       "      <td>POLYGON ((452521.07602110645 4435015.481140467...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1224</td>\n",
       "      <td>558</td>\n",
       "      <td>{54D1A59F-2B5C-4D76-913A-690314E314A7}</td>\n",
       "      <td>2023-06-20 19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T19:28:37+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 19:27:01+00:00</td>\n",
       "      <td>2023-06-20 19:26:55+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Non-vegetated &amp; dead_Rock__</td>\n",
       "      <td>6.064980</td>\n",
       "      <td>POLYGON ((452524.7605621438 4435014.4842204545...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>808</td>\n",
       "      <td>548</td>\n",
       "      <td>{C3209290-3F79-4268-AE0F-4BE7F43E1C71}</td>\n",
       "      <td>2023-06-20 21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:04:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:03:43+00:00</td>\n",
       "      <td>2023-06-20 21:03:34+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>23.032479</td>\n",
       "      <td>POLYGON ((452510.4188020002 4435431.162891659,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>792</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>792</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>793</td>\n",
       "      <td>550</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>793</td>\n",
       "      <td>551</td>\n",
       "      <td>{56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}</td>\n",
       "      <td>2023-06-20 21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-20T21:05:08+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-20 21:04:49+00:00</td>\n",
       "      <td>2023-06-20 21:04:42+00:00</td>\n",
       "      <td>Katie</td>\n",
       "      <td>0</td>\n",
       "      <td>Brainard</td>\n",
       "      <td>10cm to 1m</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Woody shrub_Woody shrub - Broadleaf__Unidentified</td>\n",
       "      <td>7.422119</td>\n",
       "      <td>POLYGON ((452516.90320151206 4435445.897912507...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>497 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pixel_Row  Pixel_Col                                GlobalID  \\\n",
       "0         1222        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "1         1222        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "2         1223        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "3         1223        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
       "4         1224        558  {54D1A59F-2B5C-4D76-913A-690314E314A7}   \n",
       "..         ...        ...                                     ...   \n",
       "492        808        548  {C3209290-3F79-4268-AE0F-4BE7F43E1C71}   \n",
       "493        792        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "494        792        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "495        793        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "496        793        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
       "\n",
       "                 CreationDate                     Creator  \\\n",
       "0   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "1   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "2   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "4   2023-06-20 19:28:37+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "..                        ...                         ...   \n",
       "492 2023-06-20 21:04:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "493 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "494 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "495 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "496 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "\n",
       "                      EditDate                      Editor description_notes  \\\n",
       "0    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "1    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "2    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "4    2023-06-20T19:28:37+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "..                         ...                         ...               ...   \n",
       "492  2023-06-20T21:04:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "493  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "494  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "495  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "496  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "\n",
       "      dbh tree_height  ...            og_flight_date  \\\n",
       "0    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "1    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "2    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "3    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "4    None        None  ... 2023-06-20 19:27:01+00:00   \n",
       "..    ...         ...  ...                       ...   \n",
       "492  None        None  ... 2023-06-20 21:03:43+00:00   \n",
       "493  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "494  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "495  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "496  None        None  ... 2023-06-20 21:04:49+00:00   \n",
       "\n",
       "              collection_date collector_name plot  location  \\\n",
       "0   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "1   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "2   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "3   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "4   2023-06-20 19:26:55+00:00          Tyler    0  Brainard   \n",
       "..                        ...            ...  ...       ...   \n",
       "492 2023-06-20 21:03:34+00:00          Katie    0  Brainard   \n",
       "493 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "494 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "495 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "496 2023-06-20 21:04:42+00:00          Katie    0  Brainard   \n",
       "\n",
       "    woody_shrub_height imagery  \\\n",
       "0                 None     AOP   \n",
       "1                 None     AOP   \n",
       "2                 None     AOP   \n",
       "3                 None     AOP   \n",
       "4                 None     AOP   \n",
       "..                 ...     ...   \n",
       "492         10cm to 1m     AOP   \n",
       "493         10cm to 1m     AOP   \n",
       "494         10cm to 1m     AOP   \n",
       "495         10cm to 1m     AOP   \n",
       "496         10cm to 1m     AOP   \n",
       "\n",
       "                         combined_all_category_species     area_m  \\\n",
       "0                        Evergreen___Picea engelmannii   7.004149   \n",
       "1                        Evergreen___Picea engelmannii   7.004149   \n",
       "2                        Evergreen___Picea engelmannii   7.004149   \n",
       "3                        Evergreen___Picea engelmannii   7.004149   \n",
       "4                          Non-vegetated & dead_Rock__   6.064980   \n",
       "..                                                 ...        ...   \n",
       "492  Woody shrub_Woody shrub - Broadleaf__Unidentified  23.032479   \n",
       "493  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "494  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "495  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "496  Woody shrub_Woody shrub - Broadleaf__Unidentified   7.422119   \n",
       "\n",
       "                                              geometry  \n",
       "0    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "1    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "2    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "3    POLYGON ((452521.07602110645 4435015.481140467...  \n",
       "4    POLYGON ((452524.7605621438 4435014.4842204545...  \n",
       "..                                                 ...  \n",
       "492  POLYGON ((452510.4188020002 4435431.162891659,...  \n",
       "493  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "494  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "495  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "496  POLYGON ((452516.90320151206 4435445.897912507...  \n",
       "\n",
       "[497 rows x 26 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, raster_path, polygons_path):\n",
    "        self.raster_path = raster_path\n",
    "        self.polygons_path = polygons_path\n",
    "        self.polygons = None\n",
    "        self.raster_meta = None\n",
    "        \n",
    "    def load_polygons(self):\n",
    "        \"\"\"Loads the polygons and ensures they are in the same CRS as the raster.\"\"\"\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            self.raster_meta = src.meta\n",
    "            self.polygons = gpd.read_file(self.polygons_path)\n",
    "            self.polygons = self.polygons.to_crs(src.crs)\n",
    "    \n",
    "    def extract_data_by_polygons(self):\n",
    "        \"\"\"Extracts the row and col indices from the raster for each polygon and appends all attributes from the polygons.\"\"\"\n",
    "        self.load_polygons()  # Load polygons and ensure CRS match\n",
    "        \n",
    "        all_data = []\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            raster_bounds = src.bounds\n",
    "            raster_box = box(*raster_bounds)\n",
    "            \n",
    "            for _, poly in self.polygons.iterrows():\n",
    "                geom = poly.geometry\n",
    "                # Skip invalid or empty geometries or those that do not intersect with the raster\n",
    "                if geom is None or geom.is_empty or not geom.intersects(raster_box):\n",
    "                    continue\n",
    "                \n",
    "                window = from_bounds(*geom.bounds, transform=src.transform)\n",
    "                # Skip windows that are completely outside the raster bounds\n",
    "                if window.width <= 0 or window.height <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert window offsets to integers\n",
    "                row_off = int(window.row_off)\n",
    "                col_off = int(window.col_off)\n",
    "                # Extract the rows and cols from the window\n",
    "                rows = range(row_off, row_off + int(window.height))\n",
    "                cols = range(col_off, col_off + int(window.width))\n",
    "                \n",
    "                # Collect all attributes from the polygon\n",
    "                attributes = poly.to_dict()\n",
    "                \n",
    "                # Append the rows and cols along with the polygon attributes to the data list\n",
    "                for row in rows:\n",
    "                    for col in cols:\n",
    "                        pixel_data = {\n",
    "                            'Pixel_Row': row,\n",
    "                            'Pixel_Col': col,\n",
    "                            **attributes  # This adds all polygon attributes\n",
    "                        }\n",
    "                        all_data.append(pixel_data)\n",
    "        \n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "\n",
    "gpkg_path = 'Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg'\n",
    "existing_raster_path = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectanceNEON_D13_NIWO_DP1_20200801_161441_reflectance__envi\"\n",
    "processor = ENVIProcessor(existing_raster_path, gpkg_path)\n",
    "df_polygons = processor.extract_data_by_polygons()\n",
    "df_polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f0347e-1b3e-4582-89d7-966af6c14aaf",
   "metadata": {},
   "source": [
    "## Extract rows from csv that match col and row pair and write to new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a55317-a896-4b16-86cf-5b48a22340c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path):\n",
    "    \"\"\"\n",
    "    Filters rows in a large CSV file, keeping only those that match the subset_columns\n",
    "    values found in the right_df. Writes the filtered rows to a new CSV file.\n",
    "    Includes a progress bar to track processing progress.\n",
    "\n",
    "    Parameters:\n",
    "    - large_csv_path (str): Path to the large CSV file.\n",
    "    - right_df (pd.DataFrame): Dataframe containing the filter criteria.\n",
    "    - subset_columns (list of str): Columns used for filtering.\n",
    "    - output_filtered_csv_path (str): Path to write the filtered CSV file.\n",
    "    \"\"\"\n",
    "    # Get the unique combinations of subset_columns in right_df\n",
    "    unique_combinations = right_df[subset_columns].drop_duplicates()\n",
    "\n",
    "    # Convert the unique combinations to a set of tuples for faster searching\n",
    "    unique_tuples = set([tuple(x) for x in unique_combinations.to_numpy()])\n",
    "\n",
    "    # Determine total number of rows for progress bar\n",
    "    total_rows = sum(1 for _ in open(large_csv_path, 'r', encoding='utf-8'))\n",
    "    chunksize = 100000  # Adjust based on your memory constraints\n",
    "    total_chunks = (total_rows // chunksize) + (1 if total_rows % chunksize else 0)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=total_chunks, desc='Processing CSV', unit='chunk')\n",
    "\n",
    "    # Initialize a DataFrame to hold chunks that pass the filter\n",
    "    filtered_chunks = []\n",
    "\n",
    "    # Read the large CSV in chunks\n",
    "    for chunk in pd.read_csv(large_csv_path, chunksize=chunksize):\n",
    "        # Filter the chunk\n",
    "        filtered_chunk = chunk[chunk.apply(lambda x: (x[subset_columns[0]], x[subset_columns[1]]) in unique_tuples, axis=1)]\n",
    "        \n",
    "        # If the filtered chunk is not empty, add it to the list\n",
    "        if not filtered_chunk.empty:\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Concatenate all filtered chunks and write to the output CSV\n",
    "    if filtered_chunks:\n",
    "        filtered_df = pd.concat(filtered_chunks)\n",
    "        filtered_df.to_csv(output_filtered_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"No matching rows found in the large CSV.\")\n",
    "\n",
    "# Example usage\n",
    "large_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance/NEON_D13_NIWO_DP1_20200801_161441_reflectance_spectral_data_all_sensors.csv'\n",
    "output_filtered_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance_polygons.csv'\n",
    "right_df = df_polygons\n",
    "filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns=['Pixel_Row', 'Pixel_Col'], output_filtered_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd35c0-352c-4e82-a082-8b2cd0669f1b",
   "metadata": {},
   "source": [
    "### View merged polygon matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "568ea7cb-f5e1-4cb6-ae07-f74d78f5c455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pixel_Row  Pixel_Col                                GlobalID  \\\n",
      "0         1222        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "1         1222        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "2         1223        554  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "3         1223        555  {E9346797-777A-4D43-BD01-02A511C57DAA}   \n",
      "4         1224        558  {54D1A59F-2B5C-4D76-913A-690314E314A7}   \n",
      "..         ...        ...                                     ...   \n",
      "492        808        548  {C3209290-3F79-4268-AE0F-4BE7F43E1C71}   \n",
      "493        792        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "494        792        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "495        793        550  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "496        793        551  {56551F70-2BB6-4C7A-80A4-99CF53CD2EC3}   \n",
      "\n",
      "                 CreationDate                     Creator  \\\n",
      "0   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "1   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "2   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "3   2023-06-20 19:27:29+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "4   2023-06-20 19:28:37+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "..                        ...                         ...   \n",
      "492 2023-06-20 21:04:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "493 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "494 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "495 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "496 2023-06-20 21:05:08+00:00  Tyler.L.McIntosh_ucboulder   \n",
      "\n",
      "                      EditDate                      Editor description_notes  \\\n",
      "0    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "1    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "2    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "3    2023-06-20T19:27:29+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "4    2023-06-20T19:28:37+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "..                         ...                         ...               ...   \n",
      "492  2023-06-20T21:04:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "493  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "494  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "495  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "496  2023-06-20T21:05:08+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
      "\n",
      "      dbh tree_height  ... Landsat_8_band_3 Landsat_8_band_4 Landsat_8_band_5  \\\n",
      "0    None        None  ...       199.417260      2332.958904       678.536658   \n",
      "1    None        None  ...       162.287684      2109.051956       631.193919   \n",
      "2    None        None  ...       265.678544      2992.149994       850.298615   \n",
      "3    None        None  ...       237.316505      2784.182382       858.551085   \n",
      "4    None        None  ...       226.060785      2049.580959       800.528072   \n",
      "..    ...         ...  ...              ...              ...              ...   \n",
      "492  None        None  ...       341.122953      2771.077355       732.773500   \n",
      "493  None        None  ...       191.722239       540.843427       627.201560   \n",
      "494  None        None  ...       179.901588       622.884959       518.918862   \n",
      "495  None        None  ...       191.639779       759.106765       422.197877   \n",
      "496  None        None  ...       191.639786       759.107073       422.603923   \n",
      "\n",
      "    Landsat_8_band_6 Landsat_9_band_1 Landsat_9_band_2 Landsat_9_band_3  \\\n",
      "0         215.799060       154.560246       206.425940       199.417260   \n",
      "1         194.405484       131.443159       169.236334       162.287684   \n",
      "2         271.548456       189.652386       267.620950       265.678544   \n",
      "3         278.141484       171.409388       236.583686       237.316505   \n",
      "4         321.026416       168.612947       223.050103       226.060785   \n",
      "..               ...              ...              ...              ...   \n",
      "492       271.937625       252.914939       388.071835       341.122953   \n",
      "493       415.391701       164.353398       180.349969       191.722239   \n",
      "494       357.653013       186.867019       193.788724       179.901588   \n",
      "495       219.440107       163.033980       200.264439       191.639779   \n",
      "496       219.721380       163.033984       200.264447       191.639786   \n",
      "\n",
      "    Landsat_9_band_4 Landsat_9_band_5 Landsat_9_band_6  \n",
      "0        2332.958904       678.536658       215.799060  \n",
      "1        2109.051956       631.193919       194.405484  \n",
      "2        2992.149994       850.298615       271.548456  \n",
      "3        2784.182382       858.551085       278.141484  \n",
      "4        2049.580959       800.528072       321.026416  \n",
      "..               ...              ...              ...  \n",
      "492      2771.077355       732.773500       271.937625  \n",
      "493       540.843427       627.201560       415.391701  \n",
      "494       622.884959       518.918862       357.653013  \n",
      "495       759.106765       422.197877       219.440107  \n",
      "496       759.107073       422.603923       219.721380  \n",
      "\n",
      "[497 rows x 903 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_merge_csv_swapped(small_csv_path, other_df, merge_columns=['Pixel_Row', 'Pixel_Col'], how='inner'):\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a DataFrame and merges it with another DataFrame based on specified columns.\n",
    "    In this version, the other DataFrame is treated as the left side of the merge.\n",
    "\n",
    "    Parameters:\n",
    "    - small_csv_path (str): Path to the CSV file to load.\n",
    "    - other_df (pd.DataFrame): The other DataFrame to merge with the loaded DataFrame, treated as the left DataFrame.\n",
    "    - merge_columns (list of str): Columns to merge on. Default is ['Pixel_Row', 'Pixel_Col'].\n",
    "    - how (str): Type of merge to be performed. Default is 'inner'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The resulting merged DataFrame.\n",
    "    \"\"\"\n",
    "    # Load the small CSV file into a DataFrame\n",
    "    small_df = pd.read_csv(small_csv_path)\n",
    "\n",
    "    # Merge the other DataFrame with the loaded DataFrame, treating other_df as the left DataFrame\n",
    "    merged_df = pd.merge(other_df, small_df, on=merge_columns, how=how)\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "\n",
    "# Perform the merge\n",
    "merged_df = load_and_merge_csv_swapped('NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectanc_polygons.csv', df_polygons)\n",
    "\n",
    "# Optionally, you might want to view or save the resulting DataFrame\n",
    "print(merged_df)\n",
    "# merged_df.to_csv('path/to/your/merged_result.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1a238a-2cde-4d2a-835e-a72a276eaceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixel_Row</th>\n",
       "      <th>Pixel_Col</th>\n",
       "      <th>GlobalID</th>\n",
       "      <th>CreationDate</th>\n",
       "      <th>Creator</th>\n",
       "      <th>EditDate</th>\n",
       "      <th>Editor</th>\n",
       "      <th>description_notes</th>\n",
       "      <th>dbh</th>\n",
       "      <th>tree_height</th>\n",
       "      <th>...</th>\n",
       "      <th>og_flight_date</th>\n",
       "      <th>collection_date</th>\n",
       "      <th>collector_name</th>\n",
       "      <th>plot</th>\n",
       "      <th>location</th>\n",
       "      <th>woody_shrub_height</th>\n",
       "      <th>imagery</th>\n",
       "      <th>combined_all_category_species</th>\n",
       "      <th>area_m</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2575</td>\n",
       "      <td>338</td>\n",
       "      <td>{FDE6BEA3-8D85-4099-A8A9-63273619D2D0}</td>\n",
       "      <td>2023-06-07 15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-07T15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-07 15:51:50+00:00</td>\n",
       "      <td>2023-06-07 15:51:25+00:00</td>\n",
       "      <td>Ian</td>\n",
       "      <td>0</td>\n",
       "      <td>Highway 72 Curve</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Herbaceous_Mixed grass &amp; forb__</td>\n",
       "      <td>85.322800</td>\n",
       "      <td>POLYGON ((455051.63677666837 4433775.201219057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2575</td>\n",
       "      <td>339</td>\n",
       "      <td>{FDE6BEA3-8D85-4099-A8A9-63273619D2D0}</td>\n",
       "      <td>2023-06-07 15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-07T15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-07 15:51:50+00:00</td>\n",
       "      <td>2023-06-07 15:51:25+00:00</td>\n",
       "      <td>Ian</td>\n",
       "      <td>0</td>\n",
       "      <td>Highway 72 Curve</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Herbaceous_Mixed grass &amp; forb__</td>\n",
       "      <td>85.322800</td>\n",
       "      <td>POLYGON ((455051.63677666837 4433775.201219057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2575</td>\n",
       "      <td>340</td>\n",
       "      <td>{FDE6BEA3-8D85-4099-A8A9-63273619D2D0}</td>\n",
       "      <td>2023-06-07 15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-07T15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-07 15:51:50+00:00</td>\n",
       "      <td>2023-06-07 15:51:25+00:00</td>\n",
       "      <td>Ian</td>\n",
       "      <td>0</td>\n",
       "      <td>Highway 72 Curve</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Herbaceous_Mixed grass &amp; forb__</td>\n",
       "      <td>85.322800</td>\n",
       "      <td>POLYGON ((455051.63677666837 4433775.201219057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2575</td>\n",
       "      <td>341</td>\n",
       "      <td>{FDE6BEA3-8D85-4099-A8A9-63273619D2D0}</td>\n",
       "      <td>2023-06-07 15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-07T15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-07 15:51:50+00:00</td>\n",
       "      <td>2023-06-07 15:51:25+00:00</td>\n",
       "      <td>Ian</td>\n",
       "      <td>0</td>\n",
       "      <td>Highway 72 Curve</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Herbaceous_Mixed grass &amp; forb__</td>\n",
       "      <td>85.322800</td>\n",
       "      <td>POLYGON ((455051.63677666837 4433775.201219057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2575</td>\n",
       "      <td>342</td>\n",
       "      <td>{FDE6BEA3-8D85-4099-A8A9-63273619D2D0}</td>\n",
       "      <td>2023-06-07 15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-07T15:52:00+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-07 15:51:50+00:00</td>\n",
       "      <td>2023-06-07 15:51:25+00:00</td>\n",
       "      <td>Ian</td>\n",
       "      <td>0</td>\n",
       "      <td>Highway 72 Curve</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Herbaceous_Mixed grass &amp; forb__</td>\n",
       "      <td>85.322800</td>\n",
       "      <td>POLYGON ((455051.63677666837 4433775.201219057...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3702</th>\n",
       "      <td>5486</td>\n",
       "      <td>200</td>\n",
       "      <td>{DD78E6FF-DFD4-47AF-805B-B02342EA64B7}</td>\n",
       "      <td>2023-06-21 21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-21T21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-21 20:45:28+00:00</td>\n",
       "      <td>2023-06-21 20:45:21+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>MRS</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>16.380218</td>\n",
       "      <td>POLYGON ((454902.5548156673 4430862.772296763,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3703</th>\n",
       "      <td>5486</td>\n",
       "      <td>201</td>\n",
       "      <td>{DD78E6FF-DFD4-47AF-805B-B02342EA64B7}</td>\n",
       "      <td>2023-06-21 21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-21T21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-21 20:45:28+00:00</td>\n",
       "      <td>2023-06-21 20:45:21+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>MRS</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>16.380218</td>\n",
       "      <td>POLYGON ((454902.5548156673 4430862.772296763,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3704</th>\n",
       "      <td>5487</td>\n",
       "      <td>199</td>\n",
       "      <td>{DD78E6FF-DFD4-47AF-805B-B02342EA64B7}</td>\n",
       "      <td>2023-06-21 21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-21T21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-21 20:45:28+00:00</td>\n",
       "      <td>2023-06-21 20:45:21+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>MRS</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>16.380218</td>\n",
       "      <td>POLYGON ((454902.5548156673 4430862.772296763,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>5487</td>\n",
       "      <td>200</td>\n",
       "      <td>{DD78E6FF-DFD4-47AF-805B-B02342EA64B7}</td>\n",
       "      <td>2023-06-21 21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-21T21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-21 20:45:28+00:00</td>\n",
       "      <td>2023-06-21 20:45:21+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>MRS</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>16.380218</td>\n",
       "      <td>POLYGON ((454902.5548156673 4430862.772296763,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3706</th>\n",
       "      <td>5487</td>\n",
       "      <td>201</td>\n",
       "      <td>{DD78E6FF-DFD4-47AF-805B-B02342EA64B7}</td>\n",
       "      <td>2023-06-21 21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>2023-06-21T21:29:46+00:00</td>\n",
       "      <td>Tyler.L.McIntosh_ucboulder</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>2023-06-21 20:45:28+00:00</td>\n",
       "      <td>2023-06-21 20:45:21+00:00</td>\n",
       "      <td>Tyler</td>\n",
       "      <td>0</td>\n",
       "      <td>MRS</td>\n",
       "      <td>None</td>\n",
       "      <td>AOP</td>\n",
       "      <td>Evergreen___Picea engelmannii</td>\n",
       "      <td>16.380218</td>\n",
       "      <td>POLYGON ((454902.5548156673 4430862.772296763,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3707 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pixel_Row  Pixel_Col                                GlobalID  \\\n",
       "0          2575        338  {FDE6BEA3-8D85-4099-A8A9-63273619D2D0}   \n",
       "1          2575        339  {FDE6BEA3-8D85-4099-A8A9-63273619D2D0}   \n",
       "2          2575        340  {FDE6BEA3-8D85-4099-A8A9-63273619D2D0}   \n",
       "3          2575        341  {FDE6BEA3-8D85-4099-A8A9-63273619D2D0}   \n",
       "4          2575        342  {FDE6BEA3-8D85-4099-A8A9-63273619D2D0}   \n",
       "...         ...        ...                                     ...   \n",
       "3702       5486        200  {DD78E6FF-DFD4-47AF-805B-B02342EA64B7}   \n",
       "3703       5486        201  {DD78E6FF-DFD4-47AF-805B-B02342EA64B7}   \n",
       "3704       5487        199  {DD78E6FF-DFD4-47AF-805B-B02342EA64B7}   \n",
       "3705       5487        200  {DD78E6FF-DFD4-47AF-805B-B02342EA64B7}   \n",
       "3706       5487        201  {DD78E6FF-DFD4-47AF-805B-B02342EA64B7}   \n",
       "\n",
       "                  CreationDate                     Creator  \\\n",
       "0    2023-06-07 15:52:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "1    2023-06-07 15:52:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "2    2023-06-07 15:52:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3    2023-06-07 15:52:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "4    2023-06-07 15:52:00+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "...                        ...                         ...   \n",
       "3702 2023-06-21 21:29:46+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3703 2023-06-21 21:29:46+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3704 2023-06-21 21:29:46+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3705 2023-06-21 21:29:46+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "3706 2023-06-21 21:29:46+00:00  Tyler.L.McIntosh_ucboulder   \n",
       "\n",
       "                       EditDate                      Editor description_notes  \\\n",
       "0     2023-06-07T15:52:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "1     2023-06-07T15:52:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "2     2023-06-07T15:52:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3     2023-06-07T15:52:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "4     2023-06-07T15:52:00+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "...                         ...                         ...               ...   \n",
       "3702  2023-06-21T21:29:46+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3703  2023-06-21T21:29:46+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3704  2023-06-21T21:29:46+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3705  2023-06-21T21:29:46+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "3706  2023-06-21T21:29:46+00:00  Tyler.L.McIntosh_ucboulder              None   \n",
       "\n",
       "       dbh tree_height  ...            og_flight_date  \\\n",
       "0     None        None  ... 2023-06-07 15:51:50+00:00   \n",
       "1     None        None  ... 2023-06-07 15:51:50+00:00   \n",
       "2     None        None  ... 2023-06-07 15:51:50+00:00   \n",
       "3     None        None  ... 2023-06-07 15:51:50+00:00   \n",
       "4     None        None  ... 2023-06-07 15:51:50+00:00   \n",
       "...    ...         ...  ...                       ...   \n",
       "3702  None        None  ... 2023-06-21 20:45:28+00:00   \n",
       "3703  None        None  ... 2023-06-21 20:45:28+00:00   \n",
       "3704  None        None  ... 2023-06-21 20:45:28+00:00   \n",
       "3705  None        None  ... 2023-06-21 20:45:28+00:00   \n",
       "3706  None        None  ... 2023-06-21 20:45:28+00:00   \n",
       "\n",
       "               collection_date collector_name plot          location  \\\n",
       "0    2023-06-07 15:51:25+00:00            Ian    0  Highway 72 Curve   \n",
       "1    2023-06-07 15:51:25+00:00            Ian    0  Highway 72 Curve   \n",
       "2    2023-06-07 15:51:25+00:00            Ian    0  Highway 72 Curve   \n",
       "3    2023-06-07 15:51:25+00:00            Ian    0  Highway 72 Curve   \n",
       "4    2023-06-07 15:51:25+00:00            Ian    0  Highway 72 Curve   \n",
       "...                        ...            ...  ...               ...   \n",
       "3702 2023-06-21 20:45:21+00:00          Tyler    0               MRS   \n",
       "3703 2023-06-21 20:45:21+00:00          Tyler    0               MRS   \n",
       "3704 2023-06-21 20:45:21+00:00          Tyler    0               MRS   \n",
       "3705 2023-06-21 20:45:21+00:00          Tyler    0               MRS   \n",
       "3706 2023-06-21 20:45:21+00:00          Tyler    0               MRS   \n",
       "\n",
       "     woody_shrub_height imagery    combined_all_category_species     area_m  \\\n",
       "0                  None     AOP  Herbaceous_Mixed grass & forb__  85.322800   \n",
       "1                  None     AOP  Herbaceous_Mixed grass & forb__  85.322800   \n",
       "2                  None     AOP  Herbaceous_Mixed grass & forb__  85.322800   \n",
       "3                  None     AOP  Herbaceous_Mixed grass & forb__  85.322800   \n",
       "4                  None     AOP  Herbaceous_Mixed grass & forb__  85.322800   \n",
       "...                 ...     ...                              ...        ...   \n",
       "3702               None     AOP    Evergreen___Picea engelmannii  16.380218   \n",
       "3703               None     AOP    Evergreen___Picea engelmannii  16.380218   \n",
       "3704               None     AOP    Evergreen___Picea engelmannii  16.380218   \n",
       "3705               None     AOP    Evergreen___Picea engelmannii  16.380218   \n",
       "3706               None     AOP    Evergreen___Picea engelmannii  16.380218   \n",
       "\n",
       "                                               geometry  \n",
       "0     POLYGON ((455051.63677666837 4433775.201219057...  \n",
       "1     POLYGON ((455051.63677666837 4433775.201219057...  \n",
       "2     POLYGON ((455051.63677666837 4433775.201219057...  \n",
       "3     POLYGON ((455051.63677666837 4433775.201219057...  \n",
       "4     POLYGON ((455051.63677666837 4433775.201219057...  \n",
       "...                                                 ...  \n",
       "3702  POLYGON ((454902.5548156673 4430862.772296763,...  \n",
       "3703  POLYGON ((454902.5548156673 4430862.772296763,...  \n",
       "3704  POLYGON ((454902.5548156673 4430862.772296763,...  \n",
       "3705  POLYGON ((454902.5548156673 4430862.772296763,...  \n",
       "3706  POLYGON ((454902.5548156673 4430862.772296763,...  \n",
       "\n",
       "[3707 rows x 26 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, raster_path, polygons_path):\n",
    "        self.raster_path = raster_path\n",
    "        self.polygons_path = polygons_path\n",
    "        self.polygons = None\n",
    "        self.raster_meta = None\n",
    "        \n",
    "    def load_polygons(self):\n",
    "        \"\"\"Loads the polygons and ensures they are in the same CRS as the raster.\"\"\"\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            self.raster_meta = src.meta\n",
    "            self.polygons = gpd.read_file(self.polygons_path)\n",
    "            self.polygons = self.polygons.to_crs(src.crs)\n",
    "    \n",
    "    def extract_data_by_polygons(self):\n",
    "        \"\"\"Extracts the row and col indices from the raster for each polygon and appends all attributes from the polygons.\"\"\"\n",
    "        self.load_polygons()  # Load polygons and ensure CRS match\n",
    "        \n",
    "        all_data = []\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            raster_bounds = src.bounds\n",
    "            raster_box = box(*raster_bounds)\n",
    "            \n",
    "            for _, poly in self.polygons.iterrows():\n",
    "                geom = poly.geometry\n",
    "                # Skip invalid or empty geometries or those that do not intersect with the raster\n",
    "                if geom is None or geom.is_empty or not geom.intersects(raster_box):\n",
    "                    continue\n",
    "                \n",
    "                window = from_bounds(*geom.bounds, transform=src.transform)\n",
    "                # Skip windows that are completely outside the raster bounds\n",
    "                if window.width <= 0 or window.height <= 0:\n",
    "                    continue\n",
    "                \n",
    "                # Convert window offsets to integers\n",
    "                row_off = int(window.row_off)\n",
    "                col_off = int(window.col_off)\n",
    "                # Extract the rows and cols from the window\n",
    "                rows = range(row_off, row_off + int(window.height))\n",
    "                cols = range(col_off, col_off + int(window.width))\n",
    "                \n",
    "                # Collect all attributes from the polygon\n",
    "                attributes = poly.to_dict()\n",
    "                \n",
    "                # Append the rows and cols along with the polygon attributes to the data list\n",
    "                for row in rows:\n",
    "                    for col in cols:\n",
    "                        pixel_data = {\n",
    "                            'Pixel_Row': row,\n",
    "                            'Pixel_Col': col,\n",
    "                            **attributes  # This adds all polygon attributes\n",
    "                        }\n",
    "                        all_data.append(pixel_data)\n",
    "        \n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "\n",
    "gpkg_path = 'Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg'\n",
    "existing_raster_path = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_170802_reflectance/NEON_D13_NIWO_DP1_20200807_170802_reflectanceNEON_D13_NIWO_DP1_20200807_170802_reflectance__envi\"\n",
    "processor = ENVIProcessor(existing_raster_path, gpkg_path)\n",
    "df_polygons = processor.extract_data_by_polygons()\n",
    "df_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5da2f7bd-19bb-4989-bb56-fb289116a5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8489ce8a3c403daac98372a46d781b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV:   0%|          | 0/64 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path):\n",
    "    \"\"\"\n",
    "    Filters rows in a large CSV file, keeping only those that match the subset_columns\n",
    "    values found in the right_df. Writes the filtered rows to a new CSV file.\n",
    "    Includes a progress bar to track processing progress.\n",
    "\n",
    "    Parameters:\n",
    "    - large_csv_path (str): Path to the large CSV file.\n",
    "    - right_df (pd.DataFrame): Dataframe containing the filter criteria.\n",
    "    - subset_columns (list of str): Columns used for filtering.\n",
    "    - output_filtered_csv_path (str): Path to write the filtered CSV file.\n",
    "    \"\"\"\n",
    "    # Get the unique combinations of subset_columns in right_df\n",
    "    unique_combinations = right_df[subset_columns].drop_duplicates()\n",
    "\n",
    "    # Convert the unique combinations to a set of tuples for faster searching\n",
    "    unique_tuples = set([tuple(x) for x in unique_combinations.to_numpy()])\n",
    "\n",
    "    # Determine total number of rows for progress bar\n",
    "    total_rows = sum(1 for _ in open(large_csv_path, 'r', encoding='utf-8'))\n",
    "    chunksize = 100000  # Adjust based on your memory constraints\n",
    "    total_chunks = (total_rows // chunksize) + (1 if total_rows % chunksize else 0)\n",
    "\n",
    "    # Initialize tqdm progress bar\n",
    "    pbar = tqdm(total=total_chunks, desc='Processing CSV', unit='chunk')\n",
    "\n",
    "    # Initialize a DataFrame to hold chunks that pass the filter\n",
    "    filtered_chunks = []\n",
    "\n",
    "    # Read the large CSV in chunks\n",
    "    for chunk in pd.read_csv(large_csv_path, chunksize=chunksize):\n",
    "        # Filter the chunk\n",
    "        filtered_chunk = chunk[chunk.apply(lambda x: (x[subset_columns[0]], x[subset_columns[1]]) in unique_tuples, axis=1)]\n",
    "        \n",
    "        # If the filtered chunk is not empty, add it to the list\n",
    "        if not filtered_chunk.empty:\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Concatenate all filtered chunks and write to the output CSV\n",
    "    if filtered_chunks:\n",
    "        filtered_df = pd.concat(filtered_chunks)\n",
    "        filtered_df.to_csv(output_filtered_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"No matching rows found in the large CSV.\")\n",
    "\n",
    "# Example usage\n",
    "large_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_170802_reflectance/NEON_D13_NIWO_DP1_20200807_170802_reflectanceNEON_D13_NIWO_DP1_20200807_170802_reflectance_spectral_data_all_sensors.csv'\n",
    "output_filtered_csv_path = 'NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200807_170802_reflectance/NEON_D13_NIWO_DP1_20200807_170802_reflectanceNEON_D13_NIWO_DP1_20200807_170802_reflectance_polygons.csv'\n",
    "right_df = df_polygons\n",
    "subset_columns=['Pixel_Row', 'Pixel_Col']\n",
    "filter_large_csv_based_on_right_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85ea9b9-c0cc-41d1-b60e-9abf6aa59b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816df44aa6f94a9da252152bb66b25dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing CSV:   0%|          | 0/66 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from rasterio.windows import from_bounds\n",
    "from shapely.geometry import box\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, raster_path, polygons_path):\n",
    "        self.raster_path = raster_path\n",
    "        self.polygons_path = polygons_path\n",
    "        self.polygons = None\n",
    "        self.raster_meta = None\n",
    "\n",
    "    def load_polygons(self):\n",
    "        \"\"\"Loads the polygons and ensures they are in the same CRS as the raster.\"\"\"\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            self.raster_meta = src.meta\n",
    "            self.polygons = gpd.read_file(self.polygons_path)\n",
    "            self.polygons = self.polygons.to_crs(src.crs)\n",
    "\n",
    "    def extract_data_by_polygons(self):\n",
    "        \"\"\"Extracts the row and col indices from the raster for each polygon and appends all attributes from the polygons.\"\"\"\n",
    "        self.load_polygons()\n",
    "\n",
    "        all_data = []\n",
    "        with rasterio.open(self.raster_path) as src:\n",
    "            raster_bounds = src.bounds\n",
    "            raster_box = box(*raster_bounds)\n",
    "\n",
    "            for _, poly in self.polygons.iterrows():\n",
    "                geom = poly.geometry\n",
    "                if geom is None or geom.is_empty or not geom.intersects(raster_box):\n",
    "                    continue\n",
    "\n",
    "                window = from_bounds(*geom.bounds, transform=src.transform)\n",
    "                if window.width <= 0 or window.height <= 0:\n",
    "                    continue\n",
    "\n",
    "                row_off, col_off = int(window.row_off), int(window.col_off)\n",
    "                rows, cols = range(row_off, row_off + int(window.height)), range(col_off, col_off + int(window.width))\n",
    "                attributes = poly.to_dict()\n",
    "\n",
    "                for row in rows:\n",
    "                    for col in cols:\n",
    "                        pixel_data = {\n",
    "                            'Pixel_Row': row,\n",
    "                            'Pixel_Col': col,\n",
    "                            **attributes  # This adds all polygon attributes\n",
    "                        }\n",
    "                        all_data.append(pixel_data)\n",
    "\n",
    "        return pd.DataFrame(all_data)\n",
    "\n",
    "def filter_csv_based_on_df_with_progress(large_csv_path, right_df, subset_columns, output_filtered_csv_path):\n",
    "    \"\"\"Filters rows in a large CSV file based on a DataFrame and saves the filtered rows to a new CSV.\"\"\"\n",
    "    if os.path.exists(output_filtered_csv_path):\n",
    "        print(f\"Output file already exists: {output_filtered_csv_path}\")\n",
    "        return\n",
    "\n",
    "    unique_combinations = right_df[subset_columns].drop_duplicates()\n",
    "    unique_tuples = set([tuple(x) for x in unique_combinations.to_numpy()])\n",
    "\n",
    "    total_rows = sum(1 for _ in open(large_csv_path, 'r', encoding='utf-8'))\n",
    "    chunksize = 100000\n",
    "    total_chunks = (total_rows // chunksize) + (1 if total_rows % chunksize else 0)\n",
    "\n",
    "    filtered_chunks = []\n",
    "    pbar = tqdm(total=total_chunks, desc='Processing CSV', unit='chunk')\n",
    "\n",
    "    for chunk in pd.read_csv(large_csv_path, chunksize=chunksize):\n",
    "        filtered_chunk = chunk[chunk.apply(lambda x: (x[subset_columns[0]], x[subset_columns[1]]) in unique_tuples, axis=1)]\n",
    "        if not filtered_chunk.empty:\n",
    "            filtered_chunks.append(filtered_chunk)\n",
    "        pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    if filtered_chunks:\n",
    "        filtered_df = pd.concat(filtered_chunks)\n",
    "        filtered_df.to_csv(output_filtered_csv_path, index=False)\n",
    "        print(f\"Filtered data saved to {output_filtered_csv_path}\")\n",
    "    else:\n",
    "        print(\"No matching rows found in the large CSV.\")\n",
    "\n",
    "def combined_processing_function(folder_path, polygons_path):\n",
    "    print(f\"Processing folder: {folder_path}\")\n",
    "    \n",
    "    # Search for the required CSV file that matches the naming convention\n",
    "    csv_files = glob.glob(os.path.join(folder_path, \"*_spectral_data_all_sensors.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No spectral data CSV found in {folder_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    large_csv_path = csv_files[0]  # Use the first matching CSV file\n",
    "\n",
    "    # Define the output CSV file path\n",
    "    output_filtered_csv_path = large_csv_path.replace(\"_spectral_data_all_sensors.csv\", \"_polygons.csv\")\n",
    "    \n",
    "    # Check if the output CSV already exists\n",
    "    if os.path.exists(output_filtered_csv_path):\n",
    "        print(f\"Output file already exists: {output_filtered_csv_path}. Skipping folder.\")\n",
    "        return\n",
    "\n",
    "    # Search for the raster file ending with '_envi' but not '.hdr' or '.img'\n",
    "    raster_files = [f for f in glob.glob(os.path.join(folder_path, \"*_envi\")) if not f.endswith('.hdr') and not f.endswith('.img')]\n",
    "    \n",
    "    if not raster_files:\n",
    "        print(f\"No ENVI raster file found in {folder_path}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    raster_path = raster_files[0]  # Use the first matching raster file\n",
    "\n",
    "    # Process the raster and polygon data, then filter the CSV\n",
    "    try:\n",
    "        processor = ENVIProcessor(raster_path, polygons_path)\n",
    "        df_polygons = processor.extract_data_by_polygons()\n",
    "        filter_csv_based_on_df_with_progress(large_csv_path, df_polygons, ['Pixel_Row', 'Pixel_Col'], output_filtered_csv_path)\n",
    "        print(f\"Filtered data saved to {output_filtered_csv_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {folder_path}: {e}\")\n",
    "        \n",
    "# Example usage:\n",
    "folder_path = \"NIWOT_calibration_flight_08_2020/NEON_D13_NIWO_DP1_20200801_161441_reflectance\"\n",
    "polygons_path = 'Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg'\n",
    "\n",
    "combined_processing_function(folder_path, polygons_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macrosystems",
   "language": "python",
   "name": "macrosystems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
