{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cross-Sensor Calibration","text":"<p>Earth Lab\u2019s cross-sensor-cal is a Python package for producing physically corrected and sensor-harmonized reflectance from NEON hyperspectral data and other fine-resolution imagery.</p> <p>It provides a reproducible workflow that:</p> <ul> <li>exports NEON HDF5 to ENVI  </li> <li>applies topographic and BRDF correction  </li> <li>harmonizes reflectance to Landsat / MicaSense bandspaces  </li> <li>writes Parquet tables  </li> <li>emits QA PNG, PDF, and JSON summaries  </li> </ul>"},{"location":"#minimal-example-cli","title":"Minimal example (CLI)","text":"<p>```bash BASE=output_demo mkdir -p \"$BASE\"</p> <p>cscal-pipeline \\   --base-folder \"$BASE\" \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --max-workers 2 \\   --engine thread Inspect QA outputs: open $BASE/NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance/*_qa.png For a Python/Jupyter version, see: Usage \u2192 Jupyter notebook example How the pipeline works HDF5 \u2192 ENVI export Topographic correction BRDF correction Sensor harmonization (bandpass convolution) Parquet extraction + merging QA reporting See the Pipeline overview for details. Next steps Quickstart Jupyter notebook example Tutorials Reference</p>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"benchmarks/#reproducible-timing-experiment-design","title":"Reproducible timing experiment design","text":"<ul> <li>Pin package versions and record hardware specifications.</li> <li>Use fixed random seeds and keep the system load constant.</li> <li>Run each benchmark multiple times, reporting mean and variance.</li> <li>Save the exact command line and configuration for future runs.</li> </ul>"},{"location":"benchmarks/#ray-cluster-knobs","title":"Ray cluster knobs","text":"<p>When scaling benchmarks on Ray, adjust:</p> <ul> <li><code>--num-cpus</code> and <code>--num-gpus</code> to control available resources.</li> <li><code>--object-store-memory</code> for large in-memory datasets.</li> <li><code>--temp-dir</code> to point to fast local storage.</li> <li><code>--dashboard-port</code> to monitor cluster status.</li> </ul>"},{"location":"benchmarks/#io-bottleneck-tips","title":"I/O bottleneck tips","text":"<ul> <li>Chunk rasters along the row and column dimensions so each worker reads contiguous blocks.</li> <li>Enable compression such as LZW or DEFLATE to reduce disk usage and transfer time.</li> <li>Cache intermediate products or use memory-mapped files to avoid repeated reads.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <p>The pipeline is configured through a <code>config.yaml</code> file that combines settings for every stage. Values shown below are the defaults unless marked as required.</p>"},{"location":"configuration/#schema","title":"Schema","text":"Key Type Default Required <code>base_folder</code> string <code>\"output\"</code> yes <code>download.site_code</code> string \u2014 yes <code>download.year_month</code> string (YYYYMM) \u2014 yes <code>download.flight_lines</code> list[string] \u2014 yes <code>download.product_code</code> string <code>\"DP1.30006.001\"</code> no <code>convert.export_ancillary</code> bool <code>true</code> no <code>convert.export_brdf_config</code> bool <code>true</code> no <code>topo_brdf.num_cpus</code> int <code>8</code> no <code>topo_brdf.file_type</code> string <code>\"envi\"</code> no <code>topo_brdf.corrections</code> list[string] <code>[\"topo\",\"brdf\"]</code> no <code>topo_brdf.bad_bands</code> list[int] <code>[]</code> no <code>topo_brdf.anc_files</code> map[string,str] \u2014 conditional\u2020 <code>topo_brdf.export.output_dir</code> string <code>\"./\"</code> no <code>topo_brdf.export.suffix</code> string <code>\"_corrected_envi\"</code> no <code>topo_brdf.export.image</code> bool <code>true</code> no <code>topo_brdf.export.masks</code> bool <code>true</code> no <code>topo_brdf.export.coeffs</code> bool <code>true</code> no <code>resample.method</code> string <code>\"convolution\"</code> no <code>resample.sensors</code> list[string] <code>[\"Landsat_8\"]</code> no <code>mask.polygon_layer</code> string \u2014 no <code>mask.raster_crs_override</code> string|int \u2014 no <code>mask.polygons_crs_override</code> string|int \u2014 no <code>mask.plot_output</code> bool <code>false</code> no <code>sort.remote_prefix</code> string <code>\"\"</code> no <code>sort.sync_files</code> bool <code>true</code> no <code>postprocess.reflectance_offset</code> int <code>0</code> no <p>\u2020 required when <code>topo_brdf.file_type</code> is <code>\"envi\"</code>.</p>"},{"location":"configuration/#example","title":"Example","text":"<pre><code>base_folder: output\n\ndownload:\n  site_code: NIWO\n  year_month: \"202008\"\n  flight_lines: [\"FL1\", \"FL2\"]\n  product_code: DP1.30006.001\n\nconvert:\n  export_ancillary: true\n  export_brdf_config: true\n\ntopo_brdf:\n  num_cpus: 8\n  file_type: envi\n  corrections: [\"topo\", \"brdf\"]\n  bad_bands: []\n  anc_files: {}\n  export:\n    output_dir: ./corrected\n    suffix: _corrected_envi\n    image: true\n    masks: true\n    coeffs: true\n\nresample:\n  method: convolution\n  sensors: [\"Landsat_8\"]\n\nmask:\n  polygon_layer: polygons.geojson\n  plot_output: false\n\nsort:\n  remote_prefix: \"\"\n  sync_files: true\n\npostprocess:\n  reflectance_offset: 0\n</code></pre>"},{"location":"configuration/#cli-overrides","title":"CLI overrides","text":"<p>The <code>cscal-pipeline</code> entry point automatically runs the download stage before spinning up per-flightline workers. Use <code>--max-workers</code> to opt into parallel processing once the <code>.h5</code> files are present, and <code>--engine</code> to pick the backend. Command-line options override the corresponding entries in <code>config.yaml</code>:</p> <ul> <li><code>bin/jefe.py BASE_FOLDER SITE YEAR_MONTH FL1,FL2</code> sets <code>base_folder</code>, <code>download.site_code</code>, <code>download.year_month</code> and <code>download.flight_lines</code>.</li> <li><code>--polygon_layer_path</code> \u2192 <code>mask.polygon_layer</code></li> <li><code>--reflectance-offset</code> \u2192 <code>postprocess.reflectance_offset</code></li> <li><code>--remote-prefix</code> \u2192 <code>sort.remote_prefix</code></li> <li><code>--no-sync</code> sets <code>sort.sync_files</code> to <code>false</code></li> <li><code>--max-workers</code> sets the parallel worker count for <code>go_forth_and_multiply()</code></li> <li><code>--engine</code> selects the backend (<code>thread</code>, <code>process</code>, or <code>ray</code>). Ray requires   the optional dependency and is only loaded when requested.</li> </ul>"},{"location":"cyverse-irods/","title":"CyVerse iRODS","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <p>CyVerse storage is accessed through iRODS paths handled by the <code>gocmd</code> utility. Remote locations are written with an <code>i:</code> prefix followed by the iRODS zone, for example <code>i:/iplant/home/your_username</code>.</p> <p>To authenticate, run <code>./gocmd init</code> once. The command records your credentials in <code>~/.irods</code> so future operations can reach the data store without re-entering them.</p> <p>Common operations:</p> <pre><code># list a collection\n./gocmd ls i:/iplant/home/your_username\n\n# download a file\n./gocmd get i:/iplant/home/your_username/data.txt\n\n# upload a file to a collection\n./gocmd put local_file.txt i:/iplant/home/your_username/\n</code></pre> <p>Scripts build remote paths using variables patterned as:</p> <pre><code>remote_path = f\"i:/iplant/{remote_prefix}/{dest_path}\"\n</code></pre> <p>Replace <code>remote_prefix</code> and <code>dest_path</code> with the appropriate subdirectory and filename for your project.</p>"},{"location":"dev-notes/","title":"Developer Notes","text":"<p>Use the following commands to work on the documentation locally:</p> <ol> <li>Install dependencies:    <pre><code>pip install -r docs/requirements.txt\n# or\nuv pip install -r docs/requirements.txt\n</code></pre></li> <li>Start a live preview:    <pre><code>mkdocs serve\n</code></pre></li> <li>Build the static site:    <pre><code>mkdocs build\n</code></pre></li> </ol>"},{"location":"documentation-overview/","title":"Documentation","text":""},{"location":"documentation-overview/#overview","title":"Overview","text":"<p>This directory hosts project documentation, including the style guide that defines how you should write and maintain docs across the repository.</p>"},{"location":"documentation-overview/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Markdown viewer or editor</li> <li>Familiarity with basic Git workflows</li> </ul>"},{"location":"documentation-overview/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<ol> <li>Review the documentation style guide to    understand structure and formatting conventions.</li> <li>Create or update documentation in other folders using the guide's narrative    approach and examples.</li> <li>After editing, run <code>pytest</code> at the repository root to ensure code snippets    still execute.</li> </ol>"},{"location":"documentation-overview/#reference","title":"Reference","text":"<ul> <li><code>documentation_style_guide.md</code> \u2013 canonical guide for all project docs</li> </ul>"},{"location":"documentation-overview/#next-steps","title":"Next steps","text":"<p>Expand this folder with API references, architecture diagrams, or tutorials as the project evolves.</p>"},{"location":"documentation_style_guide/","title":"Cross-Sensor Calibration Documentation Style Guide","text":"<p>This guide establishes conventions for writing documentation in the Cross-Sensor Calibration project. Its goal is to create a linear, pedagogical narrative that makes the package easy to understand and adopt.</p>"},{"location":"documentation_style_guide/#philosophy","title":"Philosophy","text":"<ul> <li>Clarity first. Explain concepts in plain language before introducing technical jargon.</li> <li>Narrative flow. Documentation should guide the reader from inputs through processing to outputs in a logical order.</li> <li>Pragmatic examples. Every section should include code snippets or workflows that users can run directly.</li> <li>Minimal prerequisites. Link to background materials rather than assuming extensive prior knowledge.</li> </ul>"},{"location":"documentation_style_guide/#structure","title":"Structure","text":"<ol> <li>Overview \u2013 Briefly describe the purpose of the component and how it fits into the larger workflow.</li> <li>Prerequisites \u2013 List required data, dependencies, and setup steps.</li> <li>Step-by-step tutorial \u2013 Present instructions in chronological order.</li> <li>Reference \u2013 Provide detailed API descriptions, parameters, and links to source code.</li> <li>Next steps \u2013 Suggest follow-on tasks or sections.</li> </ol>"},{"location":"documentation_style_guide/#style","title":"Style","text":"<ul> <li>Use Markdown headings (<code>#</code>, <code>##</code>, <code>###</code>) to organize content.</li> <li>Write in the second person (\u201cyou\u201d) and active voice.</li> <li>Keep sentences concise; aim for one idea per sentence.</li> <li>Use numbered lists for sequences and bullet lists for options.</li> <li>Highlight file names, parameters, and code using backticks (<code>like_this</code>).</li> <li>Wrap code examples in fenced blocks with the appropriate language tag.</li> <li>Include diagrams or figures when they clarify complex processes.</li> <li>Cross-link related documents with relative paths.</li> </ul>"},{"location":"documentation_style_guide/#formatting","title":"Formatting","text":"<ul> <li>Line length: soft wrap at 100 characters.</li> <li>Use American English spelling.</li> <li>Date format: YYYY-MM-DD.</li> <li>Reference issues or pull requests with full links.</li> </ul>"},{"location":"documentation_style_guide/#maintenance","title":"Maintenance","text":"<ul> <li>Each documentation page must include a <code>Last updated: YYYY-MM-DD</code> line at the end.</li> <li>When updating docs, ensure examples are tested against the current codebase.</li> <li>Run <code>pytest</code> before committing changes that affect code examples.</li> </ul> <p>Following this guide will keep the documentation consistent and approachable for new contributors and users.</p>"},{"location":"env-setup/","title":"Environment Setup","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"env-setup/#conda-environment","title":"Conda environment","text":"<p>An example environment file for Conda is shown below. Save it as <code>environment.yaml</code> and create the environment with <code>conda env create -f environment.yaml</code>.</p> <pre><code>name: cross-sensor-cal\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.10\n  - gdal\n  - proj\n  - pip\n  - pip:\n      - ray[default]\n</code></pre>"},{"location":"env-setup/#uv-pip-alternative","title":"uv / pip alternative","text":"<p>Instead of Conda you can install the project with <code>uv</code> or plain <code>pip</code>:</p> <pre><code>uv pip install -r requirements.txt\n# or\npip install -r requirements.txt\n</code></pre>"},{"location":"env-setup/#gdal-proj-and-ray-notes","title":"GDAL, PROJ, and Ray notes","text":"<ul> <li>GDAL and PROJ require native libraries. Installing via the   <code>conda-forge</code> channel usually resolves most platform issues.</li> <li>Ray makes heavy use of shared memory. If Ray reports <code>/dev/shm</code> errors,   increase shared memory. For Docker containers use   <code>--shm-size=8g</code> (adjust as needed).</li> </ul>"},{"location":"env-setup/#known-os-quirks","title":"Known OS quirks","text":"<ul> <li>macOS: Homebrew installations of GDAL/PROJ may conflict with Conda.   Prefer the Conda packages or ensure <code>brew</code> paths come after Conda in <code>PATH</code>.</li> <li>Windows: enable long paths (<code>git config --system core.longpaths true</code>) to   avoid checkout errors.</li> </ul>"},{"location":"env-setup/#preview-documentation-locally","title":"Preview documentation locally","text":"<p>Run the MkDocs development server from the repository root:</p> <pre><code>mkdocs serve\n</code></pre> <p>Open http://127.0.0.1:8000 in a browser to view the docs.</p>"},{"location":"env/","title":"Environment","text":"Component Known-good Python 3.10\u20133.12 OS macOS 13+, Ubuntu 22.04+ Core libs numpy, rasterio, gdal, ray, xarray, pandas"},{"location":"env/#setup-venv","title":"Setup (venv)","text":"<pre><code>python -m venv .venv &amp;&amp; source .venv/bin/activate\npip install -U pip\npip install cross-sensor-cal\n</code></pre>"},{"location":"env/#setup-conda","title":"Setup (conda)","text":"<pre><code>conda create -n cscal python=3.11 -y\nconda activate cscal\npip install -U pip\npip install cross-sensor-cal\n</code></pre>"},{"location":"extending/","title":"Extending","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"extending/#add-a-sensor","title":"Add a sensor","text":""},{"location":"extending/#overview","title":"Overview","text":"<p>This recipe shows how to integrate a new sensor into the cross\u2011sensor calibration workflow.</p>"},{"location":"extending/#prerequisites","title":"Prerequisites","text":"<ul> <li>Spectral response function (SRF) curves for the sensor.</li> <li>Familiarity with the project's naming conventions.</li> </ul>"},{"location":"extending/#step-by-step-tutorial","title":"Step-by-step tutorial","text":"<ol> <li>Add SRFs. Place the sensor's SRF files in the data directory and register    them with the SRF loader.</li> <li>Update resampling/convolution mapping. Extend the resampling and    convolution dictionaries so the pipeline knows how to transform the sensor's    bands.</li> <li>Update the naming map. Insert the sensor's canonical name and band    identifiers into the shared naming map used across modules.</li> <li>Add a golden test and validation checklist.</li> <li>Create a golden test case with expected outputs.</li> <li>Document validation steps to confirm the sensor behaves correctly.</li> </ol>"},{"location":"extending/#next-steps","title":"Next steps","text":"<p>Run the full validation suite and submit a pull request for review.</p> <p>Last updated: 2025-08-18</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#what-is-the-purpose-of-cross-sensor-calibration","title":"What is the purpose of cross-sensor calibration?","text":"<p>To make reflectance from NEON, drones, and satellites comparable by removing geometric artifacts and aligning spectral responses.</p>"},{"location":"faq/#why-does-neon-use-scaled-reflectance","title":"Why does NEON use scaled reflectance?","text":"<p>NEON stores reflectance as scaled integers for performance. The export stage automatically converts these into floating-point reflectance values using NEON\u2019s scale factors.</p>"},{"location":"faq/#do-i-need-both-topographic-and-brdf-correction","title":"Do I need both topographic and BRDF correction?","text":"<p>Yes, for most ecological analyses. Topographic correction removes slope/aspect artifacts; BRDF correction normalizes view/sun geometry.</p>"},{"location":"faq/#why-do-some-bands-look-noisy","title":"Why do some bands look noisy?","text":"<p>Low-SNR wavelength regions (especially in the SWIR) are more sensitive to BRDF fitting errors.</p>"},{"location":"faq/#what-sensors-can-i-harmonize-to","title":"What sensors can I harmonize to?","text":"<p>Currently:</p> <ul> <li>Landsat OLI / OLI-2  </li> <li>MicaSense RedEdge (via regression)  </li> </ul> <p>More sensors can be added as SRF tables become available.</p>"},{"location":"faq/#why-does-the-pipeline-take-so-long","title":"Why does the pipeline take so long?","text":"<p>NEON tiles are very large (tens of GB). BRDF and ENVI export steps are computationally expensive.</p> <p>Parallelization helps, but memory constraints matter.</p>"},{"location":"faq/#why-do-some-pixels-have-reflectance-1","title":"Why do some pixels have reflectance &gt; 1?","text":"<p>BRDF or DEM correction may amplify noise at certain angles or terrain slopes. Such pixels are reported in QA metrics and usually masked.</p>"},{"location":"faq/#can-i-run-analyses-without-touching-envi-files","title":"Can I run analyses without touching ENVI files?","text":"<p>Yes\u2014use the Parquet outputs (<code>*_merged_pixel_extraction.parquet</code>) for large-scale analysis.</p>"},{"location":"faq/#next-steps","title":"Next steps","text":"<ul> <li>Troubleshooting </li> <li>Pipeline outputs</li> </ul>"},{"location":"glossary/","title":"Glossary","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <ul> <li>AOP: Apparent Optical Property describing how a medium's optical characteristics vary with viewing geometry.</li> <li>BRDF: Bidirectional Reflectance Distribution Function giving reflectance as a function of illumination and view angles.</li> <li>SRF: Spectral Response Function representing a sensor's sensitivity across wavelengths.</li> <li>SNR: Signal-to-Noise Ratio, the level of desired signal relative to background noise.</li> <li>ENVI: Environment for Visualizing Images, a software package for processing geospatial imagery.</li> <li>MESMA: Multiple Endmember Spectral Mixture Analysis, an unmixing algorithm using variable endmember sets.</li> <li>DN: Digital Number, the raw quantized value recorded by a sensor.</li> <li>TOA: Top of Atmosphere reflectance measured above the atmosphere.</li> <li>BOA: Bottom of Atmosphere or surface reflectance after atmospheric correction.</li> <li>NIR: Near Infrared region of the electromagnetic spectrum, roughly 0.7\u20131.3 \u00b5m.</li> <li>SWIR: Shortwave Infrared region spanning approximately 1.3\u20132.5 \u00b5m.</li> <li>VIS: Visible portion of the electromagnetic spectrum between about 0.4\u20130.7 \u00b5m.</li> <li>FLAASH: Fast Line-of-sight Atmospheric Analysis of Spectral Hypercubes, an atmospheric correction tool.</li> <li>LiDAR: Light Detection and Ranging, active remote sensing using laser pulses to measure distance.</li> <li>UAV: Uncrewed Aerial Vehicle used as a platform for acquiring high-resolution imagery.</li> <li>NDVI: Normalized Difference Vegetation Index calculated from NIR and red bands to indicate vegetation vigor.</li> <li>MODIS: Moderate Resolution Imaging Spectroradiometer, a multispectral sensor on NASA's Terra and Aqua satellites.</li> <li>GSD: Ground Sample Distance, the ground size represented by one image pixel.</li> <li>RSR: Relative Spectral Response, the normalized sensitivity of a detector as a function of wavelength.</li> <li>AVIRIS: Airborne Visible/Infrared Imaging Spectrometer, a NASA hyperspectral imaging instrument.</li> </ul>"},{"location":"naming-conventions/","title":"Naming Conventions","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"naming-conventions/#canonical-filename-pattern","title":"Canonical Filename Pattern","text":"<p>All files produced by the pipeline use a common token order:</p> <pre><code>NEON_{site}_{YYYYMMDD}_{HHMMSS}_FL{line}_{product}{suffix}.ext\n</code></pre> <ul> <li><code>site</code> \u2013 NEON site code (e.g., <code>SJER</code>)</li> <li><code>YYYYMMDD</code> and <code>HHMMSS</code> \u2013 acquisition date and time in UTC</li> <li><code>FL{line}</code> \u2013 zero\u2011padded flight line identifier</li> <li><code>product</code> \u2013 base product name (e.g., <code>NIS</code>)</li> <li><code>suffix</code> \u2013 processing state (see table below)</li> <li><code>ext</code> \u2013 file extension such as <code>.img</code> or <code>.hdr</code></li> </ul> <p>Regex</p> <pre><code>^NEON_[A-Z0-9]{4}_\\d{8}_\\d{6}_FL\\d{3}_[A-Za-z0-9]+(?:_radiance|_ancillary|_corrected_envi|_reflectance)\\.(?:img|hdr)$\n</code></pre>"},{"location":"naming-conventions/#standard-suffixes","title":"Standard Suffixes","text":"Suffix Meaning <code>_radiance</code> Raw radiance from HDF5 conversion <code>_ancillary</code> Ancillary data produced with radiance <code>_corrected_envi</code> BRDF/TOPO corrected ENVI image <code>_reflectance</code> Final reflectance product"},{"location":"naming-conventions/#directory-layout","title":"Directory Layout","text":"<pre><code>site/\n\u2514\u2500\u2500 YYYYMMDD/\n    \u2514\u2500\u2500 FL###/\n        \u251c\u2500\u2500 raw/\n        \u2502   \u251c\u2500\u2500 NEON_SITE_YYYYMMDD_HHMMSS_FL###_NIS_radiance.img\n        \u2502   \u2514\u2500\u2500 NEON_SITE_YYYYMMDD_HHMMSS_FL###_NIS_ancillary.img\n        \u2514\u2500\u2500 derived/\n            \u251c\u2500\u2500 NEON_SITE_YYYYMMDD_HHMMSS_FL###_NIS_corrected_envi.img\n            \u2514\u2500\u2500 NEON_SITE_YYYYMMDD_HHMMSS_FL###_NIS_corrected_envi.hdr\n</code></pre>"},{"location":"naming-conventions/#common-violations-fixes","title":"Common Violations &amp; Fixes","text":"Violation Why it matters Fix Missing flight line token Downstream scripts cannot group files Include <code>_FL###</code> before the suffix Wrong suffix for directory (e.g., <code>_radiance</code> in <code>derived/</code>) Causes processing confusion Move file to <code>raw/</code> or rename with proper suffix Lower\u2011case site code Breaks regex patterns Use upper\u2011case site codes Spaces instead of underscores Parsing fails Replace spaces with <code>_</code>"},{"location":"overview/","title":"Overview","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <p>The Cross\u2011Sensor Calibration workflow runs every NEON flight line through an idempotent, restart-safe series of five stages. Each stage emits tqdm-style progress bars, prefixes logs with <code>[flight_stem]</code>, and writes artifacts using canonical paths from :func:<code>cross_sensor_cal.utils.naming.get_flight_paths</code>.</p> <pre><code>flowchart LR\n    D[Download .h5]\n    E[Export ENVI]\n    J[Build BRDF+topo JSON]\n    C[Correct reflectance]\n    R[Resample + Parquet]\n    D --&gt; E --&gt; J --&gt; C --&gt; R\n</code></pre> <ul> <li>Download: <code>stage_download_h5()</code> restores automatic retrieval of NEON   directional reflectance cubes and leaves each <code>.h5</code> in the workspace root for   easy cleanup.</li> <li>ENVI export: Converts the HDF5 cube to   <code>&lt;base&gt;/&lt;flight_stem&gt;/&lt;flight_stem&gt;_envi.img/.hdr/.parquet</code> with per-tile   progress updates.</li> <li>BRDF + topo JSON: Computes correction parameters once per flight line and   records them in <code>&lt;flight_stem&gt;_brdfandtopo_corrected_envi.json</code> inside the   dedicated subdirectory.</li> <li>BRDF + topo correction: Streams a chunk progress bar while writing the   canonical corrected cube (<code>*_brdfandtopo_corrected_envi.img/.hdr/.parquet</code>).</li> <li>Sensor resample + Parquet: Produces per-sensor ENVI pairs and matching   Parquet summaries in the same subfolder, leaving the raw <code>.h5</code> untouched.</li> </ul> <p>Every derived artifact now lives under <code>&lt;base&gt;/&lt;flight_stem&gt;/</code>, keeping the workspace organized and allowing long-term retention of processed products without stockpiling multi-gigabyte <code>.h5</code> inputs. <code>_scoped_log_prefix()</code> keeps parallel runs legible while <code>max_workers</code> lets you opt into ThreadPool-powered concurrency once downloads finish.</p>"},{"location":"overview/#who-is-this-for","title":"Who is this for?","text":"<p>Researchers processing NEON AOP flight lines who need reproducible ENVI deliverables, per-sensor band stacks, and Parquet summaries for cross-sensor analysis. All scientific calculations remain unchanged from previous releases\u2014 the 2025 refresh focuses on workflow, performance, and restart safety.</p>"},{"location":"pipeline/","title":"Pipeline reference","text":"<p>Cross-Sensor Calibration orchestrates the same five ordered stages for every flight line. Each stage consults <code>get_flight_paths()</code> to locate its inputs, per-flightline working directory, and outputs, validates artifacts before working, and emits emoji-rich logs that make the restart-safe behavior explicit. All persistent products land inside <code>&lt;base_folder&gt;/&lt;flight_stem&gt;/</code> (with the raw <code>.h5</code> kept at the base root) and include ENVI (<code>.img/.hdr</code>), JSON metadata, and Parquet summaries.</p>"},{"location":"pipeline/#canonical-paths-via-get_flight_paths","title":"Canonical paths via <code>get_flight_paths()</code>","text":"<p><code>get_flight_paths(base_folder, flight_stem)</code> is the authoritative source of truth for every artifact the pipeline reads or writes. For a flight line with stem <code>&lt;flight_stem&gt;</code> it yields:</p> <ul> <li><code>h5_path</code> \u2192 <code>&lt;base_folder&gt;/&lt;flight_stem&gt;.h5</code></li> <li><code>work_dir</code> \u2192 <code>&lt;base_folder&gt;/&lt;flight_stem&gt;/</code></li> <li><code>raw_envi_img</code> / <code>raw_envi_hdr</code> \u2192 <code>&lt;flight_stem&gt;_envi.img</code> and   <code>&lt;flight_stem&gt;_envi.hdr</code> inside <code>work_dir</code></li> <li><code>correction_json</code> \u2192 <code>&lt;flight_stem&gt;_brdfandtopo_corrected_envi.json</code> in   <code>work_dir</code></li> <li><code>corrected_img</code> / <code>corrected_hdr</code> \u2192   <code>&lt;flight_stem&gt;_brdfandtopo_corrected_envi.img</code> and   <code>&lt;flight_stem&gt;_brdfandtopo_corrected_envi.hdr</code></li> <li><code>sensor_products</code> \u2192 a dict mapping each sensor (e.g. <code>landsat_tm</code>,   <code>landsat_etm+</code>, <code>landsat_oli</code>, <code>landsat_oli2</code>, <code>micasense</code>) to   <code>&lt;flight_stem&gt;_&lt;sensor_name&gt;_envi.img</code> /   <code>&lt;flight_stem&gt;_&lt;sensor_name&gt;_envi.hdr</code></li> <li><code>parquet_products</code> \u2192 optional Parquet summaries colocated with their source   rasters</li> </ul> <p>All stages request their expected paths from this function and refuse to invent filenames on the fly. If naming ever changes, update <code>get_flight_paths()</code> once rather than editing every stage.</p>"},{"location":"pipeline/#stage-by-stage-details","title":"Stage-by-stage details","text":"<p>Every stage is restart-safe: it skips itself when valid outputs already exist. Validation requires both sides of an ENVI pair to exist and be non-empty or, for JSON, the file must parse successfully. When a stage skips, it logs a <code>\u2705 ... (skipping)</code> message; otherwise it performs work and logs what it produced. Live tqdm progress bars accompany downloads, ENVI chunk exports, and BRDF+topo corrections.</p>"},{"location":"pipeline/#0-download-neon-hdf5","title":"0. Download NEON HDF5","text":"<ul> <li>Inputs</li> <li>NEON site code, product code, and flight stem.</li> <li>Outputs</li> <li><code>&lt;base_folder&gt;/&lt;flight_stem&gt;.h5</code> (left in the workspace root).</li> <li>Skip criteria</li> <li>Existing <code>.h5</code> file that passes a size/metadata sanity check.</li> <li>Logging</li> <li>Logs <code>\ud83d\udce5 stage_download_h5()</code> with a streaming byte counter while downloading.</li> <li>On skip emits <code>\u2705 stage_download_h5() found existing .h5 (skipping)</code>.</li> <li>Failure handling</li> <li>Raises on HTTP errors or truncated downloads; reruns resume by revalidating the file.</li> </ul>"},{"location":"pipeline/#1-envi-export","title":"1. ENVI export","text":"<ul> <li>Inputs</li> <li>NEON directional reflectance cube (<code>&lt;flight_stem&gt;.h5</code>).</li> <li>Outputs</li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_envi.img</code></li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_envi.hdr</code></li> <li>Extras</li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_envi.parquet</code> (summary statistics per tile).</li> <li>Skip criteria</li> <li>Both ENVI files exist, are non-empty, and pass the internal ENVI validation.</li> <li>Logging</li> <li>Always logs <code>\ud83d\udd0e ENVI export target for &lt;flight_stem&gt; is ..._envi.img / ..._envi.hdr</code> with a chunked progress bar.</li> <li>On skip emits     <code>\u2705 ENVI export already complete for &lt;flight_stem&gt; -&gt; ..._envi.img / ..._envi.hdr (skipping heavy export)</code>.</li> <li>Otherwise streams progress as tiles are written and logs success.</li> <li>Failure handling</li> <li>Errors here stop the stage; reruns regenerate if outputs were missing or invalid.</li> </ul>"},{"location":"pipeline/#2-build-correction-json","title":"2. Build correction JSON","text":"<ul> <li>Inputs</li> <li>Uncorrected ENVI pair from stage 1.</li> <li>Output</li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_brdfandtopo_corrected_envi.json</code></li> <li>Skip criteria</li> <li>JSON exists and parses.</li> <li>Logging</li> <li>On skip logs     <code>\u2705 Correction JSON already complete for &lt;flight_stem&gt; -&gt; ..._brdfandtopo_corrected_envi.json (skipping)</code>.</li> <li>Otherwise logs that it is computing parameters and then writing the JSON.</li> <li>Failure handling</li> <li>Failures propagate so that reruns recompute the JSON before downstream stages continue.</li> </ul>"},{"location":"pipeline/#3-brdf-topographic-correction","title":"3. BRDF + topographic correction","text":"<ul> <li>Inputs</li> <li>Uncorrected ENVI pair.</li> <li>Correction JSON from stage 2.</li> <li>Outputs</li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_brdfandtopo_corrected_envi.img</code></li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_brdfandtopo_corrected_envi.hdr</code></li> <li>Extras</li> <li><code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_brdfandtopo_corrected_envi.parquet</code></li> <li>Skip criteria</li> <li>Corrected ENVI pair exists, is non-empty, and validates.</li> <li>Logging</li> <li>On skip logs     <code>\u2705 BRDF+topo correction already complete for &lt;flight_stem&gt; -&gt; ..._brdfandtopo_corrected_envi.img / ..._brdfandtopo_corrected_envi.hdr (skipping)</code>.</li> <li>When recomputing, streams a chunk progress bar while writing and logs completion.</li> <li>Failure handling</li> <li>Failures raise immediately because the corrected cube is the canonical science product.     Reruns recompute just this stage if its outputs were missing or corrupt.</li> </ul>"},{"location":"pipeline/#4-sensor-convolution-resampling","title":"4. Sensor convolution / resampling","text":"<ul> <li>Inputs</li> <li>Corrected ENVI pair from stage 3. This stage never reads the raw <code>.h5</code>.</li> <li>Sensor spectral response library bundled with the project.</li> <li>Outputs</li> <li>For each known sensor, an ENVI pair following     <code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_&lt;sensor_name&gt;_envi.img</code> /     <code>&lt;flight_stem&gt;/&lt;flight_stem&gt;_&lt;sensor_name&gt;_envi.hdr</code> (e.g.     <code>.../NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance_landsat_tm_envi.img</code>).</li> <li>Extras</li> <li>Optional Parquet tables named <code>&lt;flight_stem&gt;_&lt;sensor_name&gt;_envi.parquet</code> in     the same folder.</li> <li>Skip criteria</li> <li>Individual sensor ENVI pairs that already exist and validate are skipped and reported.</li> <li>Logging</li> <li>Begins with <code>\ud83c\udfaf Convolving corrected reflectance for &lt;flight_stem&gt;</code>.</li> <li>On fresh generation logs     <code>\u2705 Wrote &lt;sensor_name&gt; product for &lt;flight_stem&gt; -&gt; ..._envi.img / ..._envi.hdr</code>.</li> <li>On skip logs     <code>\u2705 &lt;sensor_name&gt; product already complete for &lt;flight_stem&gt; -&gt; ... (skipping)</code>.</li> <li>Ends with     <code>\ud83d\udcca Sensor convolution summary for &lt;flight_stem&gt; | succeeded=[...] skipped=[...] failed=[...]</code>     followed by <code>\ud83c\udf89 Finished pipeline for &lt;flight_stem&gt;</code>.</li> <li>Failure handling</li> <li>Sensors are processed independently. Missing definitions or write failures mark that     sensor as <code>failed</code> but do not abort the stage unless all sensors fail and none were     previously valid. Partial success is acceptable.</li> </ul>"},{"location":"pipeline/#example-run-transcript","title":"Example run transcript","text":"<p>The restart-safe logs surface the exact work performed. A real rerun for one flight line now looks like:</p> <pre><code>[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83d\ude80 Processing ...\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83d\udce5 stage_download_h5() found existing .h5 (skipping)\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83d\udd0e ENVI export target is ..._envi.img / ..._envi.hdr\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \u2705 ENVI export already complete -&gt; ..._envi.img / ..._envi.hdr (skipping heavy export)\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \u2705 Correction JSON already complete -&gt; ..._brdfandtopo_corrected_envi.json (skipping)\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \u2705 BRDF+topo correction already complete -&gt; ..._brdfandtopo_corrected_envi.img / ..._brdfandtopo_corrected_envi.hdr (skipping)\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83c\udfaf Convolving corrected reflectance\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \u2705 Wrote landsat_tm product -&gt; ..._landsat_tm_envi.img / ..._landsat_tm_envi.hdr\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \u2705 Wrote micasense product -&gt; ..._micasense_envi.img / ..._micasense_envi.hdr\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83d\udcca Sensor convolution summary | succeeded=['landsat_tm', 'micasense'] skipped=['landsat_etm+', 'landsat_oli', 'landsat_oli2'] failed=[]\n[NEON_D13_NIWO_DP1_L019-1_20230815_directional_reflectance] \ud83c\udf89 Finished pipeline\n</code></pre>"},{"location":"pipeline/#parallel-flightline-execution","title":"Parallel flightline execution","text":"<p>After all downloads succeed, <code>go_forth_and_multiply()</code> dispatches each flightline to a <code>ThreadPoolExecutor</code>. The <code>max_workers</code> argument controls the level of concurrency; leave it <code>None</code> to process sequentially. <code>_scoped_log_prefix()</code> prepends <code>[flight_stem]</code> to every log so parallel runs stay readable, and each worker writes only within its own <code>&lt;base&gt;/&lt;flight_stem&gt;/</code> directory to preserve idempotence.</p> <p>When <code>go_forth_and_multiply(...)</code> finishes looping over every requested flight line it logs <code>\u2705 All requested flightlines processed.</code> to confirm site-level completion.</p>"},{"location":"pipeline/#rerun-guidance","title":"Rerun guidance","text":"<p>Call <code>go_forth_and_multiply(...)</code> with the same parameters to rerun an entire site-month. The restart-safe checks ensure that valid artifacts are reused, missing or invalid stages are recomputed, and partial sensor failures do not stop progress across the rest of the flight lines. Because each sensor is accounted for independently, you can inspect the summary lists to see exactly which products succeeded, which were reused, and which need attention.</p>"},{"location":"qa/","title":"Quality Assurance (QA) panels","text":"<p>The <code>cscal-qa</code> command now emits both a PNG panel and a machine-readable <code>*_qa.json</code> file for every flightline. The PNG highlights spectral checks while the JSON records the underlying metrics so you can track drift over time or feed it into dashboards.</p> <pre><code># Deterministic quick pass (\u226425k sampled pixels per flightline)\ncscal-qa --base-folder output_demo --quick\n\n# Exhaustive sampling with custom RGB mapping\ncscal-qa --base-folder output_demo --full --n-sample 150000 --rgb-bands 650,550,480\n</code></pre> <p>Key sections on the panel:</p> <ul> <li>RGB quicklook \u2013 automatically picks 660/560/490 nm (override with   <code>--rgb-bands</code>). Red callouts overlay any flagged issues from the metrics.</li> <li>Histograms \u2013 pre vs post correction distributions with shared bins so you   can judge how BRDF/topo shifts the scene.</li> <li>\u0394 median vs wavelength \u2013 bandwise medians with IQR ribbon; uses header   wavelengths or sensor defaults.</li> <li>Convolved scatter \u2013 compares corrected data to any <code>*_convolved_envi</code>   or <code>*_resampled_&lt;sensor&gt;_envi</code> outputs with a 1:1 reference line.</li> </ul> <p>Each PNG lives alongside <code>&lt;prefix&gt;_qa.json</code>, which mirrors the <code>QAMetrics</code> dataclass (<code>provenance</code>, <code>header</code>, <code>mask</code>, <code>correction</code>, <code>convolution</code>, <code>negatives_pct</code>, <code>overbright_pct</code>, <code>issues</code>). When the brightness correction stage runs, the JSON also lists per-band gain/offsets so the QA team can trace changes back to illumination harmonisation.</p> <p>Use <code>--out-dir</code> if you want to collect all PNG/JSON pairs into a single folder for review. Re-running the command overwrites previous outputs, so a second pass after fixing headers or rerunning corrections always reflects the current state.</p>"},{"location":"qa/#qa-dashboard","title":"QA Dashboard","text":"<p>The legacy <code>cscal-qa-dashboard</code> command still expects <code>_qa_metrics.parquet</code> files. Until the dashboard is updated to read the new JSON schema, keep legacy parquet artifacts if you rely on that summary view.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>This Quickstart gives you two ways to run the pipeline end-to-end:</p> <ol> <li>CLI path \u2013 run <code>cscal-pipeline</code> from a terminal  </li> <li>Notebook path \u2013 run the pipeline inside Jupyter</li> </ol> <p>Both produce the same ENVI, Parquet, and QA artifacts.</p>"},{"location":"quickstart/#install","title":"Install","text":"<p>Install from PyPI:</p> <p>```bash pip install cross-sensor-cal For Ray support: pip install \"cross-sensor-cal[ray]\" 1. CLI path Use the CLI if you run jobs on your laptop or HPC cluster. Run a NEON flight line</p>"},{"location":"quickstart/#choose-an-output-directory","title":"Choose an output directory","text":"<p>BASE=output_quickstart mkdir -p \"$BASE\"</p>"},{"location":"quickstart/#run-the-pipeline","title":"Run the pipeline","text":"<p>cscal-pipeline \\   --base-folder \"$BASE\" \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --max-workers 2 \\   --engine thread The first time this runs, it will: download NEON HDF5 tiles export ENVI cubes apply topographic + BRDF correction convolve to Landsat style reflectance write Parquet tables produce QA PNG, PDF, and JSON summaries If rerun, completed stages are skipped safely. Inspect QA files open $BASE/NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance/_qa.png open $BASE/NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance/_qa.pdf 2. Notebook path (Jupyter) Use this if you want a reproducible, interactive workflow. Python example from cross_sensor_cal import go_forth_and_multiply</p> <p>base = \"output_quickstart_py\"</p> <p>go_forth_and_multiply(     base_folder=base,     site_code=\"NIWO\",     year_month=\"2023-08\",     product_code=\"DP1.30006.001\",     flight_lines=[\"NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance\"],     max_workers=2,     engine=\"thread\", ) Preview merged Parquet import duckdb, os</p> <p>fl = \"NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance\" merged = os.path.join(base, fl, f\"{fl}_merged_pixel_extraction.parquet\")</p> <p>duckdb.query(f\"SELECT * FROM '{merged}' LIMIT 5\").df() For a complete notebook example, see: Usage \u2192 Jupyter notebook example Next steps Why calibration? Tutorials Pipeline overview Working with Parquet</p>"},{"location":"refactor_notes/","title":"Refactor Notes: HyTools-Free Pipeline","text":""},{"location":"refactor_notes/#purpose","title":"Purpose","text":"<p>These notes document the current cross-sensor-cal processing pipeline after removing the runtime dependency on HyTools. The steps below describe what the code does today so collaborators can reproduce results and audit intermediate products.</p>"},{"location":"refactor_notes/#end-to-end-pipeline","title":"End-to-End Pipeline","text":"<ol> <li> <p>Acquire NEON reflectance flightlines    Download or copy the required NEON Airborne Observation Platform (AOP) reflectance HDF5 files to a local workspace before running the pipeline.</p> </li> <li> <p>Convert HDF5 to ENVI without HyTools <code>neon_to_envi_no_hytools()</code> opens the HDF5 file with <code>NeonCube</code>, streams the cube out in spatial tiles, and writes a float32 BSQ ENVI dataset via <code>EnviWriter</code>. It simultaneously exports ancillary rasters (solar/sensor geometry, slope, aspect, etc.) needed for correction. The result is an uncorrected directional reflectance <code>.img/.hdr</code> pair for each flightline. HyTools and Ray are not invoked in this stage\u2014the conversion logic is entirely internal.</p> </li> <li> <p>Persist correction parameters <code>build_and_write_correction_json()</code> (in <code>brdf_topo.py</code>) inspects the flightline geometry, fits BRDF coefficients, and serialises the results as <code>&lt;flightline&gt;_brdfandtopo_corrected_envi.json</code>. The helper validates the JSON via <code>is_valid_json()</code> and reuses it on reruns when intact.</p> </li> <li> <p>Topographic and BRDF correction    The pipeline allocates a new corrected cube and uses <code>EnviWriter</code> to persist it. For every spatial tile it:</p> </li> <li>Reads the tile from the uncorrected ENVI export.</li> <li>Applies topographic correction using slope, aspect, and solar geometry rasters.</li> <li>Applies BRDF correction using the saved coefficient JSON. When coefficients are missing, unreadable, or poorly conditioned, the code logs a warning and falls back to neutral BRDF terms so the tile still receives topographic correction.</li> <li> <p>Optionally adds a <code>brightness_offset</code> before writing.    The corrected output <code>&lt;flightline&gt;_brdfandtopo_corrected_envi.img/.hdr</code> carries full spatial metadata plus the wavelength list, FWHM list, and wavelength units required for spectral resampling.</p> </li> <li> <p>Spectral convolution / sensor simulation <code>convolve_resample_product()</code> opens the corrected cube as a BSQ memmap, reads spatial tiles, transposes them to <code>(y, x, bands)</code>, and multiplies each tile by sensor-specific spectral response functions (SRFs). SRFs are loaded from JSON files under <code>cross_sensor_cal/data/</code> via package-relative paths. Each simulated sensor produces its own float32 BSQ ENVI product and header. Existing resampled outputs are validated with <code>is_valid_envi_pair()</code> and skipped when already complete.</p> </li> <li> <p>Downstream consumers (optional)    Additional tooling can derive pixel stacks, polygon summaries, or parquet tables from the corrected and resampled rasters. These consumers still function but are documented separately and are not detailed here.</p> </li> </ol> <p>Every step performs the same validation checks on reruns so the pipeline is safe to resume after interruptions or partial failures.</p> <ol> <li>Recommended artifact retention    Keep the following per flightline so downstream analyses and cross-sensor comparisons remain reproducible:</li> <li><code>&lt;flightline&gt;_directional_reflectance.img/.hdr</code></li> <li><code>&lt;flightline&gt;_brdfandtopo_corrected_envi.json</code></li> <li><code>&lt;flightline&gt;_brdfandtopo_corrected_envi.img/.hdr</code></li> <li><code>&lt;flightline&gt;_resampled_&lt;sensor&gt;.img/.hdr</code></li> </ol>"},{"location":"refactor_notes/#module-structure","title":"Module Structure","text":"<ul> <li><code>cross_sensor_cal/neon_cube.py</code></li> <li><code>NeonCube</code> class</li> <li>Opens NEON HDF5 reflectance, exposes dimensions, wavelengths, ancillary angles, etc.</li> <li>Iterates spatial tiles without requiring HyTools.</li> <li><code>cross_sensor_cal/envi_writer.py</code></li> <li><code>EnviWriter</code> class</li> <li>Writes BSQ float32 rasters (<code>.img/.hdr</code>).</li> <li>Used for uncorrected export, corrected cubes, and resampled products.</li> <li><code>cross_sensor_cal/corrections.py</code></li> <li><code>fit_and_save_brdf_model()</code></li> <li><code>apply_topo_correct()</code></li> <li><code>apply_brdf_correct()</code></li> <li>Includes helpers to load and apply BRDF coefficients.</li> <li><code>cross_sensor_cal/resample.py</code></li> <li><code>resample_chunk_to_sensor()</code></li> <li>SRF loading utilities</li> <li>Convolution-friendly helpers for chunk-wise processing.</li> <li><code>cross_sensor_cal/pipelines/pipeline.py</code></li> <li><code>go_forth_and_multiply()</code></li> <li>Orchestrates downloads, H5\u2192ENVI export (no HyTools), BRDF fitting, topographic+BRDF correction, and spectral convolution.</li> <li><code>cross_sensor_cal/data/</code></li> <li>SRF JSON files for Landsat, Sentinel, etc.</li> <li>Accessed via package-relative paths at runtime.</li> </ul>"},{"location":"refactor_notes/#licensing","title":"Licensing","text":"<p>Several algorithms and data-handling conventions in <code>NeonCube</code>, the correction routines, and ENVI export logic were adapted from the HyTools project (GPLv3). Although the refactored pipeline no longer imports HyTools at runtime, we continue to credit the original HyTools authors and comply with GPLv3 obligations for the adapted code.</p>"},{"location":"references/","title":"References","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"references/#cross-calibration","title":"Cross-calibration","text":"<ul> <li>Roy, D.P., Kovalskyy, V., Zhang, H., Vermote, E.F., &amp; Yan, L. (2016). Landsat-8 and Landsat-7 cross-calibration. Remote Sensing of Environment.</li> <li>Claverie, M., Ju, J., Masek, J.G., et al. (2018). The Harmonized Landsat and Sentinel-2 surface reflectance data set. Remote Sensing of Environment.</li> </ul>"},{"location":"references/#citing-this-project","title":"Citing this project","text":"<ul> <li>Earth Lab Data Innovation Team. (2025). Cross-Sensor Calibration (Version 2.2.0) [Software]. University of Colorado Boulder. https://github.com/earthlab/cross-sensor-cal</li> </ul>"},{"location":"references/#spectral-libraries","title":"Spectral libraries","text":"<ul> <li>Baldridge, A.M., Hook, S.J., Grove, C.I., &amp; Rivera, G. (2009). The ASTER spectral library version 2.0. Remote Sensing of Environment.</li> <li>Kokaly, R.F., Clark, R.N., et al. (2017). USGS Spectral Library Version 7. U.S. Geological Survey Data Series.</li> </ul>"},{"location":"references/#mesma","title":"MESMA","text":"<ul> <li>Roberts, D.A., Gardner, M., Church, R., Ustin, S., Scheer, G., &amp; Green, R.O. (1998). Mapping chaparral in the Santa Monica Mountains using multiple endmember spectral mixture models. Remote Sensing of Environment.</li> <li>Powell, R.L., Roberts, D.A., Dennison, P.E., &amp; Hess, L.L. (2007). Sub-pixel mapping of urban land cover using MESMA. Remote Sensing of Environment.</li> </ul>"},{"location":"references/#brdftopo","title":"BRDF/topo","text":"<ul> <li>Li, X., &amp; Strahler, A.H. (1992). Geometric-optical bidirectional reflectance modeling of the discrete crown vegetation canopy. IEEE Transactions on Geoscience and Remote Sensing.</li> <li>Schaaf, C.B., Gao, F., et al. (2002). First operational BRDF, albedo, and nadir reflectance products from MODIS. Remote Sensing of Environment.</li> <li>Colby, J.D. (1991). Topographic normalization in remote sensing. Remote Sensing of Environment.</li> </ul>"},{"location":"schemas/","title":"Schemas","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"schemas/#raster-band-metadata-schema","title":"Raster band metadata schema","text":"<p>Describes the properties of each raster band after resampling or convolution.</p> Field Type Units Description <code>band</code> int \u2013 Sequential band number <code>wavelength_nm</code> float nm Center wavelength <code>fwhm_nm</code> float nm Full width at half maximum <code>unit</code> string \u2013 Measurement units for reflectance <p>Example JSON:</p> <pre><code>[\n  {\"band\": 1, \"wavelength_nm\": 450.0, \"fwhm_nm\": 20.0, \"unit\": \"nm\"},\n  {\"band\": 2, \"wavelength_nm\": 550.0, \"fwhm_nm\": 25.0, \"unit\": \"nm\"}\n]\n</code></pre>"},{"location":"schemas/#pixel-table-schema","title":"Pixel table schema","text":"<p>Each row represents one pixel extracted from a raster scene.</p> Column Type Units Description <code>Pixel_ID</code> int \u2013 Unique pixel identifier <code>Pixel_Row</code> int row Raster row index (0\u2011based) <code>Pixel_Col</code> int col Raster column index (0\u2011based) <code>B1..Bn</code> float reflectance Band reflectance values <p>Example table:</p> Pixel_ID Pixel_Row Pixel_Col B1 B2 B3 1 10 15 0.12 0.09 0.03 2 11 16 0.10 0.08 0.02"},{"location":"schemas/#spectral-library-schema","title":"Spectral library schema","text":"<p>Spectral libraries store reference spectra for endmembers used during unmixing. When saved as JSON or Parquet, each record contains:</p> Field Type Description <code>spectrum_id</code> string Unique identifier <code>class_label</code> string Endmember class (e.g., soil, vegetation) <code>wavelength_nm</code> array Wavelength centers <code>reflectance</code> array Corresponding reflectance values <code>metadata</code> object Optional information (sensor, date, notes, etc.) <p>Example JSON entry:</p> <pre><code>{\n  \"spectrum_id\": \"veg01\",\n  \"class_label\": \"vegetation\",\n  \"wavelength_nm\": [450, 550, 650],\n  \"reflectance\": [0.12, 0.32, 0.45],\n  \"metadata\": {\"sensor\": \"NEON\", \"acquired\": \"2020-08-01\"}\n}\n</code></pre>"},{"location":"schemas/#mesma-outputs-schema","title":"MESMA outputs schema","text":"<p>Results from Multiple Endmember Spectral Mixture Analysis for each pixel.</p> Column Type Units Description <code>Pixel_ID</code> int \u2013 Input pixel identifier <code>fraction_*</code> float fraction Fractional abundance per endmember band <code>RMSE</code> float reflectance Root mean square error of the model fit <code>QA</code> int \u2013 Quality flag (0=good, higher=worse) <p>Example table:</p> Pixel_ID fraction_soil fraction_veg RMSE QA 1 0.40 0.60 0.01 0 2 0.55 0.45 0.02 1 <p>Example JSON line for one pixel:</p> <pre><code>{\n  \"Pixel_ID\": 1,\n  \"fraction_soil\": 0.40,\n  \"fraction_veg\": 0.60,\n  \"RMSE\": 0.01,\n  \"QA\": 0\n}\n</code></pre> <p>Last updated: 2025-08-18</p>"},{"location":"stage-03-pixel-extraction/","title":"Stage 03 Pixel Extraction","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"stage-03-pixel-extraction/#overview","title":"Overview","text":"<p>At this stage you sample pixels from the sorted scenes and write the values to a tabular file. The table becomes the input to spectral unmixing and other downstream analyses.</p>"},{"location":"stage-03-pixel-extraction/#sampling-rules","title":"Sampling rules","text":"<ol> <li>Define a consistent random seed so repeated runs draw the same pixels.</li> <li>Sample within each land\u2010cover class or tile to avoid geographic bias.</li> <li>Drop any pixel flagged by a quality mask or falling outside the region of interest.</li> </ol>"},{"location":"stage-03-pixel-extraction/#handling-nodata-and-masks","title":"Handling nodata and masks","text":"<ul> <li>Treat nodata values (<code>-9999</code> by default) as missing and skip those records.</li> <li>Apply cloud, shadow, and water masks before sampling so invalid pixels never reach the table.</li> <li>Keep a boolean <code>is_masked</code> column to track which values were rejected.</li> </ul>"},{"location":"stage-03-pixel-extraction/#tile-vs-full-scene","title":"Tile vs full scene","text":"<ul> <li>Tiles scale better for large mosaics and let you parallelize extraction.</li> <li>Full scenes are faster when memory allows and ensure contiguous coverage. Choose the approach that matches your hardware and scene size; the output format is identical.</li> </ul>"},{"location":"stage-03-pixel-extraction/#output-tables","title":"Output tables","text":"<p>Each row represents one pixel. Columns typically include <code>scene_id</code>, <code>tile_id</code>, <code>x</code>, <code>y</code>, band values, and <code>is_masked</code>. Write tables as CSV for quick inspection or Parquet for efficient storage. Keep Parquet outputs in a <code>full_extracted_pixels</code> folder that lives alongside the tile folder so the extracted tables sit next to, not inside, the source data. Partition by scene and tile so you can read subsets without loading the whole dataset.</p>"},{"location":"stage-03-pixel-extraction/#memory-tips","title":"Memory tips","text":"<ul> <li>Process one tile at a time and release arrays with <code>del</code> to free RAM.</li> <li>When writing CSV, stream rows with a generator instead of building a huge DataFrame.</li> <li>Prefer Parquet with compression to reduce disk use and load times.</li> </ul>"},{"location":"stage-03-pixel-extraction/#quick-integrity-checks","title":"Quick integrity checks","text":"<ul> <li>Confirm row counts match the number of valid pixels expected per tile.</li> <li>Scan for remaining nodata values: <code>rg -n \"-9999\" sample.csv</code>.</li> <li>Plot a histogram of one band to detect obvious outliers before moving on.</li> </ul>"},{"location":"stage-03-pixel-extraction/#next-steps","title":"Next steps","text":"<p>Continue to Stage 04 to build the spectral library from the extracted pixels.</p> <p>Last updated: 2025-08-18</p>"},{"location":"stage-04-spectral-library/","title":"Stage 04 Spectral Library","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <p>The spectral library stores every spectrum with its full provenance. Each entry includes:</p> <ul> <li><code>site</code>: location identifier where you collected the field sample.</li> <li><code>sensor</code>: instrument or platform that measured the spectrum.</li> <li><code>wavelengths</code>: array of nanometer values shared across spectra.</li> </ul> <p>These fields live alongside the reflectance values in a row-oriented table or NetCDF group. Use them to filter spectra by site, compare sensors, or align data to the wavelength grid.</p>"},{"location":"stage-04-spectral-library/#quality-controls","title":"Quality controls","text":"<p>You can clean spectra before analysis:</p> <ol> <li>Outlier filtering \u2013 drop samples that exceed three standard deviations from the mean reflectance    at any wavelength.</li> <li>Smoothing \u2013 apply a Savitzky\u2013Golay or moving-average filter to reduce instrument noise while    preserving absorption features.</li> <li>Signal-to-noise ratio (SNR) \u2013 flag spectra with low SNR and exclude them from downstream    modeling.</li> </ol>"},{"location":"stage-04-spectral-library/#versioning-and-provenance","title":"Versioning and provenance","text":"<p>Each library release increments a semantic version. A <code>manifest.json</code> file lists source datasets, processing code commits, and software versions so you can reproduce the library or audit its origin.</p> <p>Last updated: 2025-08-18</p>"},{"location":"stage-05-mesma/","title":"Stage 05 MESMA","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p>"},{"location":"stage-05-mesma/#using-the-spectral-library","title":"Using the spectral library","text":"<p>The spectral library from Stage 04 provides the candidate endmember spectra. You load it as a NumPy array and pass it to the MESMA routine alongside the target image. MESMA iterates through library combinations to find the model with the lowest root mean square error (RMSE).</p> <pre><code>from unmixing.el_mesma import MesmaCore\n\nmesma = MesmaCore()\nfractions, residuals = mesma._mesma(image, library)\n</code></pre>"},{"location":"stage-05-mesma/#endmember-selection-strategies","title":"Endmember selection strategies","text":"<ul> <li>Exhaustive search \u2013 evaluate all combinations up to a fixed complexity.</li> <li>Class-based \u2013 restrict models to endmembers drawn from predefined   classes such as vegetation or soil.</li> <li>Random sampling \u2013 sample combinations to reduce runtime for large   libraries.</li> </ul>"},{"location":"stage-05-mesma/#outputs","title":"Outputs","text":"<ul> <li>Per-endmember fraction maps showing the proportional contribution of each   material and a shade fraction.</li> <li>A residual raster capturing the difference between observed and reconstructed   spectra.</li> </ul>"},{"location":"stage-05-mesma/#validation","title":"Validation","text":"<ul> <li>Verify that the fractions for each pixel sum to approximately <code>1.0</code>.</li> <li>Discard models with RMSE above a user-defined threshold to ensure a reliable   fit.</li> </ul> <p>Last updated: 2025-08-18</p>"},{"location":"troubleshooting/","title":"Troubleshooting Guide","text":"<p>This page lists common problems encountered when running the cross-sensor-cal pipeline, along with likely causes and recommended solutions. It is organized by pipeline stage.</p>"},{"location":"troubleshooting/#general-issues","title":"General issues","text":""},{"location":"troubleshooting/#the-pipeline-crashes-or-silently-stops","title":"The pipeline crashes or silently stops","text":"<p>Likely causes: - out-of-memory (OOM) errors - temporary directory filling up - unexpected NEON file structure  </p> <p>Solutions: - reduce <code>--max-workers</code> - set <code>CSCAL_TMPDIR</code> to a larger scratch disk - run stages independently using <code>--start-at</code> and <code>--end-at</code></p>"},{"location":"troubleshooting/#download-hdf5-access-issues","title":"Download &amp; HDF5 access issues","text":""},{"location":"troubleshooting/#missing-or-corrupted-hdf5-tiles","title":"Missing or corrupted HDF5 tiles","text":"<ul> <li>NEON occasionally updates directory structures  </li> <li>Cloud storage sessions may time out  </li> </ul> <p>Solutions: - manually verify the HDF5 path - re-run pipeline; downloads are restart-safe - use a fresh working directory  </p>"},{"location":"troubleshooting/#hdf5-envi-export-issues","title":"HDF5 \u2192 ENVI export issues","text":""},{"location":"troubleshooting/#envi-header-not-recognized-or-wavelengths-missing","title":"ENVI header not recognized or wavelengths missing","text":"<p>Causes: - malformed or incomplete HDF5 metadata - older NEON tiles with inconsistent naming conventions  </p> <p>Solutions: - verify the HDF5 product name includes <code>directional_reflectance</code> - ensure correct product code (usually <code>DP1.30006.001</code>) - re-run the export stage only:</p> <p>```bash cscal-pipeline --start-at export-envi --end-at export-envi ... ENVI export produces extremely large or slow files This stage is I/O intensive. Solutions: avoid running many exports concurrently use local SSD scratch storage use thread engine instead of Ray for single-tile workflows Topographic correction issues Dark or clipped areas after topo correction Causes: deep shadows DEM mismatch slope/aspect irregularities Solutions: check DEM resolution visually inspect slope/aspect rasters mask problematic areas if needed BRDF correction issues BRDF correction produces NaNs or extreme values Causes: unstable BRDF coefficient fitting missing or unrealistic geometry values low-SNR or noisy bands Solutions: inspect BRDF coefficients in QA JSON limit BRDF correction to certain bands check for invalid view/solar geometry Sudden brightness shift after BRDF Possible causes: incorrect per-band scaling extreme solar/view geometry BRDF coefficients failing to converge Check the QA PNG or PDF to verify brightness changes. Convolution (sensor harmonization) issues Landsat-convolved reflectance looks wrong Causes: wavelength mismatch empty or incorrect SRF tables bright or dark artifacts from BRDF stage Solutions: inspect SRF metadata in QA JSON confirm NEON wavelengths match expected ranges compare band means to expected Landsat reflectance ranges Brightness coefficients are unusually large This indicates poor alignment between corrected spectra and sensor response functions. Check: reflectance scaling BRDF coefficient stability QA brightness plots Parquet extraction issues Memory errors during extraction or merge Solutions: reduce number of workers increase scratch space use DuckDB for large-table operations rather than pandas QA issues QA PNG or PDF missing Causes: pipeline interrupted before QA stage insufficient permissions in output directory Re-run: cscal-pipeline --start-at qa ... When to reach out for help If the pipeline produces persistent artifacts, consider: sharing the QA PNG/PDF sharing a small snippet of metadata describing environment, RAM, and tile size These provide critical clues about where failure occurs. Next steps Pipeline stages QA metrics</p>"},{"location":"validation/","title":"Validation","text":"<p>DO NOT EDIT OUTSIDE MARKERS</p> <p>The validation procedure ensures quality across the pipeline.</p>"},{"location":"validation/#stage-01-raster-processing","title":"Stage 01 \u2013 Raster Processing","text":"<ul> <li>[ ] Confirm input rasters exist and are readable.</li> <li>[ ] Check projection and resolution match expected values.</li> <li>[ ] Verify no bands contain all nodata values.</li> <li>[ ] Ensure output rasters write successfully.</li> </ul> <pre><code>import rasterio, numpy as np\nwith rasterio.open(\"image.tif\") as src:\n    data = src.read()\n    assert not np.isnan(data).all(axis=(1, 2))\n</code></pre>"},{"location":"validation/#stage-02-sorting","title":"Stage 02 \u2013 Sorting","text":"<ul> <li>[ ] Confirm filenames follow <code>YYYYMMDD_sensor.tif</code> pattern.</li> <li>[ ] Check chronological ordering after sorting.</li> <li>[ ] Verify number of files per date matches expected counts.</li> </ul> <pre><code>import pandas as pd, glob\nfiles = sorted(glob.glob(\"sorted/*.tif\"))\ndates = pd.to_datetime([f.split(\"_\")[0] for f in files])\nassert dates.is_monotonic_increasing\n</code></pre>"},{"location":"validation/#stage-03-pixel-extraction","title":"Stage 03 \u2013 Pixel Extraction","text":"<ul> <li>[ ] Ensure sample coordinates fall within raster bounds.</li> <li>[ ] Validate pixel value ranges for each band.</li> <li>[ ] Cross-check sample count with original list.</li> </ul> <pre><code>import numpy as np, pandas as pd\npixels = pd.read_csv(\"pixels.csv\")\nassert ((pixels['x']&gt;=0) &amp; (pixels['y']&gt;=0)).all()\nassert pixels.drop(columns=['x','y']).apply(np.isfinite).all().all()\n</code></pre>"},{"location":"validation/#stage-04-spectral-library","title":"Stage 04 \u2013 Spectral Library","text":"<ul> <li>[ ] Verify spectra length equals number of bands.</li> <li>[ ] Check for duplicated materials or IDs.</li> <li>[ ] Inspect outlier reflectance values.</li> </ul> <pre><code>import pandas as pd\nlib = pd.read_csv(\"library.csv\")\nlib.groupby(\"material\").size().pipe(print)\nassert (lib.filter(like=\"band\") &lt;= 1).all().all()\n</code></pre>"},{"location":"validation/#stage-05-mesma","title":"Stage 05 \u2013 MESMA","text":"<ul> <li>[ ] Confirm endmember sets sum to \u22641.</li> <li>[ ] Review residual errors per pixel.</li> <li>[ ] Flag negative abundance values.</li> </ul> <pre><code>import pandas as pd\nabund = pd.read_csv(\"mesma_output.csv\")\nassert (abund.filter(like=\"EM\").sum(axis=1) &lt;= 1.01).all()\nassert (abund.filter(like=\"EM\") &gt;= 0).all().all()\n</code></pre>"},{"location":"_build/doc_drift_report/","title":"Documentation Drift Report","text":""},{"location":"_build/doc_drift_report/#naming-rule","title":"Naming rule","text":"<ul> <li>Merged parquet naming in code: _merged_pixel_extraction.parquet"},{"location":"_build/doc_drift_report/#missing-items-in-docs","title":"Missing items in docs","text":"<ul> <li>Outputs: \u2014</li> <li>Sensors: micasense_to_match_oli_and_oli, micasense_to_match_oli_oli, micasense_to_match_tm_etm_plus</li> </ul>"},{"location":"_build/doc_drift_report/#stale-items-in-docs","title":"Stale items in docs","text":"<ul> <li>Outputs: _landsat_convolved_envi.hdr, _landsat_convolved_envi.img, _landsat_etm+_envi.img, _landsat_oli_envi.parquet, _landsat_tm_envi.hdr, _landsat_tm_envi.img, _micasense*_envi.img, _micasense_envi.hdr, _micasense_envi.img</li> <li>Sensors: MicaSense, MicaSense), MicaSense,, MicaSense,,, MicaSense,,,, MicaSense,,,,, MicaSense,,,,,, MicaSense,,,,,,, MicaSense,,,,,,,, MicaSense-matched, Micasense,, Micasense,,, Micasense,,,, Micasense,,,,, Micasense,,,,,, Micasense,,,,,,, Micasense,,,,,,,, micasense'], micasense'],, micasense'],,, micasense'],,,, micasense'],,,,, micasense'],,,,,, micasense'],,,,,,, micasense_envi.img,, micasense_envi.img<code>,, micasense-to-landsat, micasense.json</code>, micasense_envi\",, micasense_envi.hdr, micasense_envi.hdr,, micasense_envi.hdr,,, micasense_envi.hdr,,,, micasense_envi.hdr,,,,, micasense_envi.hdr,,,,,, micasense_envi.hdr,,,,,,, micasense_envi.img, micasense_envi.img,, micasense_envi.img,,, micasense_envi.img,,,, micasense_envi.img,,,,, micasense_envi.img,,,,,, micasense_envi.img,,,,,,, micasense_envi.img/.hdr/.parquet, micasense_envi.img/.hdr/.parquet,, micasense_envi.img/.hdr/.parquet,,, micasense_envi.img/.hdr/.parquet,,,, micasense_envi.img/.hdr/.parquet,,,,, micasense_envi.img/.hdr/.parquet,,,,,, micasense_envi.img/.hdr/.parquet,,,,,,, micasense_input, micasense_landsat_harmonized.parquet\"), micasense_to_landsat.csv, micasense_to_match_oli_and_oli,, micasense_to_match_oli_and_oli,,, micasense_to_match_oli_and_oli,,,, micasense_to_match_oli_and_oli,,,,, micasense_to_match_oli_and_oli,,,,,, micasense_to_match_oli_and_oli,,,,,,, micasense_to_match_oli_oli,, micasense_to_match_oli_oli,,, micasense_to_match_oli_oli,,,, micasense_to_match_oli_oli,,,,, micasense_to_match_oli_oli,,,,,, micasense_to_match_oli_oli,,,,,,, micasense_to_match_tm_etm+,,,,,,, micasense<code>, micasense</code>)</li> </ul>"},{"location":"_build/doc_drift_report/#entry-points-pyproject","title":"Entry points (pyproject)","text":"<p>{   \"cscal-download\": \"cross_sensor_cal.cli:download_main\",   \"cscal-pipeline\": \"cross_sensor_cal.cli.pipeline_cli:main\",   \"cscal-qa\": \"cross_sensor_cal.cli.qa_cli:main\",   \"cscal-recover-raw\": \"cross_sensor_cal.cli.recover_cli:main\",   \"cscal-qa-dashboard\": \"cross_sensor_cal.qa_dashboard:main\",   \"csc-merge-duckdb\": \"cross_sensor_cal.merge_duckdb:main\" }</p>"},{"location":"_build/doc_drift_report/#cli-flags-discovered","title":"CLI flags discovered","text":"<p>{   \"topo_and_brdf_correction.py\": [     \"--config_file\"   ],   \"neon_to_envi.py\": [     \"--brightness-offset\"   ],   \"qa_dashboard.py\": [     \"--base-folder\",     \"--out-parquet\",     \"--out-png\"   ],   \"merge_duckdb.py\": [     \"--corrected-glob\",     \"--data-root\",     \"--flightline-glob\",     \"--merge-memory-limit\",     \"--merge-row-group-size\",     \"--merge-temp-directory\",     \"--merge-threads\",     \"--no-qa\",     \"--original-glob\",     \"--out-name\",     \"--resampled-glob\",     \"--write-feather\"   ],   \"standard_resample.py\": [     \"--hdr_path\",     \"--json_file\",     \"--output_path\",     \"--resampling_file_path\",     \"--sensor_type\"   ],   \"qa_cli.py\": [     \"--base-folder\",     \"--full\",     \"--n-sample\",     \"--out-dir\",     \"--quick\",     \"--rgb-bands\",     \"--save-json\"   ],   \"recover_cli.py\": [     \"--base-folder\",     \"--brightness-offset\"   ],   \"pipeline_cli.py\": [     \"--base-folder\",     \"--brightness-offset\",     \"--engine\",     \"--flight-lines\",     \"--max-workers\",     \"--merge-memory-limit\",     \"--merge-row-group-size\",     \"--merge-temp-directory\",     \"--merge-threads\",     \"--parquet-chunk-size\",     \"--product-code\",     \"--resample-method\",     \"--site-code\",     \"--year-month\"   ],   \"pipeline.py\": [     \"--brightness-offset\",     \"--no-sync\",     \"--polygon_layer_path\",     \"--reflectance-offset\",     \"--remote-prefix\",     \"--resample-method\",     \"--verbose\"   ],   \"download.py\": [     \"--flight\",     \"--output\",     \"--product\",     \"--year-month\"   ] }</p>"},{"location":"_build/doc_drift_report/#stage-markers-found-in-logs","title":"Stage markers found in logs","text":"<ul> <li>Writing Parquet with row_group_size=%s</li> <li>[merge] \u26a0\ufe0f QA panel after merge failed for {prefix}: {e}</li> <li>[merge] \ud83d\uddbc\ufe0f  QA panel \u2192 {out_png_path} </li> <li>[{flight_stem}] \u2b07\ufe0f Download already complete \u2014 reusing existing files ({h5_path.name})</li> <li>[{flight_stem}] \ud83c\udf10 Downloading {flight_stem} ({site_code}, {year_month}) into {h5_path} ...</li> <li>\u23ed\ufe0f Parquet already present for %s -&gt; %s (skipping)</li> <li>\u26a0\ufe0f Cannot create Parquet work directory for %s: %s</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .hdr is missing or empty</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .img is missing or empty</li> <li>\u26a0\ufe0f Cannot locate work directory for Parquet export: %s</li> <li>\u26a0\ufe0f Cannot remove invalid Parquet for %s (%s)</li> <li>\u26a0\ufe0f Existing Parquet for %s is invalid (%s); regenerating</li> <li>\u26a0\ufe0f Failed Parquet export for %s: %s</li> <li>\u26a0\ufe0f Merge failed for {future_map[future]}: {exc}</li> <li>\u26a0\ufe0f Parquet validation reported an issue for %s: %s</li> <li>\u26a0\ufe0f Skipping DuckDB merge for %s because no Parquet outputs were produced</li> <li>\u2705 BRDF+topo correction already complete for %s -&gt; %s / %s (skipping)</li> <li>\u2705 BRDF+topo correction already complete for %s, skipping</li> <li>\u2705 BRDF+topo correction completed for %s -&gt; %s / %s</li> <li>\u2705 Discovered existing ENVI export \u2192 {discovered_img.name} / {discovered_hdr.name}</li> <li>\u2705 Download complete for %s \u2192 %s</li> <li>\u2705 ENVI export already complete for %s -&gt; %s / %s (skipping heavy export)</li> <li>\u2705 ENVI export created successfully \u2192 {raw_img_path.name} / {raw_hdr_path.name}</li> <li>\u2705 ENVI export found via discovery \u2192 {discovered_img.name} / {discovered_hdr.name}</li> <li>\u2705 Existing ENVI export found \u2014 skipping heavy export ({raw_img_path.name} / {raw_hdr_path.name})</li> <li>\u2705 Parquet stage complete for %s</li> <li>\u2705 Wrote Parquet for %s -&gt; %s</li> <li>\u2728 Existing ENVI exports detected \u2014 pipeline will reuse validated files automatically.</li> <li>\ud83c\udf89 Finished pipeline for %s</li> <li>\ud83c\udf89 Finished pipeline for %s (parallel worker join)</li> <li>\ud83c\udfaf Convolving corrected reflectance for %s</li> <li>\ud83d\udccd Preparing ENVI export at: {raw_img_path.name} / {raw_hdr_path.name}</li> <li>\ud83d\udce6 No existing ENVI export detected \u2014 creating a new one from source data {h5_path.name}</li> <li>\ud83d\udce6 Parquet export for %s ...</li> <li>\ud83e\uddf1 Parquet row group size = %s (legacy=%s)</li> <li>\ud83e\uddf1 Using Parquet row group size of %s rows</li> </ul> <p>TODO: Review sensor labels in docs for formatting issues.</p>"},{"location":"api/","title":"Python API","text":"<p>Use these functions when you need fine-grained control beyond the CLI.</p>"},{"location":"api/#quick-example","title":"Quick example","text":"<pre><code>from cross_sensor_cal import merge_duckdb\nmerge_duckdb([\"parquet/a.parquet\",\"parquet/b.parquet\"], \"merged/all.parquet\")\n</code></pre>"},{"location":"api/#brightness-correction-entry-point","title":"Brightness correction entry point","text":""},{"location":"api/#apply_brightness_correctioncube-masknone-methodpercentile_match","title":"<code>apply_brightness_correction(cube, mask=None, method='percentile_match', ...)</code>","text":"<p>Normalizes per-band brightness for hyperspectral cubes before BRDF/topo stages. The docstring walks through the affine model, parameter choices, and examples. Use it when you need to harmonise tiles prior to the full pipeline; the QA JSON will surface the per-band gain/offsets when this stage runs.</p> <p>Cross-Sensor Calibration public package surface.</p>"},{"location":"api/#cross_sensor_cal._PLOT_EXPORTS","title":"<code>_PLOT_EXPORTS = (make_micasense_vs_landsat_panels.__name__, make_sensor_vs_neon_panels.__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/#cross_sensor_cal.__all__","title":"<code>__all__ = sorted(set(__all__ + (['apply_brightness_correction', load_brightness_coefficients.__name__] + list(_PLOT_EXPORTS))))</code>  <code>module-attribute</code>","text":""},{"location":"api/#cross_sensor_cal.__version__","title":"<code>__version__ = '2.2.0'</code>  <code>module-attribute</code>","text":""},{"location":"api/#cross_sensor_cal.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"api/#cross_sensor_cal.load_brightness_coefficients","title":"<code>load_brightness_coefficients(system_pair='landsat_to_micasense')</code>","text":"<p>Load brightness coefficients for a given system pair.</p>"},{"location":"api/#cross_sensor_cal.load_brightness_coefficients--parameters","title":"Parameters","text":"<p>system_pair : str     Key identifying the pair of systems, e.g. \"landsat_to_micasense\".</p>"},{"location":"api/#cross_sensor_cal.load_brightness_coefficients--returns","title":"Returns","text":"<p>dict[int, float]     Mapping from 1-based band index to brightness coefficient (percent).</p>"},{"location":"api/#cross_sensor_cal.load_brightness_coefficients--notes","title":"Notes","text":"<ul> <li>Values are stored in percent (e.g., -7.3959 means \"reduce by 7.3959%\").</li> </ul>"},{"location":"api/#cross_sensor_cal.make_micasense_vs_landsat_panels","title":"<code>make_micasense_vs_landsat_panels(merged_dir, out_dir=None, *, max_points=50000, sentinel_at_or_below=-9999.0)</code>","text":""},{"location":"api/#cross_sensor_cal.make_sensor_vs_neon_panels","title":"<code>make_sensor_vs_neon_panels(merged_dir, out_dir=None, *, max_points=50000, sentinel_at_or_below=-9999.0)</code>","text":""},{"location":"api/merge_duckdb/","title":"API: merge_duckdb","text":"<p>Merge all pixel-level parquet tables for one flightline.</p>"},{"location":"api/merge_duckdb/#cross_sensor_cal.merge_duckdb.merge_flightline--parameters","title":"Parameters","text":"<p>flightline_dir : Path     Directory containing the flightline's parquet outputs. out_name : str, optional     Custom name for the merged parquet. If None, defaults to:     _merged_pixel_extraction.parquet original_glob : str, optional     Glob used to locate original reflectance parquet tables. corrected_glob : str, optional     Glob used to locate corrected reflectance parquet tables. resampled_glob : str, optional     Glob used to locate resampled sensor parquet tables. write_feather : bool, optional     If True, writes a Feather copy of the merged table alongside the parquet. emit_qa_panel : bool, default True     If True, renders the standard QA panel (_qa.png) after merging. ray_cpus : int, optional     CPU budget forwarded to Ray validation of Parquet shards. Defaults to     <code>None</code> which allows the Ray helper to choose the configured default. merge_memory_limit_gb : float or str, optional     Upper bound on DuckDB's memory usage. Floats are interpreted as GiB.     Provide <code>None</code> to leave the default DuckDB behaviour unchanged. merge_threads : int, optional     Number of threads DuckDB should use for the merge. <code>None</code> keeps the     engine default (usually <code>os.cpu_count()</code>). merge_row_group_size : int or None, optional     Target number of rows per Parquet row group in the merged output.     If None, DuckDB will auto-determine the optimal size (better streaming performance). merge_temp_directory : Path, optional     Directory where DuckDB should spill temporary data. Defaults to a     <code>.duckdb_tmp</code> subdirectory inside <code>flightline_dir</code>."},{"location":"api/merge_duckdb/#cross_sensor_cal.merge_duckdb.merge_flightline--returns","title":"Returns","text":"<p>Path     Path to the merged parquet file.</p>"},{"location":"api/qa_plots/","title":"API: qa_plots","text":"<p>Return (png_path, metrics_dict); also writes _qa.json/_qa.pdf when requested.</p>"},{"location":"concepts/why-calibration/","title":"Why cross-sensor calibration?","text":"<p>Understanding vegetation, disturbance, and ecosystem structure often requires integrating information from multiple sensors operating at different spatial, temporal, and spectral scales. These include:</p> <ul> <li>NEON airborne imaging spectroscopy  </li> <li>drone multispectral systems (e.g., MicaSense)  </li> <li>moderate-resolution satellites like Landsat and Sentinel  </li> </ul> <p>Each sensor \u201csees\u201d the landscape differently, and these differences can obscure the ecological signals we care about unless we account for them.</p>"},{"location":"concepts/why-calibration/#the-problem-apples-and-oranges-reflectance","title":"The problem: apples and oranges reflectance","text":"<p>Sensors differ in:</p> <ul> <li>spectral response (band centers, widths, shapes)  </li> <li>illumination geometry (solar zenith, azimuth, atmospheric path)  </li> <li>viewing geometry (sensor zenith, azimuth)  </li> <li>radiometric scaling and masking conventions </li> <li>ground sampling distance and spatial aggregation </li> </ul> <p>Even when they image the same location on the same day, their raw reflectance values are not directly comparable.</p> <p>This creates challenges when trying to:</p> <ul> <li>validate satellite products using NEON  </li> <li>relate drone measurements to NEON or Landsat  </li> <li>build cross-scale ecological models  </li> <li>interpret changes in reflectance through time or across terrain  </li> </ul>"},{"location":"concepts/why-calibration/#correcting-vs-harmonizing","title":"Correcting vs. harmonizing","text":"<p>cross-sensor-cal performs two distinct operations:</p>"},{"location":"concepts/why-calibration/#1-physical-corrections","title":"1. Physical corrections","text":"<p>These aim to reduce variation caused by illumination and terrain:</p> <ul> <li>topographic correction (slope and aspect effects)  </li> <li>BRDF correction (view/sun geometry effects)  </li> </ul> <p>The result is a reflectance product that is more comparable across acquisition conditions.</p>"},{"location":"concepts/why-calibration/#2-sensor-harmonization","title":"2. Sensor harmonization","text":"<p>This converts corrected hyperspectral data into another sensor\u2019s bandspace by integrating spectra against published spectral response functions (e.g., Landsat OLI, MicaSense RedEdge).</p> <p>Optional brightness adjustments are documented in the QA outputs.</p>"},{"location":"concepts/why-calibration/#why-neon-as-the-foundation","title":"Why NEON as the foundation?","text":"<p>NEON AOP data provide:</p> <ul> <li>high spectral resolution  </li> <li>per-pixel geometry information  </li> <li>consistent radiometric processing  </li> <li>spatial coverage aligned with ecological research sites  </li> </ul> <p>These properties make NEON a powerful intermediary between plot-scale measurements and satellite observations.</p> <p>cross-sensor-cal implements a reproducible stepwise process to:</p> <ol> <li>extract NEON reflectance into ENVI  </li> <li>correct it physically  </li> <li>harmonize it to satellite/drone sensors  </li> <li>output analysis-ready tables and QA documentation  </li> </ol>"},{"location":"concepts/why-calibration/#what-still-requires-care","title":"What still requires care","text":"<p>Even after calibration and harmonization:</p> <ul> <li>residual BRDF effects can remain  </li> <li>atmospheric differences between sensors matter  </li> <li>snow, smoke, water, and shadows require attention  </li> <li>scale mismatch affects interpretation  </li> <li>masks and quality flags differ across platforms  </li> </ul> <p>The pipeline aims to make all assumptions explicit\u2014every major processing step writes a JSON sidecar describing inputs, parameters, and results.</p>"},{"location":"concepts/why-calibration/#where-to-go-next","title":"Where to go next","text":"<ul> <li> <p>Learn how each pipeline stage works: Pipeline overview &amp; stages</p> </li> <li> <p>Follow a practical workflow: NEON \u2192 corrected ENVI</p> </li> <li> <p>Explore validation and metrics: QA panels &amp; metrics</p> </li> </ul>"},{"location":"dev/architecture/","title":"Package Architecture","text":"<p>This page describes how cross-sensor-cal is organized internally. Understanding this structure helps contributors extend the pipeline or integrate new sensors.</p>"},{"location":"dev/architecture/#high-level-design","title":"High-level design","text":"<p>The package is built around a stage-based pipeline. Each stage:</p> <ul> <li>consumes well-defined inputs  </li> <li>writes ENVI/Parquet outputs  </li> <li>logs metadata to JSON sidecar files  </li> <li>is restart-safe  </li> </ul> <p>Stages are orchestrated by the main CLI driver.</p>"},{"location":"dev/architecture/#directory-structure-python-package","title":"Directory structure (Python package)","text":"<p>cross_sensor_cal/ pipeline/ download.py export_envi.py topo.py brdf.py convolution.py parquet.py qa.py utils/ data/ srf/ regression/</p>"},{"location":"dev/architecture/#pipeline-modules","title":"Pipeline modules","text":""},{"location":"dev/architecture/#downloadpy","title":"<code>download.py</code>","text":"<p>Fetches NEON HDF5 tiles.</p>"},{"location":"dev/architecture/#export_envipy","title":"<code>export_envi.py</code>","text":"<p>Extracts directional reflectance and writes ENVI + metadata.</p>"},{"location":"dev/architecture/#topopy","title":"<code>topo.py</code>","text":"<p>Applies DEM-based topographic correction.</p>"},{"location":"dev/architecture/#brdfpy","title":"<code>brdf.py</code>","text":"<p>Computes BRDF coefficients and corrects reflectance.</p>"},{"location":"dev/architecture/#convolutionpy","title":"<code>convolution.py</code>","text":"<p>Performs sensor bandpass integration.</p>"},{"location":"dev/architecture/#parquetpy","title":"<code>parquet.py</code>","text":"<p>Extracts pixel-level data and merges tables.</p>"},{"location":"dev/architecture/#qapy","title":"<code>qa.py</code>","text":"<p>Generates PNG, PDF, and JSON QA outputs.</p>"},{"location":"dev/architecture/#data-assets","title":"Data assets","text":"<p>Stored under <code>cross_sensor_cal/data/</code>:</p> <ul> <li>spectral response functions (SRFs)  </li> <li>MicaSense \u2192 Landsat regression tables  </li> <li>wavelength lookup and metadata reference files  </li> </ul>"},{"location":"dev/architecture/#adding-a-new-sensor","title":"Adding a new sensor","text":"<ol> <li>Add SRF tables under <code>data/srf/</code> </li> <li>Update convolution stage mappings  </li> <li>Add tests and metadata checks  </li> <li>Document the new sensor in tutorials and reference pages  </li> </ol>"},{"location":"dev/architecture/#next-steps","title":"Next steps","text":"<ul> <li>Contributing &amp; development workflow </li> <li>Guidelines for AI/Codex edits</li> </ul>"},{"location":"dev/codex-guidelines/","title":"Guidelines for AI / Codex Edits","text":"<p>This page defines explicit rules for using AI tools (such as GitHub Copilot, ChatGPT, or Codex) when editing this repository. The goal is to maintain scientific accuracy, structural integrity, and documentation consistency.</p>"},{"location":"dev/codex-guidelines/#core-rules","title":"Core rules","text":""},{"location":"dev/codex-guidelines/#1-do-not-invent-scientific-claims","title":"1. Do NOT invent scientific claims","text":"<p>All scientific statements must come from: - NEON documentation - sensor metadata - existing package logic - peer-reviewed sources  </p>"},{"location":"dev/codex-guidelines/#2-do-not-alter-conceptual-meaning-without-review","title":"2. Do NOT alter conceptual meaning without review","text":"<p>Edits must preserve: - scientific correctness - processing assumptions - pipeline logic  </p>"},{"location":"dev/codex-guidelines/#3-do-not-change-navigation-or-folder-structure-unless-instructed","title":"3. Do NOT change navigation or folder structure unless instructed","text":"<p>Documentation layout is intentional and should not be reorganized automatically.</p>"},{"location":"dev/codex-guidelines/#4-do-not-remove-placeholders-schemas-or-metadata","title":"4. Do NOT remove placeholders, schemas, or metadata","text":"<p>These are required for reproducibility.</p>"},{"location":"dev/codex-guidelines/#code-editing-rules","title":"Code-editing rules","text":"<ul> <li>Do not introduce new APIs without discussion  </li> <li>Do not refactor internal architecture without review  </li> <li>Avoid automatic renaming of variables or functions  </li> <li>Confirm that unit tests pass after any edit  </li> </ul>"},{"location":"dev/codex-guidelines/#documentation-editing-rules","title":"Documentation-editing rules","text":"<ul> <li>Only modify the files explicitly referenced in a prompt  </li> <li>Never rewrite entire sections unless provided with exact text  </li> <li>Preserve all fenced code blocks  </li> <li>Maintain consistency with mkdocs navigation  </li> </ul>"},{"location":"dev/codex-guidelines/#when-not-to-use-ai-tools","title":"When NOT to use AI tools","text":"<p>Avoid automatic editing when:</p> <ul> <li>adding new scientific explanations  </li> <li>modifying BRDF/topographic formulas  </li> <li>updating spectral response files  </li> <li>changing reflectance scaling logic  </li> </ul> <p>These require domain expertise.</p>"},{"location":"dev/codex-guidelines/#safe-tasks-for-ai-tools","title":"Safe tasks for AI tools","text":"<ul> <li>inserting text provided by the user  </li> <li>reorganizing text when explicitly instructed  </li> <li>generating placeholder scaffolding  </li> <li>performing mechanical refactors with tests  </li> </ul>"},{"location":"dev/codex-guidelines/#summary","title":"Summary","text":"<p>AI tools in this repository should behave like careful editors\u2014not authors. Always prioritize scientific correctness, reproducibility, and transparency.</p>"},{"location":"dev/codex-guidelines/#next-steps","title":"Next steps","text":"<ul> <li>Contributing &amp; development workflow </li> <li>Package architecture</li> </ul>"},{"location":"dev/contributing/","title":"Contributing &amp; Development Workflow","text":"<p>We welcome contributions from the community. This page outlines the workflow for proposing changes, running tests, and maintaining documentation quality.</p>"},{"location":"dev/contributing/#basic-principles","title":"Basic principles","text":"<ul> <li>Keep all processing steps transparent and auditable </li> <li>Do not introduce new scientific claims without citation or review  </li> <li>Maintain consistent naming and directory conventions  </li> <li>Ensure any new feature is accompanied by documentation updates  </li> </ul>"},{"location":"dev/contributing/#development-environment","title":"Development environment","text":"<p>Install the package in editable mode:</p> <p>```bash pip install -e \".[dev]\" Run tests: pytest Build documentation: mkdocs build Making code changes Open an issue describing the proposed change Create a feature branch Add or update unit tests Update relevant documentation pages Submit a pull request Modifying documentation Update Markdown files directly under docs/ Ensure navigation in mkdocs.yml remains valid Keep sections coherent and avoid duplication Run mkdocs serve locally to preview Adding datasets or SRFs Place files under cross_sensor_cal/data/ Update relevant stages to load the new assets Document usage in tutorials Coding standards Prefer pure functions and isolated dependencies Avoid large in-memory operations unless necessary Log processing decisions clearly Next steps Package architecture Codex edit guidelines</p>"},{"location":"dev/doc_drift/","title":"Doc drift","text":"<p>Maintain this page after any behavior change. Surface a short \"What changed?\" callout on Home if user-facing.</p>"},{"location":"dev/doc_drift/#documentation-drift-report","title":"Documentation Drift Report","text":""},{"location":"dev/doc_drift/#naming-rule","title":"Naming rule","text":"<ul> <li>Merged parquet naming in code: _merged_pixel_extraction.parquet"},{"location":"dev/doc_drift/#missing-items-in-docs","title":"Missing items in docs","text":"<ul> <li>Outputs: \u2014</li> <li>Sensors: micasense_to_match_oli_and_oli, micasense_to_match_oli_oli, micasense_to_match_tm_etm+</li> </ul>"},{"location":"dev/doc_drift/#stale-items-in-docs","title":"Stale items in docs","text":"<ul> <li>Outputs: _brdfandtopo_corrected_envi.parquet, _landsat_etm+_envi.img, _landsat_oli_envi.parquet, _landsat_tm_envi.hdr, _landsat_tm_envi.img, _micasense*_envi.img, _micasense_envi.hdr, _micasense_envi.img, _qa.json</li> <li>Sensors: MicaSense, MicaSense,, MicaSense,,, MicaSense,,,, MicaSense,,,,, MicaSense,,,,,, MicaSense,,,,,,, Micasense,, Micasense,,, Micasense,,,, Micasense,,,,, Micasense,,,,,, Micasense,,,,,,, micasense'], micasense'],, micasense'],,, micasense'],,,, micasense'],,,,, micasense'],,,,,, micasense*_envi.img<code>, micasense_envi.hdr, micasense_envi.hdr,, micasense_envi.hdr,,, micasense_envi.hdr,,,, micasense_envi.hdr,,,,, micasense_envi.hdr,,,,,, micasense_envi.img, micasense_envi.img,, micasense_envi.img,,, micasense_envi.img,,,, micasense_envi.img,,,,, micasense_envi.img,,,,,, micasense_envi.img/.hdr/.parquet, micasense_envi.img/.hdr/.parquet,, micasense_envi.img/.hdr/.parquet,,, micasense_envi.img/.hdr/.parquet,,,, micasense_envi.img/.hdr/.parquet,,,,, micasense_envi.img/.hdr/.parquet,,,,,, micasense_to_match_oli_and_oli,, micasense_to_match_oli_and_oli,,, micasense_to_match_oli_and_oli,,,, micasense_to_match_oli_and_oli,,,,, micasense_to_match_oli_and_oli,,,,,, micasense_to_match_oli_oli,, micasense_to_match_oli_oli,,, micasense_to_match_oli_oli,,,, micasense_to_match_oli_oli,,,,, micasense_to_match_oli_oli,,,,,, micasense_to_match_tm_etm+,,,,,, micasense</code>)</li> </ul>"},{"location":"dev/doc_drift/#entry-points-pyproject","title":"Entry points (pyproject)","text":"<p>{   \"cscal-download\": \"cross_sensor_cal.cli:download_main\",   \"cscal-pipeline\": \"cross_sensor_cal.cli.pipeline_cli:main\",   \"cscal-qa\": \"cross_sensor_cal.cli.qa_cli:main\",   \"cscal-recover-raw\": \"cross_sensor_cal.cli.recover_cli:main\",   \"cscal-qa-dashboard\": \"cross_sensor_cal.qa_dashboard:main\",   \"csc-merge-duckdb\": \"cross_sensor_cal.merge_duckdb:main\" }</p>"},{"location":"dev/doc_drift/#cli-flags-discovered","title":"CLI flags discovered","text":"<p>{   \"topo_and_brdf_correction.py\": [     \"--config_file\"   ],   \"merge_duckdb.py\": [     \"--corrected-glob\",     \"--data-root\",     \"--flightline-glob\",     \"--no-qa\",     \"--original-glob\",     \"--out-name\",     \"--resampled-glob\",     \"--write-feather\"   ],   \"qa_metrics.py\": [     \"--base-folder\",     \"--flight-stem\"   ],   \"standard_resample.py\": [     \"--hdr_path\",     \"--json_file\",     \"--output_path\",     \"--resampling_file_path\",     \"--sensor_type\"   ],   \"qa_dashboard.py\": [     \"--base-folder\",     \"--out-parquet\",     \"--out-png\"   ],   \"neon_to_envi.py\": [     \"--brightness-offset\"   ],   \"pipeline_cli.py\": [     \"--base-folder\",     \"--brightness-offset\",     \"--flight-lines\",     \"--max-workers\",     \"--product-code\",     \"--resample-method\",     \"--site-code\",     \"--year-month\"   ],   \"recover_cli.py\": [     \"--base-folder\",     \"--brightness-offset\"   ],   \"qa_cli.py\": [     \"--base-folder\",     \"--out-dir\",     \"--quick\",     \"--full\",     \"--save-json\",     \"--no-save-json\",     \"--n-sample\",     \"--rgb-bands\"   ],   \"pipeline.py\": [     \"--brightness-offset\",     \"--no-sync\",     \"--polygon_layer_path\",     \"--reflectance-offset\",     \"--remote-prefix\",     \"--resample-method\",     \"--verbose\"   ],   \"download.py\": [     \"--flight\",     \"--output\",     \"--product\",     \"--year-month\"   ] }</p>"},{"location":"dev/doc_drift/#stage-markers-found-in-logs","title":"Stage markers found in logs","text":"<ul> <li>[cscal-qa] \u2705 QA panels written to: {target.resolve()}</li> <li>\u23ed\ufe0f Parquet already present for %s -&gt; %s (skipping)</li> <li>\u26a0\ufe0f  QA panel generation failed for %s: %s</li> <li>\u26a0\ufe0f Cannot create Parquet work directory for %s: %s</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .hdr is missing or empty</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .img is missing or empty</li> <li>\u26a0\ufe0f Cannot locate work directory for Parquet export: %s</li> <li>\u26a0\ufe0f Failed Parquet export for %s: %s</li> <li>\u26a0\ufe0f Merge failed for {future_map[future]}: {exc}</li> <li>\u26a0\ufe0f QA panel after merge failed for {flightline_dir.name}: {e}</li> <li>\u2705 BRDF+topo correction already complete for %s -&gt; %s / %s (skipping)</li> <li>\u2705 BRDF+topo correction already complete for %s, skipping</li> <li>\u2705 BRDF+topo correction completed for %s -&gt; %s / %s</li> <li>\u2705 Download complete for %s \u2192 %s</li> <li>\u2705 ENVI export already complete for %s -&gt; %s / %s (skipping heavy export)</li> <li>\u2705 ENVI export completed for %s -&gt; %s / %s</li> <li>\u2705 Parquet stage complete for %s</li> <li>\u2705 Wrote Parquet for %s -&gt; %s</li> <li>\ud83c\udf10 Downloading %s (%s, %s) into %s ...</li> <li>\ud83c\udf89 Finished pipeline for %s</li> <li>\ud83c\udf89 Finished pipeline for %s (parallel worker join)</li> <li>\ud83c\udfaf Convolving corrected reflectance for %s</li> <li>\ud83d\udce6 ENVI export not found or invalid for %s, generating from %s</li> <li>\ud83d\udce6 Parquet export for %s ...</li> <li>\ud83d\udd0e ENVI export target for %s is %s / %s</li> <li>\ud83d\uddbc\ufe0f  Overwriting QA panel -&gt; %s</li> <li>\ud83d\uddbc\ufe0f  QA panel written \u2192 {prefix}_qa.png</li> <li>\ud83d\uddbc\ufe0f  Writing QA panel -&gt; %s</li> <li>\ud83d\uddbc\ufe0f  Wrote QA panel for %s -&gt; %s</li> </ul> <p>TODO: Review sensor labels in docs for formatting issues.</p>"},{"location":"dev/doc_drift_report/","title":"Documentation Drift Report","text":""},{"location":"dev/doc_drift_report/#naming-rule","title":"Naming rule","text":"<ul> <li>Merged parquet naming in code: _merged_pixel_extraction.parquet"},{"location":"dev/doc_drift_report/#missing-items-in-docs","title":"Missing items in docs","text":"<ul> <li>Outputs: \u2014</li> <li>Sensors: micasense_to_match_oli_and_oli, micasense_to_match_oli_oli, micasense_to_match_tm_etm_plus</li> </ul>"},{"location":"dev/doc_drift_report/#stale-items-in-docs","title":"Stale items in docs","text":"<ul> <li>Outputs: _landsat_convolved_envi.hdr, _landsat_convolved_envi.img, _landsat_etm+_envi.img, _landsat_oli_envi.parquet, _landsat_tm_envi.hdr, _landsat_tm_envi.img, _micasense*_envi.img, _micasense_envi.hdr, _micasense_envi.img</li> <li>Sensors: MicaSense, MicaSense), MicaSense,, MicaSense,,, MicaSense,,,, MicaSense,,,,, MicaSense,,,,,, MicaSense,,,,,,, MicaSense,,,,,,,, MicaSense-matched, Micasense,, Micasense,,, Micasense,,,, Micasense,,,,, Micasense,,,,,, Micasense,,,,,,, Micasense,,,,,,,, micasense'], micasense'],, micasense'],,, micasense'],,,, micasense'],,,,, micasense'],,,,,, micasense'],,,,,,, micasense_envi.img,, micasense_envi.img<code>,, micasense-to-landsat, micasense.json</code>, micasense_envi\",, micasense_envi.hdr, micasense_envi.hdr,, micasense_envi.hdr,,, micasense_envi.hdr,,,, micasense_envi.hdr,,,,, micasense_envi.hdr,,,,,, micasense_envi.hdr,,,,,,, micasense_envi.img, micasense_envi.img,, micasense_envi.img,,, micasense_envi.img,,,, micasense_envi.img,,,,, micasense_envi.img,,,,,, micasense_envi.img,,,,,,, micasense_envi.img/.hdr/.parquet, micasense_envi.img/.hdr/.parquet,, micasense_envi.img/.hdr/.parquet,,, micasense_envi.img/.hdr/.parquet,,,, micasense_envi.img/.hdr/.parquet,,,,, micasense_envi.img/.hdr/.parquet,,,,,, micasense_envi.img/.hdr/.parquet,,,,,,, micasense_input, micasense_landsat_harmonized.parquet\"), micasense_to_landsat.csv, micasense_to_match_oli_and_oli,, micasense_to_match_oli_and_oli,,, micasense_to_match_oli_and_oli,,,, micasense_to_match_oli_and_oli,,,,, micasense_to_match_oli_and_oli,,,,,, micasense_to_match_oli_and_oli,,,,,,, micasense_to_match_oli_oli,, micasense_to_match_oli_oli,,, micasense_to_match_oli_oli,,,, micasense_to_match_oli_oli,,,,, micasense_to_match_oli_oli,,,,,, micasense_to_match_oli_oli,,,,,,, micasense_to_match_tm_etm+,,,,,,, micasense<code>, micasense</code>)</li> </ul>"},{"location":"dev/doc_drift_report/#entry-points-pyproject","title":"Entry points (pyproject)","text":"<p>{   \"cscal-download\": \"cross_sensor_cal.cli:download_main\",   \"cscal-pipeline\": \"cross_sensor_cal.cli.pipeline_cli:main\",   \"cscal-qa\": \"cross_sensor_cal.cli.qa_cli:main\",   \"cscal-recover-raw\": \"cross_sensor_cal.cli.recover_cli:main\",   \"cscal-qa-dashboard\": \"cross_sensor_cal.qa_dashboard:main\",   \"csc-merge-duckdb\": \"cross_sensor_cal.merge_duckdb:main\" }</p>"},{"location":"dev/doc_drift_report/#cli-flags-discovered","title":"CLI flags discovered","text":"<p>{   \"topo_and_brdf_correction.py\": [     \"--config_file\"   ],   \"neon_to_envi.py\": [     \"--brightness-offset\"   ],   \"qa_dashboard.py\": [     \"--base-folder\",     \"--out-parquet\",     \"--out-png\"   ],   \"merge_duckdb.py\": [     \"--corrected-glob\",     \"--data-root\",     \"--flightline-glob\",     \"--merge-memory-limit\",     \"--merge-row-group-size\",     \"--merge-temp-directory\",     \"--merge-threads\",     \"--no-qa\",     \"--original-glob\",     \"--out-name\",     \"--resampled-glob\",     \"--write-feather\"   ],   \"standard_resample.py\": [     \"--hdr_path\",     \"--json_file\",     \"--output_path\",     \"--resampling_file_path\",     \"--sensor_type\"   ],   \"qa_cli.py\": [     \"--base-folder\",     \"--full\",     \"--n-sample\",     \"--out-dir\",     \"--quick\",     \"--rgb-bands\",     \"--save-json\"   ],   \"recover_cli.py\": [     \"--base-folder\",     \"--brightness-offset\"   ],   \"pipeline_cli.py\": [     \"--base-folder\",     \"--brightness-offset\",     \"--engine\",     \"--flight-lines\",     \"--max-workers\",     \"--merge-memory-limit\",     \"--merge-row-group-size\",     \"--merge-temp-directory\",     \"--merge-threads\",     \"--parquet-chunk-size\",     \"--product-code\",     \"--resample-method\",     \"--site-code\",     \"--year-month\"   ],   \"pipeline.py\": [     \"--brightness-offset\",     \"--no-sync\",     \"--polygon_layer_path\",     \"--reflectance-offset\",     \"--remote-prefix\",     \"--resample-method\",     \"--verbose\"   ],   \"download.py\": [     \"--flight\",     \"--output\",     \"--product\",     \"--year-month\"   ] }</p>"},{"location":"dev/doc_drift_report/#stage-markers-found-in-logs","title":"Stage markers found in logs","text":"<ul> <li>Writing Parquet with row_group_size=%s</li> <li>[merge] \u26a0\ufe0f QA panel after merge failed for {prefix}: {e}</li> <li>[merge] \ud83d\uddbc\ufe0f  QA panel \u2192 {out_png_path} </li> <li>[{flight_stem}] \u2b07\ufe0f Download already complete \u2014 reusing existing files ({h5_path.name})</li> <li>[{flight_stem}] \ud83c\udf10 Downloading {flight_stem} ({site_code}, {year_month}) into {h5_path} ...</li> <li>\u23ed\ufe0f Parquet already present for %s -&gt; %s (skipping)</li> <li>\u26a0\ufe0f Cannot create Parquet work directory for %s: %s</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .hdr is missing or empty</li> <li>\u26a0\ufe0f Cannot export Parquet for %s because .img is missing or empty</li> <li>\u26a0\ufe0f Cannot locate work directory for Parquet export: %s</li> <li>\u26a0\ufe0f Cannot remove invalid Parquet for %s (%s)</li> <li>\u26a0\ufe0f Existing Parquet for %s is invalid (%s); regenerating</li> <li>\u26a0\ufe0f Failed Parquet export for %s: %s</li> <li>\u26a0\ufe0f Merge failed for {future_map[future]}: {exc}</li> <li>\u26a0\ufe0f Parquet validation reported an issue for %s: %s</li> <li>\u26a0\ufe0f Skipping DuckDB merge for %s because no Parquet outputs were produced</li> <li>\u2705 BRDF+topo correction already complete for %s -&gt; %s / %s (skipping)</li> <li>\u2705 BRDF+topo correction already complete for %s, skipping</li> <li>\u2705 BRDF+topo correction completed for %s -&gt; %s / %s</li> <li>\u2705 Discovered existing ENVI export \u2192 {discovered_img.name} / {discovered_hdr.name}</li> <li>\u2705 Download complete for %s \u2192 %s</li> <li>\u2705 ENVI export already complete for %s -&gt; %s / %s (skipping heavy export)</li> <li>\u2705 ENVI export created successfully \u2192 {raw_img_path.name} / {raw_hdr_path.name}</li> <li>\u2705 ENVI export found via discovery \u2192 {discovered_img.name} / {discovered_hdr.name}</li> <li>\u2705 Existing ENVI export found \u2014 skipping heavy export ({raw_img_path.name} / {raw_hdr_path.name})</li> <li>\u2705 Parquet stage complete for %s</li> <li>\u2705 Wrote Parquet for %s -&gt; %s</li> <li>\u2728 Existing ENVI exports detected \u2014 pipeline will reuse validated files automatically.</li> <li>\ud83c\udf89 Finished pipeline for %s</li> <li>\ud83c\udf89 Finished pipeline for %s (parallel worker join)</li> <li>\ud83c\udfaf Convolving corrected reflectance for %s</li> <li>\ud83d\udccd Preparing ENVI export at: {raw_img_path.name} / {raw_hdr_path.name}</li> <li>\ud83d\udce6 No existing ENVI export detected \u2014 creating a new one from source data {h5_path.name}</li> <li>\ud83d\udce6 Parquet export for %s ...</li> <li>\ud83e\uddf1 Parquet row group size = %s (legacy=%s)</li> <li>\ud83e\uddf1 Using Parquet row group size of %s rows</li> </ul> <p>TODO: Review sensor labels in docs for formatting issues.</p>"},{"location":"pipeline/outputs/","title":"Outputs &amp; File Structure","text":"<p>The cross-sensor-cal pipeline produces a consistent directory structure for each flight line. This page describes every artifact, what it contains, and how to use it.</p>"},{"location":"pipeline/outputs/#flight-line-directory-layout","title":"Flight line directory layout","text":"<p>A typical directory looks like:</p> <p>NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance/ directional/ topo/ brdf/ convolved/ parquet/ qa/</p> <p>Within each folder, ENVI images, Parquet tables, and sidecar JSON files share common prefixes.</p>"},{"location":"pipeline/outputs/#key-outputs","title":"Key outputs","text":""},{"location":"pipeline/outputs/#1-corrected-envi-products","title":"1. Corrected ENVI products","text":"<p>Files:</p> <p>_directional_reflectance_envi.img _topocorrected_envi.img *_brdfandtopo_corrected_envi.img</p> <p>Contents:</p> <ul> <li>reflectance bands  </li> <li>wavelength metadata  </li> <li>masks (cloud, shadow, water, snow, invalid)  </li> <li>CRS and pixel geometry  </li> </ul>"},{"location":"pipeline/outputs/#2-sensor-harmonized-envi-cubes","title":"2. Sensor-harmonized ENVI cubes","text":"<p>Files:</p> <p>*_landsat_convolved_envi.img</p> <p>One file is produced for each requested sensor type.</p> <p>Contents:</p> <ul> <li>band-averaged reflectance values for the sensor  </li> <li>metadata documenting SRFs used  </li> <li>brightness adjustment coefficients (if applicable)  </li> </ul>"},{"location":"pipeline/outputs/#3-per-product-parquet-tables","title":"3. Per-product Parquet tables","text":"<p>Every ENVI file has a corresponding Parquet table:</p> <p>_brdfandtopo_corrected.parquet _landsat_convolved.parquet</p> <p>Each row = one pixel. Columns include:</p> <ul> <li>reflectance values  </li> <li>masks  </li> <li>wavelengths  </li> <li>pixel coordinates  </li> </ul> <p>These tables are ideal for large-scale analysis using DuckDB, pandas, or xarray.</p>"},{"location":"pipeline/outputs/#4-merged-pixel-extraction-table","title":"4. Merged pixel extraction table","text":"<p>*_merged_pixel_extraction.parquet</p> <p>This contains all extracted pixel-level data for the flight line, merged across products.</p>"},{"location":"pipeline/outputs/#5-qa-artifacts","title":"5. QA artifacts","text":"<p>_qa.png _qa.pdf *_qa.json</p> <p>See the QA page for details.</p>"},{"location":"pipeline/outputs/#naming-conventions","title":"Naming conventions","text":"<p>Files follow a consistent pattern:</p> <p>.{img|hdr|parquet} <p>Examples:</p> <ul> <li><code>NEON_D13_NIWO_DP1_L020-1_20230815_brdfandtopo_corrected_envi.img</code> </li> <li><code>NEON_D13_NIWO_DP1_L020-1_20230815_landsat_convolved_parquet</code> </li> </ul> <p>The prefixes and suffixes are designed for predictable sorting and automation.</p>"},{"location":"pipeline/outputs/#using-outputs-in-analysis","title":"Using outputs in analysis","text":"<p>Example with DuckDB:</p> <p>```python import duckdb duckdb.query(\"SELECT NIR, Red FROM '..._landsat_convolved.parquet' LIMIT 10\").df() Example with rioxarray: import rioxarray as rxr cube = rxr.open_rasterio(\"..._brdfandtopo_corrected_envi.img\") Next steps Pipeline stages QA panels &amp; metrics</p>"},{"location":"pipeline/polygons/","title":"Polygon spectral library pipeline","text":"<p>The polygon workflow augments the standard flightline processing by producing per-polygon spectral libraries from the existing pixel-level Parquet products. The pipeline is optional today and can be invoked programmatically without changing the default <code>cscal-pipeline</code> behaviour.</p>"},{"location":"pipeline/polygons/#overview","title":"Overview","text":"<p>The polygon pipeline performs three steps for each flightline:</p> <ol> <li>Polygon\u2013pixel index \u2013 rasterises a polygon layer against a chosen NEON    product (BRDF/topography corrected ENVI by default) and records the pixels    that intersect each polygon, including spatial metadata and polygon    attributes, in <code>*_polygon_pixel_index.parquet</code>.</li> <li>Polygon-only Parquets \u2013 filters the existing per-product Parquet tables    so that only pixels found in the index are retained.  Outputs follow the    pattern <code>*_envi_polygons.parquet</code>,    <code>*_brdfandtopo_corrected_envi_polygons.parquet</code> and    <code>*_landsat_tm_envi_polygons.parquet</code> (etc.).</li> <li>Merged polygon spectral library \u2013 joins the polygon-only Parquets with    the polygon index to produce a compact spectral library for all polygons in a    flightline: <code>*_polygons_merged_pixel_extraction.parquet</code>.</li> </ol> <p>The helper functions live in :mod:<code>cross_sensor_cal.polygons</code> and are available for bespoke workflows while we evaluate the approach.</p>"},{"location":"pipeline/polygons/#data-requirements","title":"Data requirements","text":"<ul> <li>A processed flightline directory with the usual per-product Parquet tables.</li> <li>A polygon vector file readable by GeoPandas (e.g. GeoPackage, Shapefile,   GeoJSON).  The repository ships with a sample data set at   <code>Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg</code>.</li> </ul> <p>Polygons are reprojected automatically to match the reference raster used for the index.  A <code>polygon_id</code> column is honoured if present; otherwise a unique identifier is generated.</p>"},{"location":"pipeline/polygons/#example-usage","title":"Example usage","text":"<pre><code>from cross_sensor_cal.paths import FlightlinePaths\nfrom cross_sensor_cal.polygons import run_polygon_pipeline_for_flightline\n\nflight_paths = FlightlinePaths(\"/data/flightlines\", \"NEON_D12_NIWO_2021\")\npolygons_path = \"Datasets/niwot_aop_polygons_2023_12_8_23_analysis_ready_half_diam.gpkg\"\n\nresult = run_polygon_pipeline_for_flightline(\n    flight_paths,\n    polygons_path,\n    products=[\n        \"envi\",\n        \"brdfandtopo_corrected_envi\",\n        \"landsat_tm_envi\",\n        \"landsat_oli_envi\",\n        \"micasense_envi\",\n    ],\n)\n\nprint(result[\"polygon_index_path\"])\nprint(result[\"polygon_merged_parquet\"])\n</code></pre> <p>Each helper is also available individually for advanced scenarios:</p> <ul> <li>:func:<code>build_polygon_pixel_index</code> \u2013 create the polygon pixel lookup table.</li> <li>:func:<code>extract_polygon_parquets_for_flightline</code> \u2013 generate polygon-only   Parquet tables for a subset of products.</li> <li>:func:<code>merge_polygon_parquets_for_flightline</code> \u2013 merge polygon Parquets into a   single spectral library.</li> </ul>"},{"location":"pipeline/polygons/#status-and-roadmap","title":"Status and roadmap","text":"<p>The polygon pipeline is opt-in today and does not run as part of the standard flightline orchestration.  The utilities introduced here allow data teams to experiment with polygon spectral libraries ahead of tighter integration in a future release.</p>"},{"location":"pipeline/qa/","title":"QA Panels &amp; Metrics","text":"<p>Each processed NEON flight line generates a set of quality assurance (QA) artifacts designed to summarize reflectance distributions, masks, geometry, and harmonization decisions.</p> <p>This page describes the QA PNG, QA PDF, and QA JSON outputs.</p>"},{"location":"pipeline/qa/#qa-png","title":"QA PNG","text":"<p>The QA PNG provides a quick visual summary of:</p> <ul> <li>reflectance distributions for key wavelengths  </li> <li>spatial distribution of masks  </li> <li>before/after brightness comparison  </li> <li>BRDF correction diagnostics  </li> <li>wavelength alignment checks  </li> </ul> <p>A typical PNG includes panels for:</p> <ol> <li>reflectance histograms  </li> <li>strip-based brightness summaries  </li> <li>cloud/water/snow/invalid pixel frequencies  </li> <li>wavelength metadata  </li> <li>sensor-specific diagnostics  </li> </ol> <p>Use this for fast visual inspection.</p>"},{"location":"pipeline/qa/#qa-pdf","title":"QA PDF","text":"<p>The PDF includes:</p> <ul> <li>all PNG content  </li> <li>multi-page extended diagnostics  </li> <li>tabulated statistics  </li> <li>optional per-band summaries  </li> <li>BRDF coefficient tables  </li> <li>brightness adjustment coefficients  </li> </ul> <p>This document serves as a flight-line-level audit record.</p>"},{"location":"pipeline/qa/#qa-json","title":"QA JSON","text":"<p>The JSON file contains machine-readable metrics including:</p>"},{"location":"pipeline/qa/#reflectance-metrics","title":"Reflectance metrics","text":"<ul> <li>min/max/median for each band  </li> <li>proportion of saturated or masked pixels  </li> <li>brightness differences across correction stages  </li> </ul>"},{"location":"pipeline/qa/#mask-summary-metrics","title":"Mask summary metrics","text":"<ul> <li>percent cloud  </li> <li>percent cloud shadow  </li> <li>percent snow  </li> <li>percent water  </li> <li>percent invalid  </li> </ul>"},{"location":"pipeline/qa/#brdf-metrics","title":"BRDF metrics","text":"<ul> <li>per-band BRDF coefficients  </li> <li>reconstruction error statistics  </li> </ul>"},{"location":"pipeline/qa/#sensor-harmonization-metrics","title":"Sensor harmonization metrics","text":"<ul> <li>brightness coefficients (per band)  </li> <li>spectral alignment checks  </li> <li>per-band RMSE for regression-based harmonization (if applicable)  </li> </ul>"},{"location":"pipeline/qa/#geometry-metadata","title":"Geometry metadata","text":"<ul> <li>solar zenith/azimuth  </li> <li>view zenith/azimuth  </li> <li>DEM statistics  </li> </ul>"},{"location":"pipeline/qa/#interpreting-qa-results","title":"Interpreting QA results","text":"<p>Flags to watch for:</p> <ul> <li>Reflectance values &gt; 1.5 \u2192 possible BRDF or DEM issues  </li> <li>Large brightness shifts \u2192 check convolution stage  </li> <li>High invalid or shadow fraction \u2192 consider masking strategy  </li> <li>Noisy BRDF coefficients \u2192 unstable correction in low-SNR regions  </li> </ul>"},{"location":"pipeline/qa/#next-steps","title":"Next steps","text":"<ul> <li>Pipeline stages </li> <li>Outputs &amp; file structure </li> <li>Troubleshooting</li> </ul>"},{"location":"pipeline/qa_panel/","title":"QA panel","text":"<p>The QA panel couples a metrics JSON file with an annotated PNG so that engineering and science teams can track both spectral statistics and the visual context for every flightline. See How to Interpret the Panel and the validation reference for deeper guidance on each metric.</p> <p>Additional QA products created from the merged Parquet:</p> <ul> <li> <p><code>&lt;prefix&gt;_merged__BY_SENSOR_vs_NEON_directional_BRDFTopo.png</code>   Sensor-by-sensor scatter panels versus NEON directional BRDFTopo.</p> </li> <li> <p><code>&lt;prefix&gt;_merged__MS_vs_Landsat_FIXED.png</code>   MicaSense-matched (X) versus Landsat (Y) scatter panels by band.</p> </li> </ul>"},{"location":"pipeline/qa_panel/#multi-page-qa-report","title":"Multi-page QA report","text":"<p>In addition to the single PNG QA panel (<code>&lt;prefix&gt;_qa.png</code>), the pipeline now writes a multi-page PDF report (<code>&lt;prefix&gt;_qa.pdf</code>) with three pages:</p> <ol> <li> <p>Page 1 \u2013 ENVI overview    One row with one panel per ENVI product. This is a quick visual check that    all ENVI files exist and render correctly.</p> </li> <li> <p>Page 2 \u2013 Topographic &amp; BRDF diagnostics </p> </li> <li>Row 1: pre vs post histograms and \u0394 median vs wavelength for the combined      topographic + BRDF correction stage.  </li> <li> <p>Row 2: summaries of topographic (slope/aspect) and BRDF geometry      (solar/sensor angles) derived from the correction JSON.</p> </li> <li> <p>Page 3 \u2013 Remaining QA diagnostics </p> </li> <li>Convolution scatter plots (expected vs computed bands).  </li> <li>Header and wavelength integrity summary (flags when sensor defaults are used).</li> <li>Mask coverage, negatives %, and &gt;1.2 reflectance % summary.</li> <li>Issues/warnings, including brightness coefficients that were applied.</li> </ol>"},{"location":"pipeline/qa_panel/#qa-panel-and-validation-tests","title":"QA Panel and Validation Tests","text":"<p>The QA panel is the final diagnostic step of the Cross-Sensor Calibration pipeline. It provides both a visual and quantitative summary of how well each product behaved through all correction stages (topographic, BRDF, brightness, convolution).  </p>"},{"location":"pipeline/qa_panel/#what-the-qa-tests-measure","title":"What the QA Tests Measure","text":"Test What It Checks Why It Matters Reflectance Range (negatives &amp; &gt;1.2 %) Fraction of pixels below 0 or above 1.2 Reflectance should remain physically bounded. Large negative or &gt;1.2 values indicate poor radiometric scaling or unmasked clouds/shadows. Header &amp; Wavelength Integrity Presence, count, monotonicity, and provenance of <code>wavelength</code> values in ENVI headers Ensures each band is correctly aligned; missing, non-monotonic, or defaulted wavelengths break convolution and spectral analyses. \u0394Reflectance (Pre\u2192Post Correction) Median and IQR difference in reflectance before and after BRDF/topo correction Quantifies how much the correction changed the data. Large deltas in flat terrain suggest over-correction; near-zero deltas in complex terrain may suggest under-correction. Brightness Normalization (if applied) Per-band gain and offset used in brightness correction Tracks whether correction parameters remain within expected limits (e.g., gain \u2208 [0.9, 1.1]). Large deviations imply inconsistent illumination normalization. Convolution Accuracy (per target sensor) RMSE and Spectral Angle Mapper (SAM) between expected vs computed bands Confirms spectral resampling is physically consistent. High RMSE or large SAM (&gt;0.05 radians) indicates wavelength misalignment or incorrect response functions. Mask Coverage % of valid pixels used for metrics Low valid coverage (&lt;60%) signals missing masks or unfiltered NaNs. Histogram Shape Consistency Visual histogram overlay of pre/post corrections Skewed or bimodal shapes suggest scene heterogeneity or masking issues."},{"location":"pipeline/qa_panel/#brightness-coefficients","title":"Brightness coefficients","text":"<p>When NEON data are convolved to Landsat bands, we optionally apply small per-band brightness adjustments so that Landsat-like products match a MicaSense reference.</p> <ul> <li>Coefficients are stored in <code>landsat_to_micasense.json</code> (units: percent).</li> <li>The adjustment is multiplicative:</li> </ul> <p><code>L_adj = L_raw * (1 + coeff / 100)</code>, where negative coefficients darken   Landsat bands slightly.</p> <ul> <li>Applied coefficients are recorded in the QA JSON under   <code>brightness_coefficients.landsat_to_micasense</code> and displayed on Page 3 of   the QA PDF.</li> </ul> <p>This makes it easy to verify when a brightness adjustment was applied and to audit the exact per-band values.</p>"},{"location":"pipeline/qa_panel/#why-these-tests-are-appropriate","title":"Why These Tests Are Appropriate","text":"<p>These diagnostics are physically interpretable and sensor-agnostic:</p> <ul> <li>Radiometric realism: Reflectance outside [0,1.2] is physically implausible and signals calibration drift or shadow contamination.  </li> <li>Spectral continuity: Monotonic wavelengths ensure that per-band corrections and convolutions follow real sensor band order.  </li> <li>Conservation principle: \u0394Reflectance checks whether corrections preserve brightness globally (no systematic over-darkening).  </li> <li>Geometric realism: Mask coverage and illumination correlation prevent interpreting shadowed or topographically inverted pixels as valid reflectance.  </li> <li>Spectral fidelity: RMSE and SAM compare corrected spectra to expected bandpasses, verifying physical sensor compatibility.</li> </ul>"},{"location":"pipeline/qa_panel/#how-to-interpret-the-panel","title":"How to Interpret the Panel","text":"<p>Each panel includes:</p> <ol> <li>Left: RGB quicklook using auto-selected bands (660, 560, 490 nm).  </li> <li>Uniform color tone: good illumination normalization.  </li> <li>Patchy shadows or gradients: check DTM alignment or BRDF model.</li> <li>Top-right: Pre (gray) vs Post (green) histograms.  </li> <li>Slight narrowing: normal (flattening illumination gradients).  </li> <li>Severe shift left/right: over- or under-correction.</li> <li>Middle-right: \u0394Reflectance vs Wavelength curve (median \u00b1 IQR).  </li> <li>Smooth near-zero line: ideal.  </li> <li>Large band-specific spikes: band-specific sensor noise or cloud edges.</li> <li>Bottom-right: Convolution scatter (expected vs computed).  </li> <li>Points close to 1:1 line: good.  </li> <li>Systematic bias or slope \u2260 1: check wavelength alignment or FWHM mismatch.</li> <li>Footer: Metadata (flightline ID, date, package version, git SHA).</li> </ol>"},{"location":"pipeline/qa_panel/#quantitative-thresholds-for-not-good","title":"Quantitative Thresholds for \u201cNot Good\u201d","text":"Metric Acceptable Range \u201cNeeds Review\u201d \u201cProblematic\u201d Negatives % &lt; 0.5 % 0.5\u20132 % &gt; 2 % &gt;1.2 reflectance % &lt; 0.5 % 0.5\u20132 % &gt; 2 % \u0394Reflectance median &lt; 0.02 (normally good) &gt; 0.05 (over/under-correction) Brightness gain 0.9\u20131.1 0.85\u20130.9 / 1.1\u20131.15 &lt; 0.85 or &gt; 1.15 Convolution RMSE &lt; 0.02 0.02\u20130.05 &gt; 0.05 SAM (radians) &lt; 0.03 0.03\u20130.05 &gt; 0.05 Mask coverage &gt; 80 % 60\u201380 % &lt; 60 %"},{"location":"pipeline/qa_panel/#deciding-when-a-product-fails-qa","title":"Deciding When a Product Fails QA","text":"<p>Mark a product as Needs Review when: - \u2265 2 metrics fall in the \u201cNeeds Review\u201d column, or - Any single metric hits the \u201cProblematic\u201d range.</p> <p>Mark a product as Fail when: - &gt; 10 % of bands exceed thresholds (e.g., \u0394Reflectance &gt; 0.05), - Wavelengths are missing or non-monotonic, - Convolution RMSE &gt; 0.05 and SAM &gt; 0.05, - Mask coverage &lt; 60 %.</p> <p>All QA results are summarized in the sidecar JSON (<code>*_qa.json</code>), enabling programmatic filtering.</p>"},{"location":"pipeline/qa_panel/#next-steps-after-qa-flags","title":"Next Steps After QA Flags","text":"Issue Likely Cause Recommended Fix Many negatives or high reflectance Mis-scaled input, wrong gain offset Re-run brightness correction or check calibration constants. Non-monotonic wavelengths Corrupted or edited header Re-export ENVI or fix <code>wavelength</code> list manually. Large \u0394Reflectance Over-aggressive BRDF correction Adjust BRDF parameters or review illumination mask. High RMSE/SAM Wrong sensor response curves Verify target sensor config file. Low mask coverage Cloud or DTM mask mismatch Improve masking or fill small gaps before QA."},{"location":"pipeline/qa_panel/#automating-qa-review","title":"Automating QA Review","text":"<p>Each QA JSON includes numeric thresholds. You can quickly summarize or flag tiles programmatically:</p> <pre><code>import json, glob\nbad = []\nfor f in glob.glob(\"*/**/*_qa.json\", recursive=True):\n    q = json.load(open(f))\n    if (\n        q[\"negatives_pct\"] &gt; 2.0\n        or q.get(\"overbright_pct\", 0) &gt; 2.0\n        or q[\"mask\"][\"valid_pct\"] &lt; 60\n    ):\n        bad.append(f)\nprint(\"Tiles needing review:\", bad)\n</code></pre>"},{"location":"pipeline/stages/","title":"Pipeline Overview &amp; Stages","text":"<p>The cross-sensor-cal pipeline transforms NEON HDF5 directional reflectance into physically corrected and sensor-harmonized reflectance products. Each stage is restart-safe and produces structured, auditable outputs.</p> <p>This page describes every stage of the pipeline, what it consumes, what it produces, and what can go wrong.</p>"},{"location":"pipeline/stages/#pipeline-summary","title":"Pipeline summary","text":"<ol> <li>Data acquisition (download NEON HDF5 tiles)  </li> <li>HDF5 \u2192 ENVI export </li> <li>Topographic correction </li> <li>BRDF correction </li> <li>Sensor harmonization (convolution) </li> <li>Parquet extraction + merging </li> <li>Quality assurance (QA PNG, PDF, JSON) </li> </ol> <p>Each stage can be run independently using the <code>--start-at</code> and <code>--end-at</code> flags.</p>"},{"location":"pipeline/stages/#1-data-acquisition","title":"1. Data acquisition","text":"<p>Inputs: - NEON API paths or local HDF5 files  </p> <p>Outputs: - cached HDF5 tiles stored under the selected <code>--base-folder</code></p> <p>The pipeline fetches only the tiles required for the selected flight line.</p> <p>Common issues: - missing HDF5 files in NEON storage - interrupted downloads in cloud environments - insufficient space in temporary directories  </p>"},{"location":"pipeline/stages/#2-hdf5-envi-export","title":"2. HDF5 \u2192 ENVI export","text":"<p>Inputs: - <code>*_directional_reflectance.h5</code> - per-pixel geometry and metadata  </p> <p>Outputs: - <code>*_directional_reflectance_envi.img/.hdr</code> - sidecar JSON documenting extracted wavelengths, masks, and scaling  </p> <p>This stage produces an ENVI image that mirrors the HDF5 directional reflectance dataset.</p> <p>What the ENVI file contains: - reflectance (scaled NEON values) - wavelength metadata - per-pixel masks (cloud, cloud shadow, water, snow, invalid)  </p> <p>Common issues: - mismatch between HDF5 metadata and ENVI header - extremely large tile sizes causing I/O delays - NaN bands due to malformed HDF5 datasets  </p>"},{"location":"pipeline/stages/#3-topographic-correction","title":"3. Topographic correction","text":"<p>Inputs: - directional reflectance ENVI - DEM-derived slope and aspect - solar geometry  </p> <p>Outputs: - <code>*_topocorrected_envi.img</code> </p> <p>Topographic correction reduces slope- and aspect-driven variation in illumination.  </p> <p>The method assumes surface reflectance behaves consistently with simple terrain-adjustment models.</p> <p>Common issues: - DEM resolution mismatch - strong terrain shadows that remain after correction - negative reflectance in deep shadows (masked)  </p>"},{"location":"pipeline/stages/#4-brdf-correction","title":"4. BRDF correction","text":"<p>Inputs: - topographically corrected ENVI reflectance - view geometry (sensor zenith / azimuth) - solar geometry  </p> <p>Outputs: - <code>*_brdfandtopo_corrected_envi.img</code> - BRDF coefficient tables in the QA JSON  </p> <p>BRDF correction adjusts reflectance to a consistent view/illumination angle, making spectra across the flight line more comparable.</p> <p>Common issues: - instabilities in BRDF coefficient fitting - extreme reflectance values that must be masked - spatial artifacts in low-SNR bands  </p>"},{"location":"pipeline/stages/#5-sensor-harmonization-spectral-convolution","title":"5. Sensor harmonization (spectral convolution)","text":"<p>Inputs: - BRDF+topo corrected ENVI - sensor spectral response functions (SRFs)  </p> <p>Outputs: - <code>*_landsat_convolved_envi.img</code> or other sensor-equivalent ENVI products - bandpass-harmonized Parquet files  </p> <p>This stage integrates the corrected spectrum against the target sensor's SRFs. Supported sensors include Landsat OLI/OLI-2; others can be added.</p> <p>Common issues: - wavelength misalignment - missing SRF tables - sensor bands with near-zero response across NEON wavelengths  </p>"},{"location":"pipeline/stages/#6-parquet-extraction-merging","title":"6. Parquet extraction &amp; merging","text":"<p>Inputs: - any ENVI cube produced by earlier stages  </p> <p>Outputs: - a Parquet file per cube (one row per pixel) - a merged pixel extraction table for the whole flight line  </p> <p>This step makes downstream analysis easy in Python, R, or DuckDB.</p> <p>Common issues: - extremely large tables (billions of rows) - insufficient memory for merges - incorrect CRS metadata in ENVI headers  </p>"},{"location":"pipeline/stages/#7-quality-assurance-qa","title":"7. Quality assurance (QA)","text":"<p>Inputs: - all previous outputs  </p> <p>Outputs: - <code>*_qa.png</code> - <code>*_qa.pdf</code> - <code>*_qa.json</code> </p> <p>QA artifacts summarize reflectance distributions, masks, wavelength metadata, and BRDF/brightness statistics. See the QA page for details.</p>"},{"location":"pipeline/stages/#running-a-partial-pipeline","title":"Running a partial pipeline","text":"<p>You can run only part of the pipeline:</p> <p>```bash cscal-pipeline \\   --start-at brdf \\   --end-at convolution Or run a single stage manually if needed. Next steps Outputs &amp; file structure QA panels &amp; metrics</p>"},{"location":"reference/configuration/","title":"Configuration","text":"<p>Most users can rely on default configuration, but the pipeline allows fine-grained control over processing stages, performance, and file handling.</p> <p>This page documents the available configuration parameters and how they affect pipeline behavior.</p>"},{"location":"reference/configuration/#configuration-sources","title":"Configuration sources","text":"<ol> <li>Command-line options (highest priority)  </li> <li>Environment variables  </li> <li>Default internal settings  </li> </ol>"},{"location":"reference/configuration/#common-settings","title":"Common settings","text":""},{"location":"reference/configuration/#base-folder","title":"<code>base-folder</code>","text":"<p>Directory where:</p> <ul> <li>downloaded HDF5 tiles  </li> <li>ENVI exports  </li> <li>Parquet tables  </li> <li>QA artifacts  </li> </ul> <p>are written.</p>"},{"location":"reference/configuration/#engine","title":"<code>engine</code>","text":"<p>Execution backend:</p> <ul> <li><code>thread</code> (default)  </li> <li><code>ray</code> (distributed/hyperparallel workflows)</li> </ul>"},{"location":"reference/configuration/#max-workers","title":"<code>max-workers</code>","text":"<p>Controls concurrency in:</p> <ul> <li>ENVI export  </li> <li>BRDF and topo correction  </li> <li>Parquet extraction  </li> </ul> <p>Use cautiously when memory is limited.</p>"},{"location":"reference/configuration/#start-at-and-end-at","title":"<code>start-at</code> and <code>end-at</code>","text":"<p>Define subsets of the pipeline to run.</p> <p>Example:</p> <p>```bash --start-at brdf --end-at convolution Environment variables VariableMeaning CSCAL_TMPDIROverride temporary directory CSCAL_LOGLEVELSet logging verbosity CSCAL_RAY_ADDRESSUse an existing Ray cluster Advanced configuration These settings primarily matter for large-scale workflows: chunk sizes for Parquet extraction memory thresholds for Ray worker processes default CRS assignments sensor SRF paths Details of internal architecture appear in the Developer section. Next steps JSON schemas Validation metrics Pipeline stages</p>"},{"location":"reference/extending/","title":"Extending","text":"<p>When do I need this? When adding a new target sensor or swapping readers/writers; follow the extension points listed here.</p>"},{"location":"reference/extending/#purpose","title":"Purpose","text":"<p>Guide contributions that add sensors to Stage 4 or new exporters feeding Outputs.</p>"},{"location":"reference/extending/#inputs","title":"Inputs","text":"<ul> <li>Bandpass definitions (CSV/JSON) for the new sensor</li> <li>Implementation classes under <code>cross_sensor_cal</code> to register</li> <li>Tests covering the new workflow</li> </ul>"},{"location":"reference/extending/#outputs","title":"Outputs","text":"<p>Updated convolution products and schemas consumed by Parquet export and Merge.</p>"},{"location":"reference/extending/#run-it","title":"Run it","text":"<pre><code>pytest tests/convolution/test_new_sensor.py\n</code></pre> <pre><code>from cross_sensor_cal.convolution import registry\n\nprint(registry.available_sensors())\n</code></pre>"},{"location":"reference/extending/#pitfalls","title":"Pitfalls","text":"<ul> <li>Forgetting to update schemas will break Stage 6 merges.</li> <li>Ship lightbox-friendly QA thumbnails when adding new visualization layers.</li> <li>Document new sensors in Pipeline Stages and Troubleshooting.</li> </ul>"},{"location":"reference/schemas/","title":"JSON Schemas","text":"<p>The pipeline emits several JSON files containing structured metadata and QA metrics. These files allow downstream tools to audit processing decisions and validate outputs.</p> <p>This page summarizes the purpose and structure of each schema.</p>"},{"location":"reference/schemas/#1-envi-export-metadata","title":"1. ENVI export metadata","text":"<p>Recorded in:</p> <p>*_export_metadata.json</p> <p>Contains:</p> <ul> <li>wavelength array  </li> <li>band names  </li> <li>reflectance scaling (NEON conventions)  </li> <li>mask types and bit fields  </li> <li>CRS and affine transform  </li> </ul>"},{"location":"reference/schemas/#2-topographic-correction-metadata","title":"2. Topographic correction metadata","text":"<p>*_topo_metadata.json</p> <p>Includes:</p> <ul> <li>slope and aspect statistics  </li> <li>solar geometry used  </li> <li>mask adjustments  </li> <li>correction parameters  </li> </ul>"},{"location":"reference/schemas/#3-brdf-metadata","title":"3. BRDF metadata","text":"<p>*_brdf_metadata.json</p> <p>Includes:</p> <ul> <li>BRDF coefficients per wavelength  </li> <li>RMSE of BRDF fits  </li> <li>flags for unstable fits  </li> <li>view/sun geometry summaries  </li> </ul>"},{"location":"reference/schemas/#4-convolution-metadata-sensor-harmonization","title":"4. Convolution metadata (sensor harmonization)","text":"<p>*_convolution_metadata.json</p> <p>Contains:</p> <ul> <li>SRF files used  </li> <li>wavelength alignment checks  </li> <li>brightness correction coefficients  </li> <li>bandpass integration statistics  </li> </ul>"},{"location":"reference/schemas/#5-qa-metrics-schema","title":"5. QA metrics schema","text":"<p>The QA JSON file contains:</p> <ul> <li>reflectance summary statistics  </li> <li>mask percentages  </li> <li>wavelength metadata  </li> <li>BRDF/brightness coefficients  </li> <li>geometry metadata  </li> </ul> <p>Exact fields are defined in the validation section.</p>"},{"location":"reference/schemas/#using-schemas-in-downstream-workflows","title":"Using schemas in downstream workflows","text":"<p>These JSON files support:</p> <ul> <li>reproducibility  </li> <li>debugging  </li> <li>statistical evaluation of harmonization quality  </li> <li>provenance tracking for scientific analyses  </li> </ul>"},{"location":"reference/schemas/#next-steps","title":"Next steps","text":"<ul> <li>Validation metrics </li> <li>Pipeline outputs</li> </ul>"},{"location":"reference/validation/","title":"Validation Metrics","text":"<p>This page documents the validation metrics generated by cross-sensor-cal and recorded primarily in the QA JSON file. These metrics characterize reflectance quality, BRDF stability, harmonization accuracy, and mask distributions.</p>"},{"location":"reference/validation/#reflectance-metrics","title":"Reflectance metrics","text":"<ul> <li>min/max/median per band  </li> <li>out-of-range counts (&gt;1.5 or &lt;0)  </li> <li>percent masked </li> <li>per-band histograms (in QA PNG/PDF)  </li> </ul> <p>High out-of-range values may indicate issues in BRDF correction or DEM artifacts.</p>"},{"location":"reference/validation/#mask-metrics","title":"Mask metrics","text":"<ul> <li>cloud fraction  </li> <li>cloud-shadow fraction  </li> <li>snow fraction  </li> <li>water fraction  </li> <li>invalid/no-data fraction  </li> </ul> <p>These metrics help identify flights with adverse conditions or sensor anomalies.</p>"},{"location":"reference/validation/#brdf-metrics","title":"BRDF metrics","text":"<ul> <li>per-band BRDF model coefficients  </li> <li>reconstruction RMSE  </li> <li>flags for unstable or ill-conditioned fits  </li> </ul> <p>These diagnostics help identify pixels or bands where geometric normalization may be unreliable.</p>"},{"location":"reference/validation/#convolution-harmonization-metrics","title":"Convolution / harmonization metrics","text":"<ul> <li>brightness adjustment coefficients  </li> <li>per-band regression statistics (if applicable)  </li> <li>spectral alignment differences  </li> <li>RMSE for harmonized vs. reference spectra  </li> </ul> <p>These values help assess the quality of sensor harmonization.</p>"},{"location":"reference/validation/#geometry-and-ancillary-metrics","title":"Geometry and ancillary metrics","text":"<ul> <li>solar zenith / azimuth  </li> <li>view zenith / azimuth  </li> <li>DEM slope/aspect summaries  </li> </ul> <p>These values allow interpretation of scene illumination conditions.</p>"},{"location":"reference/validation/#using-metrics-for-quality-control","title":"Using metrics for quality control","text":"<p>Typical red flags:</p> <ul> <li>Large brightness shifts (&gt;0.1 reflectance)  </li> <li>High invalid fraction (&gt;20%)  </li> <li>Poor BRDF fits (RMSE &gt; threshold for the site/sensor)  </li> <li>Unusual wavelength alignment differences </li> </ul> <p>These indicators suggest reviewing the flight line before using it in ecological analyses.</p>"},{"location":"reference/validation/#next-steps","title":"Next steps","text":"<ul> <li>QA panels </li> <li>JSON schemas</li> </ul>"},{"location":"tutorials/cloud-workflow/","title":"Tutorial: Cloud &amp; HPC Workflows","text":"<p>This tutorial describes how to run the cross-sensor-cal pipeline efficiently in cloud or HPC environments where data access, storage, and memory constraints differ from a local workstation.</p>"},{"location":"tutorials/cloud-workflow/#overview","title":"Overview","text":"<p>You will learn:</p> <ul> <li>best practices for running the pipeline in object-storage environments  </li> <li>when to use thread mode vs. Ray mode  </li> <li>how to work with large NEON datasets without local persistence  </li> <li>strategies for scaling multi-flightline workflows  </li> </ul>"},{"location":"tutorials/cloud-workflow/#1-working-with-object-storage-eg-cyverse","title":"1. Working with object storage (e.g., CyVerse)","text":"<p>NEON HDF5 tiles can be accessed using <code>gocmd</code> or iRODS commands.</p> <p>Recommended workflow:</p> <ol> <li>Stage a small number of HDF5 tiles into a temporary working directory  </li> <li>Run the pipeline on those tiles  </li> <li>Upload corrected ENVI + Parquet outputs to persistent storage  </li> <li>Clean intermediate files to save space  </li> </ol> <p>Example staging command:</p> <p>```bash gocmd get i:/iplant/home/.../NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance.h5 . 2. Engine selection: threads vs Ray Thread engine Best for: small to medium flight lines machines with limited memory single-tile debugging Ray engine Best for: many flight lines distributed cloud environments parallel extraction and merging Enable Ray: pip install cross-sensor-cal[ray] Run: cscal-pipeline ... --engine ray 3. Memory considerations Large NEON flight lines may be tens of gigabytes. To avoid memory pressure: reduce chunk sizes use --max-workers conservatively prefer per-tile processing rather than large batch merging avoid keeping many ENVI cubes in memory simultaneously In Ray mode, ensure worker memory matches expected tile size (e.g., 16\u201332 GB per worker). 4. Recommended HPC workflow Submit one job per flight line Request enough memory for a single NEON tile (~20\u201340 GB) Use local scratch storage for temporary files Upload final ENVI + Parquet products to shared storage Merge results downstream using DuckDB or Python This scales cleanly across sites and years. 5. Example SLURM script</p>"},{"location":"tutorials/cloud-workflow/#binbash","title":"!/bin/bash","text":""},{"location":"tutorials/cloud-workflow/#sbatch-job-namecscal","title":"SBATCH --job-name=cscal","text":""},{"location":"tutorials/cloud-workflow/#sbatch-mem64g","title":"SBATCH --mem=64G","text":""},{"location":"tutorials/cloud-workflow/#sbatch-cpus-per-task8","title":"SBATCH --cpus-per-task=8","text":"<p>module load python</p> <p>BASE=$SCRATCH/cscal_${SLURM_JOB_ID} mkdir -p \"$BASE\"</p> <p>cscal-pipeline \\   --base-folder \"$BASE\" \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --engine thread \\   --max-workers 8 6. Next steps Pipeline stages Using Parquet outputs Troubleshooting</p>"},{"location":"tutorials/micasense-to-landsat/","title":"Tutorial: MicaSense \u2192 Landsat Harmonization","text":"<p>This tutorial demonstrates how to harmonize drone-scale multispectral reflectance (e.g., MicaSense RedEdge) into Landsat-equivalent band values using the cross-sensor-cal regression workflow.</p>"},{"location":"tutorials/micasense-to-landsat/#overview","title":"Overview","text":"<p>You will learn how to:</p> <ol> <li>prepare MicaSense reflectance inputs  </li> <li>apply band mapping and wavelength matching  </li> <li>use regression tables to harmonize reflectance  </li> <li>inspect harmonized Landsat-style products  </li> </ol> <p>This allows direct comparison between drone and satellite observations.</p>"},{"location":"tutorials/micasense-to-landsat/#1-inputs","title":"1. Inputs","text":"<p>You need reflectance values from a calibrated drone multispectral system. These can be:</p> <ul> <li>stacked reflectance TIFFs  </li> <li>ENVI-formatted band images  </li> <li>Parquet tables exported from your workflow  </li> </ul> <p>Each band should have known center wavelengths.</p>"},{"location":"tutorials/micasense-to-landsat/#2-harmonization-workflow","title":"2. Harmonization workflow","text":"<p>cross-sensor-cal uses regression relationships linking MicaSense band values to Landsat OLI bands. These regressions are derived from calibrated field and NEON comparisons.</p> <p>Run the following command:</p> <p>```bash cscal-micasense-to-landsat \\   --input your_micasense_input \\   --output ms_to_ls_output \\   --regression-table data/regression/micasense_to_landsat.csv Outputs include: Landsat-equivalent reflectance table diagnostic statistics optional ENVI export 3. Inspecting harmonized reflectance Example: import pandas as pd</p> <p>df = pd.read_parquet(\"ms_to_ls_output/micasense_landsat_harmonized.parquet\") df.head() Columns will match Landsat bands: Blue Green Red NIR SWIR1 SWIR2 4. NDVI sanity check df[\"ndvi\"] = (df[\"NIR\"] - df[\"Red\"]) / (df[\"NIR\"] + df[\"Red\"]) df[\"ndvi\"].describe() If harmonization worked correctly, NDVI should fall within typical vegetation ranges (0.1\u20130.9 for most cases). 5. Next steps Integrate drone \u2192 NEON \u2192 Landsat comparisons Combine harmonized products with NEON-derived spectra Use the merged output in ecological modeling workflows See also: NEON \u2192 Landsat tutorial Pipeline outputs</p>"},{"location":"tutorials/neon-to-envi/","title":"Tutorial: NEON \u2192 Corrected ENVI (BRDF + Topographic)","text":"<p>This tutorial walks through converting NEON hyperspectral HDF5 files into physically corrected ENVI reflectance cubes. The output is the foundational product used in all downstream harmonization workflows (e.g., Landsat-style reflectance).</p>"},{"location":"tutorials/neon-to-envi/#overview","title":"Overview","text":"<p>You will learn how to:</p> <ol> <li>download NEON directional reflectance tiles  </li> <li>export them to ENVI format  </li> <li>apply topographic correction  </li> <li>apply BRDF correction  </li> <li>inspect corrected outputs and QA artifacts  </li> </ol> <p>This tutorial assumes you have installed cross-sensor-cal and have run through the Quickstart.</p>"},{"location":"tutorials/neon-to-envi/#1-set-up-a-working-directory","title":"1. Set up a working directory","text":"<p>```bash BASE=output_neon_to_envi mkdir -p \"$BASE\" 2. Run the pipeline for one flight line Here we process a single NEON hyperspectral flight line at NIWO. cscal-pipeline \\   --base-folder \"$BASE\" \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --engine thread \\   --max-workers 2 The command will: fetch the necessary HDF5 files export directional reflectance to ENVI (_envi.img/.hdr) apply topographic correction apply BRDF correction write corrected ENVI reflectance (_brdfandtopo_corrected_envi.img) generate QA PNG, PDF, and JSON files Re-running the command will skip completed stages. 3. What the corrected ENVI product contains A corrected flight line directory contains files such as: _directional_reflectance_envi.img _topocorrected_envi.img _brdfandtopo_corrected_envi.img _qa.png _qa.pdf _qa.json The key output is: *_brdfandtopo_corrected_envi.img This file contains physically corrected reflectance values suitable for sensor harmonization or modeling. 4. Inspect the corrected ENVI cube You can view the ENVI product using: ENVI/IDL QGIS (with raster bands exposed) Python libraries such as rioxarray or spectral Example in Python: import rioxarray as rxr</p> <p>cube = rxr.open_rasterio(\"path/to/..._brdfandtopo_corrected_envi.img\") cube This loads the corrected reflectance cube into an xarray DataArray. 5. Inspect QA outputs Open the QA PNG: open \"$BASE/..._qa.png\" The QA panel includes: reflectance range checks brightness differences across correction stages mask summary statistics wavelength and band metadata BRDF / topographic coefficient summaries The PDF version includes multi-page diagnostics and expanded panels. 6. Next steps Proceed to: NEON \u2192 Landsat reflectance Pipeline stages for in-depth explanations Working with Parquet to extract and analyze pixel spectra</p>"},{"location":"tutorials/neon-to-landsat/","title":"Tutorial: NEON \u2192 Landsat-Style Reflectance","text":"<p>This tutorial shows how to convert physically corrected NEON ENVI reflectance into Landsat-equivalent reflectance using sensor spectral response functions (SRFs).</p> <p>The output is a set of ENVI cubes and Parquet tables representing NEON reflectance expressed in the Landsat bandspace.</p>"},{"location":"tutorials/neon-to-landsat/#prerequisites","title":"Prerequisites","text":"<p>Before running this tutorial, complete:</p> <ul> <li>NEON \u2192 corrected ENVI</li> </ul> <p>You will need the <code>*_brdfandtopo_corrected_envi.img</code> output for convolution.</p>"},{"location":"tutorials/neon-to-landsat/#1-select-a-corrected-envi-cube","title":"1. Select a corrected ENVI cube","text":"<p>Your corrected data should look like:</p> <p>.../NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance/ NEON_D13_NIWO_..._brdfandtopo_corrected_envi.img</p> <p>This file is the input to Landsat bandpass convolution.</p>"},{"location":"tutorials/neon-to-landsat/#2-run-the-convolution-stage","title":"2. Run the convolution stage","text":"<p>If you used the main pipeline, convolution runs automatically after BRDF+topo correction. To run only the convolution stage manually:</p> <p>```bash cscal-pipeline \\   --base-folder \"$BASE\" \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --max-workers 2 \\   --engine thread \\   --start-at convolution Outputs include: _landsat_convolved_envi.img _landsat_convolved_envi.hdr *_landsat_convolved.parquet 3. Understanding Landsat bandpass integration The convolution stage integrates each corrected spectrum against Landsat OLI SRFs: Coastal / Blue / Green / Red NIR SWIR1 SWIR2 This process produces reflectance values aligned to Landsat definitions, enabling: direct comparison to satellite imagery validation exercises cross-scale modeling harmonized NDVI and other vegetation indices 4. Inspecting Landsat-like ENVI outputs Example in Python: import rioxarray as rxr</p> <p>landsat = rxr.open_rasterio(\"path/to/..._landsat_convolved_envi.img\") landsat Band order, wavelengths, and metadata will match the Landsat OLI sensor. 5. Checking the QA metrics The QA JSON and QA PNG will now include: brightness adjustments used in Landsat harmonization bandwise statistics after convolution wavelength alignment checks mask and no-data diagnostics Use this to verify successful harmonization. 6. Using the Parquet tables The *_landsat_convolved.parquet file contains: one row per pixel columns for each Landsat-equivalent band pixel geometry and mask indicators Example: import duckdb</p> <p>duckdb.query(\"\"\"     SELECT Red, NIR, (NIR - Red) / (NIR + Red) AS ndvi     FROM '..._landsat_convolved.parquet'     LIMIT 5 \"\"\").df() 7. Next steps Continue to: MicaSense \u2192 Landsat harmonization Working with Parquet Pipeline QA</p>"},{"location":"usage/cli/","title":"Command-Line Interface (CLI)","text":"<p>The cross-sensor-cal CLI provides a unified, restart-safe workflow for processing NEON flight lines from raw HDF5 to corrected and sensor-harmonized ENVI/Parquet outputs. This page documents the primary commands, flags, and usage patterns.</p>"},{"location":"usage/cli/#primary-command-cscal-pipeline","title":"Primary command: <code>cscal-pipeline</code>","text":"<p>This is the recommended entry point for processing NEON hyperspectral data.</p>"},{"location":"usage/cli/#example","title":"Example","text":"<p>```bash cscal-pipeline \\   --base-folder output \\   --site-code NIWO \\   --year-month 2023-08 \\   --product-code DP1.30006.001 \\   --flight-lines NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance \\   --engine thread \\   --max-workers 4 Required arguments FlagDescription --base-folderRoot directory for all outputs --site-codeNEON site code (e.g., NIWO, KONZ) --year-monthAcquisition month in YYYY-MM format --product-codeNEON product code (usually DP1.30006.001) --flight-linesOne or more NEON flight line identifiers Optional arguments FlagDescription --engine {thread,ray}Execution engine --max-workersNumber of parallel workers --start-atStage to begin the pipeline --end-atStage to stop the pipeline --cleanRemove intermediate files --dry-runPrint what would be run without executing Pipeline stages You may specify partial workflows: cscal-pipeline \\   --start-at brdf \\   --end-at convolution Valid stages: download export-envi topo brdf convolution parquet qa Engine selection Thread engine (default) Suitable for local machines and moderate-sized flight lines. Ray engine Install: pip install cross-sensor-cal[ray] Run: cscal-pipeline --engine ray ... Use Ray for: batch processing of many flight lines distributed cloud/HPC environments large-scale Parquet extraction Inspecting available options cscal-pipeline --help This prints detailed descriptions for every flag. Other CLI tools Brief mention (documented elsewhere) cscal-micasense-to-landsat debugging utilities (internal / experimental) These experimental tools may change as the package evolves. Next steps Working with Parquet outputs Pipeline stages</p>"},{"location":"usage/notebook-example/","title":"Jupyter Notebook Example","text":"<p>This page shows a complete Jupyter-style workflow using cross-sensor-cal:</p> <ol> <li>Run the pipeline from Python  </li> <li>Preview the merged Parquet table  </li> <li>Compute a simple NDVI diagnostic  </li> </ol> <p>You can copy these cells directly into a notebook.</p>"},{"location":"usage/notebook-example/#1-imports-and-configuration","title":"1. Imports and configuration","text":"<p>```python import os import duckdb import pandas as pd</p> <p>from cross_sensor_cal import go_forth_and_multiply</p> <p>base_folder = \"output_notebook_demo\" site_code = \"NIWO\" year_month = \"2023-08\" product_code = \"DP1.30006.001\" flight_lines = [\"NEON_D13_NIWO_DP1_L020-1_20230815_directional_reflectance\"] 2. Run the pipeline from Python go_forth_and_multiply(     base_folder=base_folder,     site_code=site_code,     year_month=year_month,     product_code=product_code,     flight_lines=flight_lines,     max_workers=2,     engine=\"thread\",  # start with thread; switch to \"ray\" if configured ) This cell will: download NEON HDF5 tiles if needed export ENVI cubes apply topographic + BRDF correction convolve to Landsat (if configured) write Parquet tables generate QA PNG, PDF, and JSON 3. Locate the merged Parquet file fl_prefix = flight_lines[0] fl_dir = os.path.join(base_folder, fl_prefix)</p> <p>merged = os.path.join(     fl_dir,     f\"{fl_prefix}_merged_pixel_extraction.parquet\", ) merged 4. Preview the merged table with DuckDB con = duckdb.connect()</p> <p>head_df = con.execute(     f\"SELECT * FROM read_parquet('{merged}') LIMIT 5\" ).df() head_df 5. Compute a quick NDVI diagnostic This example assumes Landsat-style band names Red and NIR are present in the merged table. ndvi_df = con.execute(     \"\"\"     SELECT       Red,       NIR,       (NIR - Red) / NULLIF(NIR + Red, 0) AS ndvi     FROM read_parquet($merged)     LIMIT 5000     \"\"\",     {\"merged\": merged}, ).df()</p> <p>ndvi_df[\"ndvi\"].describe() You can extend this pattern to: filter by masks aggregate by plot or polygon export subsets for modeling 6. Visualize NDVI distribution (optional) If you have matplotlib installed: import matplotlib.pyplot as plt</p> <p>ndvi_df[\"ndvi\"].plot.hist(bins=50) plt.xlabel(\"NDVI\") plt.ylabel(\"Count\") plt.title(\"NDVI distribution (sample)\") plt.show() Next steps Quickstart Working with Parquet outputs Pipeline overview &amp; stages</p>"},{"location":"usage/parquet/","title":"Working with Parquet Outputs","text":"<p>The cross-sensor-cal pipeline writes Parquet files for each ENVI product it generates. These tables contain one row per pixel and are optimized for high-performance analytics with DuckDB, pandas, or xarray.</p>"},{"location":"usage/parquet/#why-parquet","title":"Why Parquet?","text":"<ul> <li>Supports efficient columnar reads  </li> <li>Compresses well for large datasets  </li> <li>Easily queryable using SQL (DuckDB)  </li> <li>Allows out-of-core or streaming access  </li> </ul>"},{"location":"usage/parquet/#file-structure","title":"File structure","text":"<p>Typical Parquet file naming:</p> <p>_brdfandtopo_corrected.parquet _landsat_convolved.parquet *_merged_pixel_extraction.parquet</p> <p>Each file contains columns for:</p> <ul> <li>reflectance values  </li> <li>band metadata  </li> <li>masks  </li> <li>pixel coordinates  </li> <li>optional ancillary variables  </li> </ul>"},{"location":"usage/parquet/#quick-preview-using-duckdb","title":"Quick preview using DuckDB","text":"<p>```python import duckdb</p> <p>duckdb.query(\"\"\"     SELECT *     FROM '..._brdfandtopo_corrected.parquet'     LIMIT 5 \"\"\").df() DuckDB provides efficient SQL queries without needing to load the entire dataset into memory. Checking dimensions duckdb.query(\"\"\"     SELECT COUNT(*) AS nrows     FROM '..._landsat_convolved.parquet' \"\"\").df() Loading with pandas import pandas as pd df = pd.read_parquet(\"..._merged_pixel_extraction.parquet\") df.head() Use with caution for large flight lines. Loading with xarray Parquet \u2192 xarray workflows work best after pivoting or aggregating data. For full spatial cubes, ENVI files remain easier to load. Streaming large files DuckDB can scan files lazily: duckdb.query(\"\"\"     SELECT AVG(NIR), AVG(Red)     FROM '..._landsat_convolved.parquet' \"\"\").df() This avoids loading the full table. Next steps CLI usage Pipeline outputs</p>"},{"location":"usage/parquet_preview/","title":"Parquet Preview","text":"pandasDuckDBpolars <pre><code>import pandas as pd\ndf = pd.read_parquet(\"merged/demo_merged_pixel_extraction.parquet\")\ndf.head()\nprint([c for c in df.columns if c.startswith(\"band_\")][:10])\n</code></pre> <pre><code>import duckdb\ncon = duckdb.connect()\ncon.execute(\"SELECT * FROM 'merged/demo_merged_pixel_extraction.parquet' LIMIT 5\").df()\n</code></pre> <pre><code>import polars as pl\ndf = pl.read_parquet(\"merged/demo_merged_pixel_extraction.parquet\")\ndf.head(5)\n</code></pre>"}]}