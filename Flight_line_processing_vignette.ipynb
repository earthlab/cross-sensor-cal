{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4aef327-6a42-4f72-b6c5-6c34f07e887f",
   "metadata": {},
   "source": [
    "# Processing and Correcting NEON Hyperspectral Flight Lines for Scalable Spectral Data Analysis\n",
    "\n",
    "Welcome to this vignette! This guide provides a detailed walkthrough for processing NEON (National Ecological Observatory Network) flight line data, taking you from raw downloads to actionable outputs. The workflow includes converting raw NEON flight lines into ENVI-compatible formats, applying essential data corrections (such as topographic and BRDF adjustments), and extracting hyperspectral data to build comprehensive tables for numerical and statistical analysis.\n",
    "\n",
    "This workflow has been carefully designed to address the challenges of processing large datasets, ensuring efficient memory usage and high data integrity, even within the constraints of a machine with 250 GB of RAM. By the end of this guide, you will have the tools and understanding to transform raw hyperspectral data into corrected, high-quality datasets ready for advanced ecological and environmental research.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Prerequisites](#2-prerequisites)\n",
    "3. [Environment Setup](#3-environment-setup)\n",
    "4. [Understanding NEON Flight Lines](#4-understanding-neon-flight-lines)\n",
    "5. [Finding NEON Flight Codes](#5-finding-neon-flight-codes)\n",
    "6. [Running the `jefe` Function](#6-running-the-jefe-function)\n",
    "7. [Handling Large Data Processing](#7-handling-large-data-processing)\n",
    "8. [Extracting Data and Building Tables](#8-extracting-data-and-building-tables)\n",
    "9. [Workarounds for RAM Limitations](#9-workarounds-for-ram-limitations)\n",
    "10. [Conclusion](#10-conclusion)\n",
    "11. [References](#11-references)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Hyperspectral data collected through NEON (National Ecological Observatory Network) flight lines provides a high-resolution spectral view of the Earth's surface, capturing detailed information about vegetation, soils, water, and other environmental components. However, the raw NEON data comes in specialized formats that require processing and correction before they can be used for meaningful analysis. \n",
    "\n",
    "This vignette provides a detailed, step-by-step guide to download, convert, and process NEON flight line data, ensuring efficient memory usage and high data integrity throughout the workflow. Along the way, we will create specific file types needed for both corrections and analysis, bridging the gap between raw hyperspectral data and actionable insights.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Extract Hyperspectral Signals?**\n",
    "Hyperspectral data is invaluable for ecological and environmental research as it provides detailed spectral signatures across hundreds of bands. By extracting these signals and applying corrections, researchers can:\n",
    "- **Translate Patterns Across Scales:** Connect fine-scale field measurements to broader regional or global observations.\n",
    "- **Quantify Environmental Changes:** Monitor vegetation health, water quality, or land cover changes over time.\n",
    "- **Improve Decision-Making Tools:** Build robust models for ecological resilience, biodiversity, and conservation planning.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of This Workflow**\n",
    "\n",
    "This workflow is particularly suited for:\n",
    "- **Scaling Insights Across Spatial Domains:** Translating fine-scale hyperspectral data to broader landscapes ensures consistency and comparability across scales.\n",
    "- **Monitoring Environmental Changes:** Creating corrected and high-fidelity datasets to track vegetation health, water quality, or land cover over time.\n",
    "- **Enabling Cross-Sensor Calibration:** Harmonizing hyperspectral data across platforms by applying consistent corrections and resampling techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prerequisites\n",
    "\n",
    "Before you begin, ensure you have the following:\n",
    "\n",
    "- **Hardware Requirements:**\n",
    "  - A machine with at least **250 GB RAM** to handle large datasets efficiently.\n",
    "\n",
    "- **Software Requirements:**\n",
    "  - Access to a pre-configured Python environment with necessary libraries installed, including:\n",
    "    - `geopandas`, `rasterio`, `pandas`, `numpy`, `hytools`, `scikit-learn`, `matplotlib`, `requests`, `h5py`, `ray`.\n",
    "\n",
    "- **Data Requirements:**\n",
    "  - NEON flight line data.\n",
    "  - Corresponding flight codes to identify and process the relevant flight lines.\n",
    "\n",
    "- **Additional Tools:**\n",
    "  - A Jupyter Notebook interface to follow this vignette step by step.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Python Setup\n",
    "\n",
    "To follow this vignette, you'll need a Python environment configured with the necessary dependencies. If you haven't set up your environment yet, follow the steps below to install the required tools and libraries. This guide assumes you are working in a Jupyter Notebook.\n",
    "\n",
    "### Required Libraries\n",
    "Ensure the following libraries are available in your environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ef913d0-cb8a-4b4a-8367-fa6ff501f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hytools as ht\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import nbconvert\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3e25445-87f1-4374-ba9c-fc46a40cf35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spectral in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (0.23.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/macrosystems/lib/python3.10/site-packages (from spectral) (1.26.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "feecdd2a-7c7c-47fe-9b1e-fa11b646ec05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "['ENVIProcessor', 'GradientBoostingRegressor', '__builtins__', '__cached__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'apply_topo_and_brdf_corrections', 'boosted_quantile_plot', 'boosted_quantile_plot_by_sensor', 'box', 'clean_data_and_write_to_csv', 'concatenate_sensors', 'control_function', 'download_neon_file', 'download_neon_flight_lines', 'extract_overlapping_layers_to_2d_dataframe', 'find_raster_files', 'fit_models_with_different_alpha', 'flight_lines_to_envi', 'generate_config_json', 'generate_correction_configs', 'generate_correction_configs_for_directory', 'get_spectral_data_and_wavelengths', 'glob', 'go_forth_and_multiply', 'gpd', 'h5py', 'ht', 'jefe', 'json', 'load_and_combine_rasters', 'load_spectra', 'mask', 'np', 'open_image', 'os', 'pd', 'plot_each_sensor_with_highlight', 'plot_spectral_data', 'plot_with_highlighted_sensors', 'plt', 'prepare_spectral_data', 'process_all_subdirectories', 'process_and_flatten_array', 'process_hdf5_with_neon2envi', 'random', 'rasterio', 'rasterize', 'rasterize_polygons_to_match_envi', 'ray', 'requests', 'resample_translation_to_other_sensors', 'reshape_spectra', 'show', 'show_rgb', 'subprocess', 'time', 'translate_to_other_sensors']\n"
     ]
    }
   ],
   "source": [
    "### Loading Earth Lab Spectral Tools\n",
    "\n",
    "# 1. Enable autoreload in your Jupyter Notebook:\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 2. Import the custom tools module:\n",
    "\n",
    "import spectral_unmixing_tools_original as el_spectral\n",
    "\n",
    "# 3. Verify that the tools loaded correctly by printing the module's directory:\n",
    "\n",
    "print(dir(el_spectral))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aede84-ae4b-4f38-8cc1-584a862069f6",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Understanding and Finding NEON Flight Lines\n",
    "\n",
    "NEON flight lines are aerial survey paths designed to collect high-resolution spectral data across various ecological sites. These datasets are vital for studying vegetation, soil, water bodies, and other environmental parameters, forming the foundation for many ecological and environmental analyses.\n",
    "\n",
    "### **How to Find Flight Codes**\n",
    "To process NEON flight lines with the `jefe` function, you’ll need the flight codes corresponding to your desired data. Follow these steps to find them:\n",
    "1. **Access the NEON Data Portal:** Visit the [NEON Data Portal](https://data.neonscience.org/) to browse available datasets.\n",
    "2. **Navigate to Flight Line Data:** Locate the section for flight line spectral data at your site of interest.\n",
    "3. **Identify Relevant Flight Codes:** Each flight line has associated metadata, including its unique flight code. Record the codes for the lines you wish to process.\n",
    "\n",
    "### **Important Considerations**\n",
    "1. **Data Availability:**\n",
    "   - NEON’s Airborne Observation Platform (AOP) data is generally available 60 days after the last collection day at a site.\n",
    "   - Data collection schedules may shift due to weather or logistical factors. For the latest updates, consult the [NEON Flight Schedules and Coverage page](https://www.neonscience.org/data-collection/flight-schedules-coverage).\n",
    "\n",
    "2. **Data Quality Updates:**\n",
    "   - NEON regularly updates its data products to address quality concerns or implement new processing methods.\n",
    "   - Stay informed about updates or changes that could affect your datasets by checking the [AOP Data Availability Notification](https://www.neonscience.org/impact/observatory-blog/aop-data-availability-notification-release-2024).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Running the `jefe` Function\n",
    "\n",
    "The `jefe` function orchestrates the entire workflow, including converting flight lines into appropriate file formats, applying corrections, and extracting pixel data to build tables.\n",
    "\n",
    "### Parameters for `jefe`\n",
    "\n",
    "To effectively utilize the `jefe` function for processing NEON flight line data, it's crucial to understand and accurately specify its parameters. Below is a detailed guide on each parameter, including how to obtain the necessary information.\n",
    "\n",
    "#### **`base_folder` (str)**\n",
    "- **Description:** The directory where output files will be stored.\n",
    "- **How to Specify:** Choose or create a directory path on your local system where you want the processed data to be saved.\n",
    "\n",
    "#### **`site_code` (str)**\n",
    "- **Description:** The NEON site code representing the specific field site.\n",
    "- **How to Find:**\n",
    "  - NEON assigns unique four-letter codes to each field site (e.g., \"NIWO\" for Niwot Ridge).\n",
    "  - You can find these codes on the [NEON Field Sites page](https://www.neonscience.org/field-sites/explore).\n",
    "\n",
    "#### **`product_code` (str)**\n",
    "- **Description:** The NEON data product code identifying the specific data product.\n",
    "- **How to Find:**\n",
    "  - NEON data products have unique identifiers (e.g., \"DP1.30003.001\" for discrete return LiDAR point cloud data).\n",
    "  - Browse the [NEON Data Products Catalog](https://data.neonscience.org/data-products/explore) to locate the product code relevant to your research.\n",
    "\n",
    "#### **`year_month` (str)**\n",
    "- **Description:** The year and month of data collection in `'YYYY-MM'` format.\n",
    "- **How to Determine:**\n",
    "  - Data collection periods vary by site and product. Consult the [NEON Data Availability page](https://data.neonscience.org/visualizations/data-availability) to check when data was collected for your site and product of interest.\n",
    "  - **Important Note:** Data availability is subject to change due to factors like weather conditions and program planning adjustments.\n",
    "\n",
    "#### **`flight_lines` (list)**\n",
    "- **Description:** A list of flight line codes to process.\n",
    "- **How to Find:**\n",
    "  - Flight line codes correspond to specific aerial survey paths.\n",
    "  - Access the [NEON Data Portal](https://data.neonscience.org/) and navigate to the desired data product and site.\n",
    "  - Flight line codes are typically listed in the metadata associated with each dataset.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b48a48-e40b-4702-8ac5-56253fe5a20f",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a0b8e5c2-6fb5-43d2-9eae-3e30aa63c8c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing flight line: D13_NIWO_DP1_20200807_170802\n",
      "Data retrieved successfully for 2020-08!\n",
      "Downloading NEON_D13_NIWO_DP1_20200807_170802_reflectance.h5 from https://storage.googleapis.com/neon-aop-products/2020/FullSite/D13/2020_NIWO_4/L1/Spectrometer/ReflectanceH5/2020080714/NEON_D13_NIWO_DP1_20200807_170802_reflectance.h5\n",
      "Download completed for NEON_D13_NIWO_DP1_20200807_170802_reflectance.h5\n",
      "Download completed.\n",
      "\n",
      "Processing: ./NEON_D13_NIWO_DP1_20200807_170802_reflectance.h5\n",
      "Error executing command: /opt/conda/envs/macrosystems/bin/python neon2envi2_generic.py --images './NEON_D13_NIWO_DP1_20200807_170802_reflectance.h5' --output_dir 'Next_try' -anc\n",
      "Standard Output: Here we GO!\n",
      "\n",
      "Error Output: 2024-12-18 20:58:58,945\tERROR services.py:1329 -- Failed to start the dashboard , return code 1\n",
      "2024-12-18 20:58:58,946\tERROR services.py:1354 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure' to find where the log file is.\n",
      "2024-12-18 20:58:58,946\tERROR services.py:1398 -- \n",
      "The last 20 lines of /tmp/ray/session_2024-12-18_20-58-57_268828_64589/logs/dashboard.log (it contains the error message from the dashboard): \n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/dashboard/modules/job/cli.py\", line 16, in <module>\n",
      "    from ray.job_submission import JobStatus, JobSubmissionClient\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/job_submission/__init__.py\", line 2, in <module>\n",
      "    from ray.dashboard.modules.job.pydantic_models import DriverInfo, JobDetails, JobType\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/dashboard/modules/job/pydantic_models.py\", line 4, in <module>\n",
      "    from ray._private.pydantic_compat import BaseModel, Field, PYDANTIC_INSTALLED\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/_private/pydantic_compat.py\", line 100, in <module>\n",
      "    monkeypatch_pydantic_2_for_cloudpickle()\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/_private/pydantic_compat.py\", line 58, in monkeypatch_pydantic_2_for_cloudpickle\n",
      "    pydantic._internal._model_construction.SchemaSerializer = (\n",
      "AttributeError: module 'pydantic._internal' has no attribute '_model_construction'\n",
      "2024-12-18 20:58:58,949\tWARNING services.py:1996 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2024-12-18 20:59:00,074\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/data-store/cross-sensor-cal/neon2envi2_generic.py\", line 197, in <module>\n",
      "    main()\n",
      "  File \"/home/jovyan/data-store/cross-sensor-cal/neon2envi2_generic.py\", line 188, in main\n",
      "    _ = ray.get([a.read_file.remote(image, 'neon') for a, image in zip(actors, args.images)])\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/macrosystems/lib/python3.10/site-packages/ray/_private/worker.py\", line 2565, in get\n",
      "    raise value\n",
      "ray.exceptions.RayActorError: The actor died unexpectedly before finishing this task.\n",
      "\tclass_name: HyTools\n",
      "\tactor_id: 48a390c2324e771a0330367801000000\n",
      "\tnamespace: 43b89b31-e0c1-47f9-bafd-72976ab9b9b4\n",
      "The actor is dead because its owner has died. Owner Id: 01000000ffffffffffffffffffffffffffffffffffffffffffffffff Owner Ip address: 10.33.0.24 Owner worker exit type: SYSTEM_ERROR Worker exit detail: Owner's node has crashed.\n",
      "The actor never ran - it was cancelled before it started running.\n",
      "\u001b[33m(raylet)\u001b[0m [2024-12-18 20:59:00,430 E 64930 64977] (raylet) agent_manager.cc:70: The raylet exited immediately because one Ray agent failed, agent_name = dashboard_agent/424238335.\n",
      "\u001b[33m(raylet)\u001b[0m The raylet fate shares with the agent. This can happen because\n",
      "\u001b[33m(raylet)\u001b[0m - The version of `grpcio` doesn't follow Ray's requirement. Agent can segfault with the incorrect `grpcio` version. Check the grpcio version `pip freeze | grep grpcio`.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent failed to start because of unexpected error or port conflict. Read the log `cat /tmp/ray/session_latest/logs/{dashboard_agent|runtime_env_agent}.log`. You can find the log file structure here https://docs.ray.io/en/master/ray-observability/ray-logging.html#logging-directory-structure.\n",
      "\u001b[33m(raylet)\u001b[0m - The agent is killed by the OS (e.g., out of memory).\n",
      "\n",
      "Starting topo and BRDF correction. This takes a long time.\n",
      "All done!\n",
      "Starting tranlation to other sensors\n",
      "done resampling\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# el jefe takes 3-5 hours to run and it creates a lot of files. You should have 200+GB of RAM and Storage available.\n",
    "base_folder = \"Next_try\"\n",
    "site_code = 'NIWO'\n",
    "product_code = 'DP1.30006.001'\n",
    "year_month = '2020-08'\n",
    "flight_lines = [\n",
    "    'D13_NIWO_DP1_20200807_170802'\n",
    "]\n",
    "# BRDF correction is failing with only one flight line provided but works when a list is longer than one. \n",
    "\n",
    "# Error out after correction when it should be moving to translation. That's the function I've been trying to work and have been trying to isolate. \n",
    "# Run the jefe function with the provided example parameters\n",
    "el_spectral.jefe(base_folder, site_code, product_code, year_month, flight_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d050aae-66d8-4d5a-80b8-a7942fb5d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import rasterio\n",
    "from spectral import open_image\n",
    "\n",
    "# ----- Supporting Classes and Functions -----\n",
    "\n",
    "class ENVIProcessor:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.data = None  # This will hold the raster data array\n",
    "        self.file_type = \"envi\"\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads the raster data from the file_path into self.data\"\"\"\n",
    "        with rasterio.open(self.file_path) as src:\n",
    "            self.data = src.read()  # Read all bands\n",
    "\n",
    "    def get_chunk_from_extent(self, corrections=[], resample=False):\n",
    "        self.load_data()  # Ensure data is loaded\n",
    "        return self.data\n",
    "\n",
    "\n",
    "def find_raster_files(directory):\n",
    "    \"\"\"\n",
    "    Searches for raster files in the given directory, capturing both original and corrected ENVI files,\n",
    "    plus resampled ones, while excluding .hdr, .json, .csv, and any files containing '_mask' or '_ancillary'.\n",
    "    We'll look for filenames containing '_reflectance' (original) or '_envi' (corrected/resampled).\n",
    "    \"\"\"\n",
    "    pattern = \"*\"\n",
    "    full_pattern = os.path.join(directory, pattern)\n",
    "    all_files = glob.glob(full_pattern)\n",
    "\n",
    "    filtered_files = [\n",
    "        file for file in all_files\n",
    "        if (\n",
    "            ('_reflectance' in os.path.basename(file) or '_envi' in os.path.basename(file)) and\n",
    "            '_mask' not in os.path.basename(file) and\n",
    "            '_ancillary' not in os.path.basename(file) and\n",
    "            not file.endswith('.hdr') and\n",
    "            not file.endswith('.json') and\n",
    "            not file.endswith('.csv')\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    found_files_set = set(filtered_files)\n",
    "    found_files = list(found_files_set)\n",
    "    found_files.sort()\n",
    "\n",
    "    return found_files\n",
    "\n",
    "\n",
    "def load_and_combine_rasters(raster_paths):\n",
    "    \"\"\"\n",
    "    Loads and combines raster data from a list of file paths.\n",
    "    Assumes each raster has shape (bands, rows, cols) and that\n",
    "    all rasters can be concatenated along the band dimension.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    for path in raster_paths:\n",
    "        processor = ENVIProcessor(path)\n",
    "        chunk = processor.get_chunk_from_extent(corrections=['some_correction'], resample=False)\n",
    "        chunks.append(chunk)\n",
    "    combined_array = np.concatenate(chunks, axis=0)  # Combine along the first axis (bands)\n",
    "    return combined_array\n",
    "\n",
    "\n",
    "def process_and_flatten_array(array, json_dir='Resampling', original_bands=426, corrected_bands=426,\n",
    "                              original_wavelengths=None, corrected_wavelengths=None, folder_name=None,\n",
    "                              map_info=None):\n",
    "    \"\"\"\n",
    "    Processes a 3D numpy array to a DataFrame, adds metadata columns, \n",
    "    renames columns dynamically based on JSON configuration, and adds Pixel_id.\n",
    "    Uses provided wavelength lists to name original and corrected bands, and includes geocoordinates.\n",
    "\n",
    "    Parameters:\n",
    "    - array: A 3D numpy array of shape (bands, rows, cols).\n",
    "    - json_dir: Directory containing the landsat_band_parameters.json file.\n",
    "    - original_bands: Number of original bands expected.\n",
    "    - corrected_bands: Number of corrected bands expected.\n",
    "    - original_wavelengths: List of wavelengths for the original bands (floats).\n",
    "    - corrected_wavelengths: List of wavelengths for the corrected bands (floats).\n",
    "    - folder_name: Name of the subdirectory (flight line identifier).\n",
    "    - map_info: The map info array from the metadata for georeferencing.\n",
    "\n",
    "    Returns:\n",
    "    - A pandas DataFrame with additional metadata columns and renamed band columns.\n",
    "    \"\"\"\n",
    "    if len(array.shape) != 3:\n",
    "        raise ValueError(\"Input array must be 3-dimensional. Expected (bands, rows, cols).\")\n",
    "\n",
    "    json_file = os.path.join(json_dir, 'landsat_band_parameters.json')\n",
    "    if not os.path.isfile(json_file):\n",
    "        raise FileNotFoundError(f\"JSON file not found: {json_file}\")\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    bands, rows, cols = array.shape\n",
    "    print(f\"[DEBUG] array shape: bands={bands}, rows={rows}, cols={cols}\")\n",
    "\n",
    "    reshaped_array = array.reshape(bands, -1).T  # (pixels, bands)\n",
    "    pixel_indices = np.indices((rows, cols)).reshape(2, -1).T  # (pixels, 2)\n",
    "    df = pd.DataFrame(reshaped_array, columns=[f'Band_{i+1}' for i in range(bands)])\n",
    "\n",
    "    # Extract map info for georeferencing:\n",
    "    # Format: [projection, x_pixel_start, y_pixel_start, map_x, map_y, x_res, y_res, ...]\n",
    "    # Typically:\n",
    "    #   x_pixel_start, y_pixel_start = 1,1 for upper-left pixel\n",
    "    #   map_x, map_y = coordinates of that upper-left pixel\n",
    "    #   x_res, y_res = pixel sizes (y_res should be positive but we assume north-down in ENVI)\n",
    "    if map_info is not None and len(map_info) >= 7:\n",
    "        projection = map_info[0]\n",
    "        x_pixel_start = float(map_info[1])\n",
    "        y_pixel_start = float(map_info[2])\n",
    "        map_x = float(map_info[3])\n",
    "        map_y = float(map_info[4])\n",
    "        x_res = float(map_info[5])\n",
    "        y_res = float(map_info[6])\n",
    "    else:\n",
    "        # Fallback if map_info is not provided\n",
    "        projection = 'Unknown'\n",
    "        x_pixel_start, y_pixel_start = 1.0, 1.0\n",
    "        map_x, map_y = 0.0, 0.0\n",
    "        x_res, y_res = 1.0, 1.0\n",
    "\n",
    "    # Compute Easting, Northing\n",
    "    # Pixel_row and Pixel_col are zero-based. \n",
    "    # According to ENVI conventions:\n",
    "    # Easting = map_x + (pixel_col - (x_pixel_start - 1)) * x_res\n",
    "    # Northing = map_y - (pixel_row - (y_pixel_start - 1)) * y_res\n",
    "    pixel_row = pixel_indices[:, 0]\n",
    "    pixel_col = pixel_indices[:, 1]\n",
    "    Easting = map_x + (pixel_col - (x_pixel_start - 1)) * x_res\n",
    "    Northing = map_y - (pixel_row - (y_pixel_start - 1)) * y_res\n",
    "\n",
    "    # Insert Pixel info and coordinates\n",
    "    df.insert(0, 'Pixel_Col', pixel_col)\n",
    "    df.insert(0, 'Pixel_Row', pixel_row)\n",
    "    df.insert(0, 'Pixel_id', np.arange(len(df)))\n",
    "    df.insert(3, 'Easting', Easting)\n",
    "    df.insert(4, 'Northing', Northing)\n",
    "\n",
    "    # Check we have enough bands\n",
    "    if bands < (original_bands + corrected_bands):\n",
    "        raise ValueError(\n",
    "            f\"Not enough bands. Expected at least {original_bands + corrected_bands} (original+corrected), but got {bands}.\"\n",
    "        )\n",
    "\n",
    "    # Determine Corrected and Resampled flags\n",
    "    remaining_bands = bands - (original_bands + corrected_bands)\n",
    "    corrected_flag = \"Yes\" if corrected_bands > 0 else \"No\"\n",
    "    resampled_flag = \"Yes\" if remaining_bands > 0 else \"No\"\n",
    "\n",
    "    # Metadata columns: Subdirectory, Data_Source, Sensor_Type, Corrected, Resampled\n",
    "    # Insert these at the very front\n",
    "    df.insert(0, 'Resampled', resampled_flag)\n",
    "    df.insert(0, 'Corrected', corrected_flag)\n",
    "    df.insert(0, 'Sensor_Type', 'Hyperspectral')\n",
    "    df.insert(0, 'Data_Source', 'Flight line')\n",
    "    df.insert(0, 'Subdirectory', folder_name if folder_name else 'Unknown')\n",
    "\n",
    "    # Rename bands with wavelengths\n",
    "    band_names = []\n",
    "    # Original bands\n",
    "    if original_wavelengths is not None and len(original_wavelengths) >= original_bands:\n",
    "        for i in range(original_bands):\n",
    "            wl = original_wavelengths[i]\n",
    "            band_names.append(f\"Original_band_{i+1}_wl_{wl}nm\")\n",
    "    else:\n",
    "        for i in range(1, original_bands + 1):\n",
    "            band_names.append(f\"Original_band_{i}\")\n",
    "\n",
    "    # Corrected bands\n",
    "    if corrected_wavelengths is not None and len(corrected_wavelengths) >= corrected_bands:\n",
    "        for i in range(corrected_bands):\n",
    "            wl = corrected_wavelengths[i]\n",
    "            band_names.append(f\"Corrected_band_{i+1}_wl_{wl}nm\")\n",
    "    elif original_wavelengths is not None and len(original_wavelengths) >= corrected_bands:\n",
    "        for i in range(corrected_bands):\n",
    "            wl = original_wavelengths[i]\n",
    "            band_names.append(f\"Corrected_band_{i+1}_wl_{wl}nm\")\n",
    "    else:\n",
    "        for i in range(1, corrected_bands + 1):\n",
    "            band_names.append(f\"Corrected_band_{i}\")\n",
    "\n",
    "    print(f\"[DEBUG] remaining_bands for resampled sensors: {remaining_bands}\")\n",
    "\n",
    "    sensor_bands_assigned = 0\n",
    "    for sensor, details in config.items():\n",
    "        wavelengths = details.get('wavelengths', [])\n",
    "        for i, wl in enumerate(wavelengths, start=1):\n",
    "            if sensor_bands_assigned < remaining_bands:\n",
    "                band_names.append(f\"{sensor}_band_{i}_wl_{wl}nm\")\n",
    "                sensor_bands_assigned += 1\n",
    "            else:\n",
    "                break\n",
    "        if sensor_bands_assigned >= remaining_bands:\n",
    "            break\n",
    "\n",
    "    if sensor_bands_assigned < remaining_bands:\n",
    "        extra = remaining_bands - sensor_bands_assigned\n",
    "        print(f\"[DEBUG] {extra} leftover bands have no matching sensors/wavelengths in JSON. Naming them generically.\")\n",
    "        for i in range(1, extra + 1):\n",
    "            band_names.append(f\"Unassigned_band_{i}\")\n",
    "\n",
    "    # Now we have Pixel_id, Pixel_Row, Pixel_Col, Easting, Northing, and multiple metadata columns.\n",
    "    # Determine how many leading metadata columns we have before bands:\n",
    "    # Currently: Subdirectory, Data_Source, Sensor_Type, Corrected, Resampled, Pixel_id, Pixel_Row, Pixel_Col, Easting, Northing\n",
    "    # That's 10 columns before bands start.\n",
    "    metadata_count = 10\n",
    "\n",
    "    new_columns = list(df.columns[:metadata_count]) + band_names\n",
    "    if len(new_columns) != df.shape[1]:\n",
    "        raise ValueError(\n",
    "            f\"Band naming mismatch: {len(new_columns)} columns assigned vs {df.shape[1]} in df. Check indexing.\"\n",
    "        )\n",
    "\n",
    "    df.columns = new_columns\n",
    "\n",
    "    print(f\"[DEBUG] Final DataFrame shape: {df.shape}\")\n",
    "    print(\"[DEBUG] Columns assigned successfully.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data_and_write_to_csv(df, output_csv_path, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Cleans a large DataFrame by processing it in chunks and then writes it to a CSV file.\n",
    "    \"\"\"\n",
    "    total_rows = df.shape[0]\n",
    "    num_chunks = (total_rows // chunk_size) + (1 if total_rows % chunk_size else 0)\n",
    "\n",
    "    print(f\"Cleaning data and writing to CSV in {num_chunks} chunk(s).\")\n",
    "\n",
    "    first_chunk = True\n",
    "    for i, start_row in enumerate(range(0, total_rows, chunk_size)):\n",
    "        chunk = df.iloc[start_row:start_row + chunk_size].copy()\n",
    "        non_pixel_cols = [col for col in chunk.columns if not col.startswith('Pixel') and \n",
    "                          col not in ['Subdirectory','Data_Source','Sensor_Type','Corrected','Resampled',\n",
    "                                      'Easting','Northing']]\n",
    "\n",
    "        # Replace -9999 values with NaN\n",
    "        chunk[non_pixel_cols] = chunk[non_pixel_cols].apply(\n",
    "            lambda x: np.where(np.isclose(x, -9999, atol=1), np.nan, x)\n",
    "        )\n",
    "\n",
    "        # Drop rows with all NaNs in non-pixel columns (spectral data)\n",
    "        chunk.dropna(subset=non_pixel_cols, how='all', inplace=True)\n",
    "\n",
    "        mode = 'w' if first_chunk else 'a'\n",
    "        header = True if first_chunk else False\n",
    "        chunk.to_csv(output_csv_path, mode=mode, header=header, index=False)\n",
    "\n",
    "        print(f\"Chunk {i+1}/{num_chunks} processed and written.\")\n",
    "        first_chunk = False\n",
    "\n",
    "    print(f\"Data cleaning complete. Output written to: {output_csv_path}\")\n",
    "\n",
    "\n",
    "def control_function(directory):\n",
    "    \"\"\"\n",
    "    Orchestrates the finding, loading, processing of raster files found in a specified directory,\n",
    "    cleans the processed data, and saves it to a CSV file in the same directory.\n",
    "    \"\"\"\n",
    "    raster_paths = find_raster_files(directory)\n",
    "\n",
    "    if not raster_paths:\n",
    "        print(f\"No matching raster files found in {directory}.\")\n",
    "        return\n",
    "\n",
    "    # Assume original file name (without _envi etc.) is the directory name\n",
    "    base_name = os.path.basename(os.path.normpath(directory))\n",
    "    hdr_file = os.path.join(os.path.dirname(directory), base_name + '.hdr')\n",
    "    if not os.path.isfile(hdr_file):\n",
    "        hdr_file = os.path.join(directory, base_name + '.hdr')\n",
    "\n",
    "    original_wavelengths = None\n",
    "    map_info = None\n",
    "    if os.path.isfile(hdr_file):\n",
    "        img = open_image(hdr_file)\n",
    "        original_wavelengths = img.metadata.get('wavelength', [])\n",
    "        # Convert to float if they are strings\n",
    "        original_wavelengths = [float(w) for w in original_wavelengths]\n",
    "        map_info = img.metadata.get('map info', None)\n",
    "    else:\n",
    "        print(f\"No HDR file found at {hdr_file}. Will use generic band names and no geocoords.\")\n",
    "\n",
    "    corrected_wavelengths = original_wavelengths\n",
    "\n",
    "    # Load and combine raster data\n",
    "    combined_array = load_and_combine_rasters(raster_paths)  \n",
    "    print(f\"Combined array shape for directory {directory}: {combined_array.shape}\")\n",
    "\n",
    "    # Attempt to process and flatten the array into a DataFrame\n",
    "    try:\n",
    "        df_processed = process_and_flatten_array(\n",
    "            combined_array,\n",
    "            json_dir='Resampling',\n",
    "            original_bands=426,\n",
    "            corrected_bands=426,\n",
    "            original_wavelengths=original_wavelengths,\n",
    "            corrected_wavelengths=corrected_wavelengths,\n",
    "            folder_name=base_name,\n",
    "            map_info=map_info\n",
    "        )  \n",
    "        print(f\"DataFrame shape after flattening for directory {directory}: {df_processed.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"ValueError encountered during processing of {directory}: {e}\")\n",
    "        print(\"Check the number of bands vs. the expected column names in process_and_flatten_array().\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {directory}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Extract the folder name from the directory path\n",
    "    folder_name = os.path.basename(os.path.normpath(directory))\n",
    "    output_csv_name = f\"{folder_name}_spectral_data_all_sensors.csv\"\n",
    "    output_csv_path = os.path.join(directory, output_csv_name)\n",
    "\n",
    "    # Always overwrite if CSV exists\n",
    "    if os.path.exists(output_csv_path):\n",
    "        print(f\"CSV {output_csv_path} already exists and will be overwritten.\")\n",
    "\n",
    "    # Clean data and write to CSV\n",
    "    clean_data_and_write_to_csv(df_processed, output_csv_path)  \n",
    "    print(f\"Processed and cleaned data saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "def process_all_subdirectories(parent_directory):\n",
    "    \"\"\"\n",
    "    Searches for all subdirectories within the given parent directory, excluding non-directory files,\n",
    "    and applies raster file processing to each subdirectory found.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(parent_directory):\n",
    "        full_path = os.path.join(parent_directory, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            try:\n",
    "                control_function(full_path)\n",
    "                print(f\"Finished processing for directory: {full_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing directory '{full_path}': {e}\")\n",
    "        else:\n",
    "            print(f\"Skipping non-directory item: {full_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5ef0af-0546-43d8-9082-ca9d3cb18033",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined array shape for directory New_Test/NEON_D13_NIWO_DP1_20200807_170802_reflectance: (897, 10487, 994)\n",
      "[DEBUG] array shape: bands=897, rows=10487, cols=994\n",
      "[DEBUG] remaining_bands for resampled sensors: 45\n",
      "[DEBUG] Final DataFrame shape: (10424078, 907)\n",
      "[DEBUG] Columns assigned successfully.\n",
      "DataFrame shape after flattening for directory New_Test/NEON_D13_NIWO_DP1_20200807_170802_reflectance: (10424078, 907)\n",
      "CSV New_Test/NEON_D13_NIWO_DP1_20200807_170802_reflectance/NEON_D13_NIWO_DP1_20200807_170802_reflectance_spectral_data_all_sensors.csv already exists and will be overwritten.\n",
      "Cleaning data and writing to CSV in 105 chunk(s).\n",
      "Chunk 1/105 processed and written.\n",
      "Chunk 2/105 processed and written.\n",
      "Chunk 3/105 processed and written.\n",
      "Chunk 4/105 processed and written.\n",
      "Chunk 5/105 processed and written.\n",
      "Chunk 6/105 processed and written.\n",
      "Chunk 7/105 processed and written.\n",
      "Chunk 8/105 processed and written.\n",
      "Chunk 9/105 processed and written.\n",
      "Chunk 10/105 processed and written.\n",
      "Chunk 11/105 processed and written.\n",
      "Chunk 12/105 processed and written.\n",
      "Chunk 13/105 processed and written.\n",
      "Chunk 14/105 processed and written.\n",
      "Chunk 15/105 processed and written.\n",
      "Chunk 16/105 processed and written.\n",
      "Chunk 17/105 processed and written.\n",
      "Chunk 18/105 processed and written.\n",
      "Chunk 19/105 processed and written.\n",
      "Chunk 20/105 processed and written.\n",
      "Chunk 21/105 processed and written.\n",
      "Chunk 22/105 processed and written.\n",
      "Chunk 23/105 processed and written.\n",
      "Chunk 24/105 processed and written.\n",
      "Chunk 25/105 processed and written.\n",
      "Chunk 26/105 processed and written.\n",
      "Chunk 27/105 processed and written.\n",
      "Chunk 28/105 processed and written.\n",
      "Chunk 29/105 processed and written.\n",
      "Chunk 30/105 processed and written.\n",
      "Chunk 31/105 processed and written.\n",
      "Chunk 32/105 processed and written.\n",
      "Chunk 33/105 processed and written.\n",
      "Chunk 34/105 processed and written.\n",
      "Chunk 35/105 processed and written.\n",
      "Chunk 36/105 processed and written.\n",
      "Chunk 37/105 processed and written.\n",
      "Chunk 38/105 processed and written.\n",
      "Chunk 39/105 processed and written.\n",
      "Chunk 40/105 processed and written.\n",
      "Chunk 41/105 processed and written.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m base_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew_Test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mprocess_all_subdirectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 342\u001b[0m, in \u001b[0;36mprocess_all_subdirectories\u001b[0;34m(parent_directory)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(full_path):\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 342\u001b[0m         \u001b[43mcontrol_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished processing for directory: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[3], line 329\u001b[0m, in \u001b[0;36mcontrol_function\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSV \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists and will be overwritten.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    328\u001b[0m \u001b[38;5;66;03m# Clean data and write to CSV\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m \u001b[43mclean_data_and_write_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m    330\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed and cleaned data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 256\u001b[0m, in \u001b[0;36mclean_data_and_write_to_csv\u001b[0;34m(df, output_csv_path, chunk_size)\u001b[0m\n\u001b[1;32m    254\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_chunk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    255\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m first_chunk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m \u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_csv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_chunks\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processed and written.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m first_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3777\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/io/formats/csvs.py:259\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/io/formats/csvs.py:264\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_header()\n\u001b[0;32m--> 264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/io/formats/csvs.py:302\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m end_i:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/io/formats/csvs.py:309\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    306\u001b[0m slicer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(start_i, end_i)\n\u001b[1;32m    307\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc[slicer]\n\u001b[0;32m--> 309\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_number_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m data \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39miget_values(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res\u001b[38;5;241m.\u001b[39mitems))]\n\u001b[1;32m    312\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_index[slicer]\u001b[38;5;241m.\u001b[39m_format_native_types(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_number_format)\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/core/internals/managers.py:512\u001b[0m, in \u001b[0;36mBaseBlockManager.to_native_types\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_native_types\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    508\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    Convert values to native types (strings / python objects) that are used\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m    in formatting (repr / csv).\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_native_types\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/core/internals/managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/core/internals/blocks.py:531\u001b[0m, in \u001b[0;36mBlock.to_native_types\u001b[0;34m(self, na_rep, quoting, **kwargs)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_native_types\u001b[39m(\u001b[38;5;28mself\u001b[39m, na_rep: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m, quoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Block:\n\u001b[1;32m    530\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"convert to our native types format\"\"\"\u001b[39;00m\n\u001b[0;32m--> 531\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mto_native_types\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_rep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_rep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_block(result)\n",
      "File \u001b[0;32m/opt/conda/envs/macrosystems/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2538\u001b[0m, in \u001b[0;36mto_native_types\u001b[0;34m(values, na_rep, quoting, float_format, decimal, **kwargs)\u001b[0m\n\u001b[1;32m   2535\u001b[0m mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m   2537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quoting:\n\u001b[0;32m-> 2538\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2540\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_folder = \"New_Test\"\n",
    "process_all_subdirectories(base_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04c31a34-2434-4aa8-9eec-1d35fa43087c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12th Column Data (NaN values removed):\n",
      "651          0.0\n",
      "1639        17.0\n",
      "1640         0.0\n",
      "1641         0.0\n",
      "1642         0.0\n",
      "           ...  \n",
      "4117842    124.0\n",
      "4117843     79.0\n",
      "4117844     98.0\n",
      "4117845    227.0\n",
      "4117846      0.0\n",
      "Name: Original_band_2_wl_386.674988nm, Length: 1972503, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "csv_file = \"New_Test/NEON_D13_NIWO_DP1_20200807_170802_reflectance/NEON_D13_NIWO_DP1_20200807_170802_reflectance_spectral_data_all_sensors.csv\"\n",
    "\n",
    "# Load the CSV file\n",
    "try:\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Preview the 12th column\n",
    "    if data.shape[1] >= 12:  # Ensure there are at least 12 columns\n",
    "        twelfth_column = data.iloc[:, 11]  # Column indices are zero-based\n",
    "        twelfth_column_cleaned = twelfth_column.dropna()  # Remove NaN values\n",
    "        print(\"12th Column Data (NaN values removed):\")\n",
    "        print(twelfth_column_cleaned)\n",
    "    else:\n",
    "        print(\"The file does not have 12 columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file at {csv_file} was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1c9354-0020-4901-8e8b-0c89c755d759",
   "metadata": {},
   "source": [
    "### What Happens When `jefe` Runs\n",
    "\n",
    "When you run the `jefe` function, a sequence of operations is executed, and multiple outputs are generated. Here's a detailed breakdown:\n",
    "\n",
    "1. **Downloading Raw Data:**\n",
    "   - The original NEON flight line folder is downloaded to the specified output directory.\n",
    "   - The raw folder contains the reflectance data and associated metadata files.\n",
    "\n",
    "2. **Conversion to Multiple Formats:**\n",
    "   - The downloaded folder is processed to generate additional formats required for analysis.\n",
    "   - These files are named systematically to represent the processing step or correction applied. For example:\n",
    "     - **`_envi`:** Reflectance data in ENVI format.\n",
    "     - **`_envi_mask`:** Mask files indicating areas to include or exclude during analysis.\n",
    "     - **`.hdr`:** Header files describing the structure of the associated data.\n",
    "     - **`.json`:** Configuration files for corrections and processing steps.\n",
    "\n",
    "3. **Application of Corrections:**\n",
    "   - Topographic corrections (TOPO) and bidirectional reflectance distribution function (BRDF) corrections are applied to ensure data accuracy.\n",
    "   - Outputs include:\n",
    "     - **`_brdf_coeffs__envi.json`:** Coefficients for BRDF corrections.\n",
    "     - **`_topo_coeffs__envi.json`:** Coefficients for topographic corrections.\n",
    "\n",
    "4. **Data Extraction and Processing:**\n",
    "   - Spectral data is extracted pixel by pixel and saved in tabular formats for further analysis.\n",
    "   - These extractions are saved incrementally to avoid memory overuse.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Outputs from a Single Flight Line\n",
    "\n",
    "After running the `jefe` function, the output directory contains processed files at the top level and a folder for the original raw data. Here’s what you can expect for a single trial run:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Main Output Directory:**\n",
    "- **Processed Files:** Includes ENVI-format files, masks, headers, and configuration files. These represent the final processed outputs ready for analysis.\n",
    "- **Raw Folder:** A subdirectory containing the original reflectance data downloaded from NEON.\n",
    "\n",
    "| File Name                                             | Description                                         |\n",
    "|-------------------------------------------------------|-----------------------------------------------------|\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance__envi` | Reflectance data converted to ENVI format.         |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance__mask` | Mask file for the reflectance data.                |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance.hdr`   | Header file describing the ENVI data structure.    |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance__brdf_coeffs__envi.json` | BRDF correction coefficients. |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance__topo_coeffs__envi.json` | TOPO correction coefficients. |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Raw Folder (Inside the Output Directory):**\n",
    "- **Original Files:** Contains the raw reflectance data downloaded directly from NEON before any processing steps.\n",
    "\n",
    "| File Name                                             | Description                                         |\n",
    "|-------------------------------------------------------|-----------------------------------------------------|\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance`       | Original reflectance data from NEON.               |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance_ancillary` | Ancillary metadata for corrections.               |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance_config__envi.json` | Configuration for ENVI data processing. |\n",
    "| `NEON_D13_NIWO_DP1_20200807_170802_reflectance_config__anc.json`  | Configuration for ancillary corrections. |\n",
    "\n",
    "---\n",
    "\n",
    "This structure ensures that:\n",
    "1. The **processed files** are readily available in the main directory for analysis.\n",
    "2. The **raw data** is preserved in its original form for reference or reprocessing if needed.\n",
    "\n",
    "By organizing outputs this way, you can easily navigate between raw and processed data while maintaining a clear workflow history.\n",
    "\n",
    "\n",
    "### Process Overview\n",
    "\n",
    "1. **Data Conversion:**\n",
    "   - Converts NEON reflectance data to formats compatible with ENVI tools and downstream analyses.\n",
    "\n",
    "2. **Data Corrections:**\n",
    "   - Applies topographic and BRDF corrections to improve data quality.\n",
    "\n",
    "3. **Outputs Generated:**\n",
    "   - Reflectance data in corrected formats.\n",
    "   - Mask files for regions of interest.\n",
    "   - Configuration files describing the processing steps.\n",
    "   - Coefficients for TOPO and BRDF corrections.\n",
    "\n",
    "By the end of this process, you will have a comprehensive set of files ready for analysis, including corrected reflectance data, metadata, and configurations.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Handling Large Data Processing<a name=\"handling-large-data-processing\"></a>\n",
    "\n",
    "Processing NEON flight lines involves managing large amounts of spectral data. This workflow incorporates strategies to optimize memory usage and prevent bottlenecks.\n",
    "\n",
    "### Key Strategies\n",
    "\n",
    "1. **Chunk Processing:** Processes data in smaller chunks to avoid memory overload.\n",
    "2. **Direct Disk Writing:** Saves intermediate and final results directly to storage.\n",
    "3. **Optimized Data Structures:** Uses efficient formats like NumPy arrays and Pandas DataFrames.\n",
    "4. **Parallel Processing:** Utilizes libraries like `ray` for distributed processing.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## 9. Conclusion<a name=\"conclusion\"></a>\n",
    "\n",
    "This vignette provided a comprehensive, step-by-step guide to processing NEON flight line data, highlighting key techniques and strategies for handling large, complex datasets. The workflow included downloading NEON flight lines, converting them into suitable file formats, applying critical corrections, and extracting hyperspectral data from pixels before writing the results to CSV files for further numerical analysis.\n",
    "\n",
    "By completing this process, you gain the ability to transform raw NEON airborne data into actionable datasets, enabling robust ecological and environmental research. This workflow is designed to balance efficiency, accuracy, and scalability, ensuring that even massive datasets can be processed on machines with limited resources.\n",
    "\n",
    "### **Key Takeaways**\n",
    "1. **Efficient Data Handling:** \n",
    "   - From downloading raw flight line data to saving corrected and processed outputs, this workflow demonstrates how to manage large-scale operations effectively.\n",
    "   - Chunk processing and direct-to-disk writing ensure that memory constraints are respected while maintaining high data fidelity.\n",
    "\n",
    "2. **Robust Data Corrections:** \n",
    "   - The inclusion of topographic and BRDF corrections ensures that the processed data is accurate and reliable for downstream analysis, accounting for variability in reflectance and terrain.\n",
    "\n",
    "3. **Hyperspectral Data for Analysis:** \n",
    "   - The extraction of hyperspectral data from individual pixels provides a valuable resource for detailed numerical and statistical studies, enabling deeper insights into ecological and environmental processes.\n",
    "\n",
    "4. **Scalability and Reproducibility:** \n",
    "   - This workflow is scalable to handle additional flight lines, datasets, and sites, making it a versatile tool for researchers working across diverse geographies and ecological systems.\n",
    "   - By following standardized steps and leveraging robust tools, you can ensure that your processing is reproducible and aligned with scientific best practices.\n",
    "\n",
    "---\n",
    "\n",
    "## 10. References<a name=\"references\"></a>\n",
    "\n",
    "- **NEON Data Portal:** [https://data.neonscience.org/](https://data.neonscience.org/)\n",
    "- **GeoPandas Documentation:** [https://geopandas.org/](https://geopandas.org/)\n",
    "- **Rasterio Documentation:** [https://rasterio.readthedocs.io/](https://rasterio.readthedocs.io/)\n",
    "- **NumPy Documentation:** [https://numpy.org/doc/](https://numpy.org/doc/)\n",
    "- **HyTools Documentation:** [https://hytools.readthedocs.io/](https://hytools.readthedocs.io/)\n",
    "- **Ray Documentation:** [https://docs.ray.io/en/latest/](https://docs.ray.io/en/latest/)\n",
    "- **NEON Field Sites Page:** [https://www.neonscience.org/field-sites/explore](https://www.neonscience.org/field-sites/explore)\n",
    "- **NEON Data Products Catalog:** [https://data.neonscience.org/data-products/explore](https://data.neonscience.org/data-products/explore)\n",
    "- **NEON Data Availability Page:** [https://data.neonscience.org/visualizations/data-availability](https://data.neonscience.org/visualizations/data-availability)\n",
    "- **NEON Flight Schedules and Coverage:** [https://www.neonscience.org/data-collection/flight-schedules-coverage](https://www.neonscience.org/data-collection/flight-schedules-coverage)\n",
    "- **AOP Data Availability Notification:** [https://www.neonscience.org/impact/observatory-blog/aop-data-availability-notification-release-2024](https://www.neonscience.org/impact/observatory-blog/aop-data-availability-notification-release-2024)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macrosystems",
   "language": "python",
   "name": "macrosystems"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
